[
  {
    "Event": ".conf19",
    "Title": "IT1287 - 2019 State of DevOps Expert Panel",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "VictorOps"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "In this session, you will hear from practitioners, decision-makers, and visionaries from market-leading companies who are committed to transforming how software is created. DevOps has moved well past the hype stage, but that does not mean that change is instant or even welcome. \"Agility versus stability,\" CI/CD/DevSecOps/observability, oh my! Let's get into the important issues for debate and discussion with an expert panel of DevOps visionaries with your participation!\n"
  },
  {
    "Event": ".conf19",
    "Title": "BA1321 - 40 Ways to Use Splunk in Financial Services",
    "Track": "Business Analytics",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk Machine Learning Toolkit"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BA1321.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BA1321.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "In this session we will show you how Splunk customers around the world are using Splunk in creative ways to solve a range of challenges, from Trading Strategies to Market Abuse, and from Customer On-boarding to Customer Churn. There are no limits to what Splunk can be used for, and this session will help you get new ideas for how to deploy Splunk in your business.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT1968 - 4 Easy Steps for Reclaiming Your Life (without quitting your job)",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "VictorOps"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1968.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1968.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Everything breaks at 3:00 a.m. No one wants a team of engineers managing incidents on little to no sleep. Think about how tired you were the morning after the last all-nighter you pulled. Coffee can only do so much. If you work in any type of technical operations role you know how easy it is for on-call to take over your life. When managing an on-call team, you must think about the human side of on-call to avoid employee burnout and alert fatigue. In this session we will outline a four-step process to streamline incident management, review how to manage alerts through VictorOps, and give your team their lives back!"
  },
  {
    "Event": ".conf19",
    "Title": "FNC2259 - 5 Tips to Faster Support Case Resolution",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FNC2259.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FNC2259.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "The sooner a support engineer understands your case and has sufficient information, the sooner your case will be resolved. Splunk Support engineers are vey good at what they do, but they can't read minds, and they can't see into your computers. How can you make them understand your issue better and faster? What steps can you take ahead of submitting a support case that will ensure that you understand your issue as well? Attendees will receive some important tips resulting in a faster TTR.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1445 - 7 Super Solutions- Growing, Developing and Administering Splunk@Murex",
    "Track": "Foundations/Platform",
    "Industry": "Technology",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1445.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1445.pdf",
    "SkillLevel": "Intermediate",
    "Description": "The 2017 “Splunk Ninja Winner” and 2019 “Commander Award” finalist will tell the story of… \n\nMoving from 1 to 100 global users the issues faced and solutions produced! This talk discusses the technical and human challenges when working for a major financial software company Murex. By using 7 examples, the talk will illustrate….\n1.    Where is my Data! (Growing)\n2.    Fast Forwarder Deployment (Admin)\n3.    Data is only ½ the Battle (Growing)\n4.    A Snapshot in Time (Dev)\n5.    Different user’s different roles (Admin)\n6.    Improving Debugging X10 (Dev)\n7.    Code Versioning (Dev)\nThe 2017 “Splunk Ninja Winner” and 2019 “Commander Award” finalist will tell the story of… The 2017 “Splunk Ninja Winner” and 2019 “Commander Award” finalist will tell the story of… Moving from 1 to 100 global users the issues faced and solutions produced! This talk discusses the technical and human challenges when working for a major financial software company Murex. By using 7 examples, the talk will illustrate….\n1.    Where is my Data! (Growing)\n2.    Fast Forwarder Deployment (Admin)\n3.    Data is only ½ the Battle (Growing)\n4.    A Snapshot in Time (Dev)\n5.    Different user’s different roles (Admin)\n6.    Improving Debugging X10 (Dev)\n7.    Code Versioning (Dev)Moving from 1 to 100 global users the issues faced and solutions produced! This talk discusses the technical and human challenges when working for a major financial software company Murex. By using 7 examples, the talk will illustrate….\n1.    Where is my Data! (Growing)\n2.    Fast Forwarder Deployment (Admin)\n3.    Data is only ½ the Battle (Growing)\n4.    A Snapshot in Time (Dev)\n5.    Different user’s different roles (Admin)\n6.    Improving Debugging X10 (Dev)\n7.    Code Versioning (Dev)Moving from 1 to 100 global users the issues faced and solutions produced! This talk discusses the technical and human challenges when working for a major financial software company Murex. By using 7 examples, the talk will illustrate….Moving from 1 to 100 global users the issues faced and solutions produced! This talk discusses the technical and human challenges when working for a major financial software company Murex. By using 7 examples, the talk will illustrate….Moving from 1 to 100 global users the issues faced and solutions produced! This talk discusses the technical and human challenges when working for a major financial software company Murex. By using 7 examples, the talk will illustrate….Moving from 1 to 100 global users the issues faced and solutions produced! This talk discusses the technical and human challenges when working for a major financial software company Murex. By using 7 examples, the talk will illustrate….1.    Where is my Data! (Growing)1.    Where is my Data! (Growing)1.    Where is my Data! (Growing)1.    Where is my Data! (Growing)2.    Fast Forwarder Deployment (Admin)2.    Fast Forwarder Deployment (Admin)2.    Fast Forwarder Deployment (Admin)2.    Fast Forwarder Deployment (Admin)3.    Data is only ½ the Battle (Growing)3.    Data is only ½ the Battle (Growing)3.    Data is only ½ the Battle (Growing)3.    Data is only ½ the Battle (Growing)4.    A Snapshot in Time (Dev)4.    A Snapshot in Time (Dev)4.    A Snapshot in Time (Dev)4.    A Snapshot in Time (Dev)5.    Different user’s different roles (Admin)5.    Different user’s different roles (Admin)5.    Different user’s different roles (Admin)5.    Different user’s different roles (Admin)6.    Improving Debugging X10 (Dev)6.    Improving Debugging X10 (Dev)6.    Improving Debugging X10 (Dev)6.    Improving Debugging X10 (Dev)7.    Code Versioning (Dev)7.    Code Versioning (Dev)7.    Code Versioning (Dev)7.    Code Versioning (Dev)"
  },
  {
    "Event": ".conf19",
    "Title": "IT1171 - Accelerate your ability to sniff out application exceptions and detect outliers in performance KPIs",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1171.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1171.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Are you frustrated with how long a Splunk time series query of your data can take when you need it now, and are you looking to use machine learning to quickly gain insights about your app’s performance? Finding application exceptions or detecting outliers in your performance KPIs too late can lead your business to suffer without the information it needs to make the right decisions in a timely manner. We will show you how we used the metrics index and machine learning capabilities in Splunk to make better alerts, build scheduled performance reports, and ultimately gain deeper insights and make better decisions based on our data. Sharing these insights as a weekly scheduled report helped our team find hidden issues, increase performance awareness, and maintain SLAs around performance KPIs. Additionally, better alerts operationally helped us to detect outliers in performance metrics within minutes after they occur. Join this session to see queries, demos and several examples for you to take back with you and implement this solution at your company. "
  },
  {
    "Event": ".conf19",
    "Title": "BAS2981 - Accelerating Business Decisions across end-to-end process",
    "Track": "Business Analytics",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "Today’s business processes are complex and are largely a 'Black Box' of interwoven technologies spanning multiple geographies. DRYiCE™ iControl enables the business to map their end-to-end business processes digitally, providing operational intelligence to pinpoint process flow violations allowing you to conduct clear corrective action at the right place, at the right time.\nUtilizing the power of both Splunk Enterprise™ and Splunk ITSI™, DRYiCE iControl connects Business and IT into a single view."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1500 - Accelerating Cloud Migration with Splunk",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Enterprise"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "National Australia Bank has announced a goal to move 35% of its 2500 applications to the public cloud by the end of 2020. We'll discuss how we’re using our AWS-hosted Splunk platform to enable this migration within a regulated industry by providing visibility of compliance status across hundreds of AWS accounts, as well as implementing consistent infrastructure, application health, and security monitoring patterns across all environments (on-premises, AWS, and Azure).  We’ll also describe how we’ve automated Splunk onboarding and development processes to support the resulting increase in Splunk usage.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT3123 - Actionable Alerts Using Splunk and VictorOps at Irdeto’s 24-7 Service Operations Center",
    "Track": "IT Operations",
    "Products": [],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT3123.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT3123.pdf",
    "Description": "In Irdeto’s 24/7 Service Operations Center, our goal is to enable proactive enable proactive event management based on performance. We manage a vast infrastructure and operations stack and it’s important we have the right tools to make better decisions. We fought the war on “white noise”. Playbooks were established and actionable alerts created. On-Call schedules gave us access to domain expertise. Reporting capabilities helped drive continuous improvement. To better understand how we did this you’ll see an in-depth demo on Integration, the Rules Engine and gain insight into our evolution using ITSI with VictorOps.\nIn Irdeto’s 24/7 Service Operations Center, our goal is to enable proactive enable proactive event management based on performance. We manage a vast infrastructure and operations stack and it’s important we have the right tools to make better decisions. We fought the war on “white noise”. Playbooks were established and actionable alerts created. On-Call schedules gave us access to domain expertise. Reporting capabilities helped drive continuous improvement. To better understand how we did this you’ll see an in-depth demo on Integration, the Rules Engine and gain insight into our evolution using ITSI with VictorOps.In Irdeto’s 24/7 Service Operations Center, our goal is to enable proactive enable proactive event management based on performance. We manage a vast infrastructure and operations stack and it’s important we have the right tools to make better decisions. We fought the war on “white noise”. Playbooks were established and actionable alerts created. On-Call schedules gave us access to domain expertise. Reporting capabilities helped drive continuous improvement. To better understand how we did this you’ll see an in-depth demo on Integration, the Rules Engine and gain insight into our evolution using ITSI with VictorOps.In Irdeto’s 24/7 Service Operations Center, our goal is to enable proactive enable proactive event management based on performance. We manage a vast infrastructure and operations stack and it’s important we have the right tools to make better decisions. We fought the war on “white noise”. Playbooks were established and actionable alerts created. On-Call schedules gave us access to domain expertise. Reporting capabilities helped drive continuous improvement. To better understand how we did this you’ll see an in-depth demo on Integration, the Rules Engine and gain insight into our evolution using ITSI with VictorOps."
  },
  {
    "Event": ".conf19",
    "Title": "IoT1410 - Add value to your SIEM- how Israel's Ministry of Energy applies Machine Learning to protect their Critical Infrastructure and OT Operations",
    "Track": "Internet of Things",
    "Industry": "Public Sector",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IOT1410.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Critical OT Infrastructure is a potential target for cyberattacks that can affect national security. Therefore the Ministry of Energy of the State of Israel implemented an advanced security strategy to protect all power plants in the country. By monitoring all their decentralized SIEM systems and SCADA events from power plant operations they get a comprehensive view of their nationwide security posture in energy supply. Due to highly heterogeneous SIEM landscapes they normalize those with Splunk's CIM and leverage machine learning based approaches on this data to get an edge over manual hand written rules. With the help of Splunk's Machine Learning Toolkit (MLTK) they can not only iterate faster on their use cases but also get immediate outcomes that help their security operations. In this session you learn in detail how they approached anomaly detection use case and their journey with MLTK to get practical insights.\nCritical OT Infrastructure is a potential target for cyberattacks that can affect national security. Therefore the Ministry of Energy of the State of Israel implemented an advanced security strategy to protect all power plants in the country. By monitoring all their decentralized SIEM systems and SCADA events from power plant operations they get a comprehensive view of their nationwide security posture in energy supply. Due to highly heterogeneous SIEM landscapes they normalize those with Splunk's CIM and leverage machine learning based approaches on this data to get an edge over manual hand written rules. With the help of Splunk's Machine Learning Toolkit (MLTK) they can not only iterate faster on their use cases but also get immediate outcomes that help their security operations. In this session you learn in detail how they approached anomaly detection use case and their journey with MLTK to get practical insights."
  },
  {
    "Event": ".conf19",
    "Title": "IT1203 - A deep dive into Boss of the NOC 2019",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1203.mp4",
    "SkillLevel": "Beginner",
    "Description": "In this session we will review this year's Boss of the NOC event, including event results, question review, and next steps, and we will show you how to run this event at your organization.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FNC1549 - Administrators Anonymous- Splunk Best Practices and Useful Tricks I Learned the Hard Way",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk Enterprise Security"
    ],
    "SkillLevel": "Intermediate",
    "Description": "Throughout my journey as Splunk admin, I've experienced a few stumbling blocks along the way (okay, maybe more than just a 'few'). Having seen some of these same mistakes being made by others, I want to show you how you can avoid them. In this session, you will learn about some of the common pitfalls and how to detect them, best practices and how to leverage them, and other useful knowledge from my experiences as a Splunk architect, admin, and instructor that you can take away.\n"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1248 - Advanced Threat Hunting and Anomaly Detection with Splunk UBA",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk User Behavior Analytics",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1248.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1248.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Splunk User Behavior Analytics (UBA) contains the largest library of unsupervised machine learning in the market. In this session we'll show how to analyze data from both cloud and on-premises data sources in both types of deployment (cloud/on-premises) to convey the unique benefits of Splunk UBA. We'll discuss real world examples that showcase the importance of using UBA and all other tools at your disposal for day-to-day threat hunting. Specifically, we'll show how to use Splunk Enterprise, Splunk Enterprise Security, and Splunk UBA together to hunt and detect anomalies that can reveal significant threats. We'll wrap up with best and worst practices from deployments seen throughout the world. "
  },
  {
    "Event": ".conf19",
    "Title": "FN1409 - Advances in Deep Learning with the MLTK Container for TensorFlow 2.0, PyTorch and Jupyter Notebooks",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1409.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1409.pdf",
    "SkillLevel": "Advanced",
    "Description": "Deep Learning frameworks like TensorFlow and PyTorch let you extend Splunk's Machine Learning Toolkit with custom algorithms that provide you with an edge for advanced AI and ML use cases in Security, IT Operations, IoT or for any advanced custom analytics. In this talk you learn about the latest evolution to streamline the usage of TensorFlow 2.0 and PyTorch with the MLTK Container extension. Integrated Jupyter Notebooks help data scientist to accelerate their custom model development, deployment and operationalization. The MLTK Container can leverage GPUs for parallel computing and accelerate model training for big complex datasets. This session is suitable for all python-minded data scientists and developers who want to tap into deep learning use cases with Splunk. \n "
  },
  {
    "Event": ".conf19",
    "Title": "FN1408 - Agent Based Modeling for CryptoFinance in Splunk",
    "Track": "Foundations/Platform",
    "Industry": "Technology",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1408.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1408.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Cryptocurrency ecosystems are highly complex, distributed, and rapidly evolving, rendering many existing financial models ineffective. By aggregating the heterogeneous data streams that are produced by distinct groups within crypto (blockchains, mining pools, exchanges, etc.), we have built a unified analytical platform called Nakamoto Terminal (NTerminal) using Splunk. By leveraging NTerminal, we are creating an adapted agent-based modeling (ABM) system; agents monitor the state of the ecosystem by consuming real time updates from the individual data sources that modulate their state and connectivity. Different heuristic models are called upon to facilitate data transformations and agent interactions. Within this ecosystem, collective agent activity reveals emergent properties and patterns of behavior. With Splunk as the centerpiece, integrated reports, dashboards, or searches allow you to better navigate the ecosystem of interest."
  },
  {
    "Event": ".conf19",
    "Title": "IoT1413 - A healthcare company changing lives with Splunk",
    "Track": "Internet of Things",
    "Industry": "Healthcare",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1413.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IoT1413.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "This session is about a healthcare company born from cybersecurity using Splunk to disrupt the traditional healthcare delivery model. Accuhealth has created a remote patient monitoring platform to analyze health and biometric data collected from IoT devices to improve the patients' quality of life. A doctor is alerted in real-time of any abnormal changes to their patient's vitals and have immediate access to medical trends, allowing the doctor to make faster and more informed decisions. Splunk is at the nerve center of our platform. Using machine learning and health data, the tool provides actionable insights and predictive analysis to patients, doctors, caregivers, and researchers. Additionally, we have leveraged Splunk to integrate the three pillars of telehealth: Telemedicine, Chronic Care Management, and Remote Patient Monitoring. Learn first hand and see a demo of how Accuhealth is creating a paradigm shift in healthcare. "
  },
  {
    "Event": ".conf19",
    "Title": "IT1648 - AIB IRELAND – Splunk-ITSI monitoring of time critical Payment Business Flows with real time insights into health of Mobile and Payment Applications",
    "Track": "IT Operations",
    "Industry": "Financial Services",
    "Products": [
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1648.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1648.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Splunk Insights platform supporting AIB Digital Transformation on the journey to AIOps. This presentation will outline the payments business and technology strategy, steps, challenges, and benefits AIB has realized leveraging Splunk and IT Service Intelligence to deliver:\n* Payments end-to-end business activity monitoring, including payment-level integrity checking, trends, and performance against time critical cut offs; and\n* Mobile and Payment App “live” health monitoring using Splunk ITSI for predictive service insights into business, application, and inframetrics, including OS and Oracle health diagnostics.\nAIB is using Splunk ITSI to monitor service health across the stack and will share examples where health degradation of critical services was highlighted, allowing AIB to intervene in time and resolve the issue before a critical incident occurred."
  },
  {
    "Event": ".conf19",
    "Title": "IT1119 - AIOps - How to build a Self Learning Event Analytics Platform",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence",
      "Splunk Machine Learning Toolkit"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1119.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1119.pdf",
    "SkillLevel": "Advanced",
    "Description": "AIOps platforms utilize big data, modern machine learning, and advanced analytics technologies to directly and indirectly enhance IT Operations functions. This session shows how to leverage Splunk IT Service Intelligence (ITSI) and the Machine Learning Toolkit (MLTK) to build a basic, self-learning recommendation engine. Your Operations Center will reap the benefits from having assisted recovery input, but this session does not stop there. It also will show you how to fully automate the recovery. If you have AIOps initiatives on your radar come on and participate in this session "
  },
  {
    "Event": ".conf19",
    "Title": "FN1425 - Analyzing and Visualizing Streaming Telemetry Data with Splunk",
    "Track": "Foundations/Platform",
    "Industry": "Technology",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1425.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1425.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Streaming telemetry is a new approach supported by major networking companies such as Cisco,  as well as service providers. It provides better real time data of routing/switching devices without the performance impact that comes with SNMP. Network operators who turn to streaming telemetry often have sophisticated goals in mind and are not simply looking to collect data and throw it onto a graph. Instead, they are looking at advanced use cases where further processing is performed on the data (e.g., analytics or machine learning) and intelligent action is triggered based on this analysis. This presentation. will discuss how to get telemetry data into Splunk to search, analyze, visualize, and create alerts including examples from an existing deployment."
  },
  {
    "Event": ".conf19",
    "Title": "SECS3014 - Anatomy of an Attack",
    "Track": "Security, Compliance and Fraud",
    "Products": [],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SECS3014.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC3014.pdf",
    "Description": "This session covers how an actual phishing attack from APT29 came together. We will discuss how incident responders can learn more about the attack and build out a timeline of key events. We will also explain how DNS based security plays an important role in proactively blocking threats and how integrations with Splunk can help you with effective threat hunting.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT1118 - Anatomy of a Successful Event Analytics Deployment",
    "Track": "IT Operations",
    "Industry": "CommunicationsFinancial ServicesHealthcareHigher EducationNon-ProfitOnline ServicesPublic SectorRetailTechnologyTravel & Transportation",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1118.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1118.pdf",
    "SkillLevel": "Intermediate",
    "Description": "This session dives deep into event analytics deployments and how to execute them. From data onboarding to event consolidation and event reduction, this session covers proven strategies and tips and tricks to show you how to approach replacing an existing Event Management Systems while avoiding information overload for your operations staff. If you want to move off your legacy MoM and replace it with Splunk's IT Service Intelligence you don't want to miss this!"
  },
  {
    "Event": ".conf19",
    "Title": "SECS3058 - An Information Security Approach to Feature Engineering",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security"
    ],
    "SkillLevel": "Advanced",
    "Description": "Feature engineering in data science is central to obtaining satisfactory results from deep learning models. When considering how to create features for InfoSec purposes it is important to consider the context of the features and what their underlying meaning is. Common data science techniques such as feature hashing and one-hot encoding, while effective for certain tasks, often fall short when creating features for security related models. This talk expands upon feature encoders and scalers and the rationale used to arrive at these methods of encoding. It also goes into detail on the algorithms used to build these new encoders. Attendees to this presentation will come away with a new approach to encoding InfoSec features for machine learning that should increase the fidelity of their deep learning models.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN2519 - Announcing Splunk Investigate",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2519.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2519.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Learn about our latest SaaS offering, Splunk Investigate. This session will provide an overview of the new features of this cloud application and how you can quickly get started investigating all your machine data. Now teams have a way to easily collaborate on investigations and resolve incidents faster than before.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1586 - A novel introduction to Machine Learning",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1586.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1586.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "It’s time to demystify Machine learning:\n-             Do you typically ask your friends what they will order, before you decide what you will order? Did you know that already the ancient Greeks clued out that numbers rules the universe?\n-             What makes machine learning so powerful considering the human brain is way smarter.... but maybe not always?\n-             This session will walk you through the basics of machine learning, what is it, what it is not. How to avoid pitfalls.\n-             After this session you will have knowledge around the concept of Machine Learning,  what the pre-built Splunk ML products are as well as IKEAs exploration of Splunk`s different ML techniques.\nIt’s time to demystify Machine learning:It’s time to demystify Machine learning:-             Do you typically ask your friends what they will order, before you decide what you will order? Did you know that already the ancient Greeks clued out that numbers rules the universe?-             Do you typically ask your friends what they will order, before you decide what you will order? Did you know that already the ancient Greeks clued out that numbers rules the universe?-             What makes machine learning so powerful considering the human brain is way smarter.... but maybe not always?-             What makes machine learning so powerful considering the human brain is way smarter.... but maybe not always?-             This session will walk you through the basics of machine learning, what is it, what it is not. How to avoid pitfalls.-             This session will walk you through the basics of machine learning, what is it, what it is not. How to avoid pitfalls.-             After this session you will have knowledge around the concept of Machine Learning,  what the pre-built Splunk ML products are as well as IKEAs exploration of Splunk`s different ML techniques.-             After this session you will have knowledge around the concept of Machine Learning,  what the pre-built Splunk ML products are as well as IKEAs exploration of Splunk`s different ML techniques."
  },
  {
    "Event": ".conf19",
    "Title": "BAS2794 - Applied Intelligence - Powered Pivot to the Future",
    "Track": "Business Analytics",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BAS2794.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BAS2794.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "AI and automation are fueling a new norm of digital disruption and breeding new forms of competition. To stay ahead, businesses must continuously innovate with speed and agility like never before. Learn how major companies are using Splunk powered intelligent warehouses with machine learning led inventory optimization, end to end supply chain visibility and chatbot integrations to create unprecedented business outcomes. Check out the secrets of “Rotation Masters” and how they are using AI to do things differently and drive double-digit growth."
  },
  {
    "Event": ".conf19",
    "Title": "BAS2795 - Applying Artificial Intelligence - Making It Real by Empowering Our Businesses and Communities",
    "Track": "Business Analytics",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BAS2795.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BAS2795.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Data is disrupting every industry, and tapping into dark data with applied intelligence is empowering businesses & communities to make real world impact. From enhanced consumer experiences to intelligent enterprises, the use of digital technologies powered by analytics and artifical intelligence are creating new business avenues. Learn how amanufacturing giants are using AI powered digital twins to improve their product journey from design to maintenance. Leveraging Splunk to create smart, connected products, platforms and business models that live both physically and digitally."
  },
  {
    "Event": ".conf19",
    "Title": "IoT1103 - Applying Splunk Essentials for Predictive Maintenance",
    "Track": "Internet of Things",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1103.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IoT1103.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Predictive maintenance is a key initiative and a strategy that directly impacts the bottom lines of many industrial operations around the globe. Yet many of the organizations don’t know where and how to start due to a lack of knowledge about data platforms, methodology, and analytics techniques. Based on the recently released “Splunk Essentials for Predictive Maintenance” app that offers key methodologies and Splunk’s powerful machine learning capability, this session will demystify the data science elements of predictive maintenance to make the process real and pragmatic. Through this session, the audience will learn and appreciate the power of Splunk in a way that will allow agile application of analytic-driven predictive maintenance to the broader moving parts of their operations. "
  },
  {
    "Event": ".conf19",
    "Title": "FN1097 - App Sorcery 2- Building Better Splunk Apps with Best Practice",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1097.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1097.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Let's build a Splunk App and take it further! What makes a Splunk App tick? How do we build them? How do they make your Splunk life easier? How do they work in clusters? How can your app be approved for Splunk Cloud deployments, and how can your tried-and-tested, on-premises apps be migrated? All these questions will be answered in this session with real world examples direct from Splunk Professional Services. This is about Splunk app creation, from barebones to enterprise deployment."
  },
  {
    "Event": ".conf19",
    "Title": "IT1514 - A Prescriptive Design for Enterprise-Wide Alerts in IT Service Intelligence",
    "Track": "IT Operations",
    "Industry": "Financial ServicesHealthcareNot industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1514.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1514.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Producing meaningful, trusted alerts is the holy grail of any IT service monitoring tool, yet the road to get there is winding and perilous. While Splunk IT Service Intelligence is uniquely suited to convert machine data into actionable alerts, how to design your ITSI deployment to effectively produce, maintain, and scale alerts isn't obvious, until now! In this talk, we'll walk you through the alerting strategy GEHA implemented in conjunction with Splunk to achieve meaningful alerts show you a highly streamlined design that you can replicate in your deployment."
  },
  {
    "Event": ".conf19",
    "Title": "FN2067 - Architecting Splunk for High Availability and Disaster Recovery",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2067.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2067.pdf",
    "SkillLevel": "Intermediate",
    "Description": "As Splunk Enterprise becomes more critical to organizations and business functions, it becomes crucial to maximize the uptime of the service. We'll talk about general principles of resiliency/high availability and disaster recovery, and how they apply to a Splunk deployment. We'll also discuss the various mechanisms for implementing them, levels of availability, relative advantages, and the costs of each."
  },
  {
    "Event": ".conf19",
    "Title": "FN1945 - Artificial Intelligence got you down- Here’s Machine Learning for Humans!",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1945.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Did you struggle with college statistics? Do the letters ‘AI’ mean ‘Always Intimidating’ to you? Have you been putting off learning about the Machine Learning Toolkit because you aren’t a data scientist? Well don’t fret, because we aren’t data scientists either! We are, however, able to teach you how to crack the code on learning, leveraging, and operationalizing one of the most powerful components of the Splunk platform. Learn where, when, and how to use it, and the steps to make Splunk’s AI successful for you. We break it down into easy-to-understand segments, walk step-by-step through some use cases that you can take home with you, and arm you with the resources to successfully continue down the AI path. Don’t miss this opportunity and you won’t suffer any longer from machine learning envy! Data scientists need not apply. "
  },
  {
    "Event": ".conf19",
    "Title": "FN2084 - Artificial Intelligence- How Will It Rock Your World-  ",
    "Track": "Foundations/Platform",
    "Industry": "Non-ProfitNot industry specific",
    "Products": [
      "Splunk Enterprise",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2084.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2084.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "With the rapid evolution and adoption of artificial intelligence and machine learning underway, what do you need to know about the upsides and the risks?  How is the hype and reality of AI going to change your world over the next five years?  How will government policies and standards affect your work in implementing the technology?  A panel of thought leaders will share their perspectives on these key questions.\nWith the rapid evolution and adoption of artificial intelligence and machine learning underway, what do you need to know about the upsides and the risks?  How is the hype and reality of AI going to change your world over the next five years?  How will government policies and standards affect your work in implementing the technology?  A panel of thought leaders will share their perspectives on these key questions.With the rapid evolution and adoption of artificial intelligence and machine learning underway, what do you need to know about the upsides and the risks?  How is the hype and reality of AI going to change your world over the next five years?  How will government policies and standards affect your work in implementing the technology?  A panel of thought leaders will share their perspectives on these key questions."
  },
  {
    "Event": ".conf19",
    "Title": "DEV1308 - A Timely Development- I Built a Splunk App To Save Security Analysts Time - And So Can You!",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1308.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV1308.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Have you ever had an idea that would improve incident response? I did. I knew I could save security analysts time by providing a tool that enabled analysts to determine if an endpoint had persistent malware present in seconds. However, it would need to integrate seamlessly into their incident response workflow and have a quality user interface. Frankly, that felt like an insurmountable hurdle for someone with little front-end development experience. I was pleasantly surprised to find that even as a solo developer, I was able to create a full-featured Splunk App with an interface that looks like it was designed by someone far more talented. Through a demonstration of my incident response app and a discussion of my experience building it, I’ll show you how Splunk makes it easier and, more importantly, realistic to bring your own ideas to life. I’ll also share a few pain-points I encountered so you can avoid some of the mistakes I made."
  },
  {
    "Event": ".conf19",
    "Title": "SEC2286 - Attacking and Defending Kubernetes- A Purple Team Approach to Improving Detection Using Splunk Enterprise Security, Splunk Phantom and Peirates",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2286.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2286.pdf",
    "SkillLevel": "Advanced",
    "Description": "Would you be able to detect a sophisticated adversary targeting your Kubernetes clusters and workloads tonight? How do busy teams with stacked backlogs find time to learn how to attack Kubernetes clusters, detect those attacks, and build defenses to reduce the attack surface? We will demonstrate an effective purple team methodology that \"uses every part of the buffalo\" by 1) executing attacks on Kubernetes using the open source tool Peirates, 2) tracking the attack artifacts from the adversary simulation in Splunk, 3) teaching the defenders how the attack was performed and where to look for forensic artifacts, and 4) working together in the purple-est way possible to improve detection and response capabilities using Splunk Enterprise Security, Splunk Phantom, and Peirates."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1156 - ATT&CK™ing Linux with SPL",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1156.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1156.pdf",
    "SkillLevel": "Advanced",
    "Description": "In this session we will discuss using Splunk to detect a range of Linux-based adversary techniques from MITRE’s ATT&CK™ framework. We will also demonstrate how event sequencing can be used to map a path through the ATT&CK™ matrix and improve overall detection fidelity. We will provide auditd configuration suggestions for Linux endpoints to support greater coverage."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1927 - ATT&CK™ Yourself Before Someone Else Does",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1927.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1927.pdf",
    "SkillLevel": "Beginner",
    "Description": "Do you love the idea of the MITRE ATT&CK™ framework, but you’re not sure how to use it in your Splunk-centric security program? This talk will teach you practical ways to use the framework in your own organization and the Splunk security tools that will help you do so. We'll start the talk by identifying an adversary and some of their known techniques, and then we'll show how to choose an appropriate set of detections and how to test whether those detections are working as expected. You'll leave the talk better able to take advantage of threat intelligence, cover the right set of ATT&CK™ tactics and adversary groups, and eliminate organizational blind spots."
  },
  {
    "Event": ".conf19",
    "Title": "FN1752 - Augmented Reality- A Day in the Field with Splunk AR",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Mobile"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1752.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1752.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Do you want to deliver the power of Splunk AR to your colleagues in the field? Splunk AR ties Splunk data to real-world objects, so users can easily consume the data where it lives. We’ll go over several use cases of technicians in the field using Splunk AR to discover, diagnose, and resolve issues in various environments. You'll walk away equipped with the tools to construct these experiences from scratch and enable your organization to deploy Splunk AR."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1374 - Augment Your Security Monitoring Use Cases with Splunk's Machine Learning Toolkit",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1374.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1374.pdf",
    "SkillLevel": "Advanced",
    "Description": "Do you want to use machine learning to enhance your datacenter security monitoring, but you don’t know where to start? Then this is the talk for you. Come learn how high secure datacenter operations benefit from operationalizing machine learning. With the help of the Splunk's Machine Learning Toolkit, your security analysts can take different approaches to use case creation and gain new insight into what's going on in your environment. We'll detail the challenges, benefits and use cases of using machine learning for datacenter security monitoring, and we'll answer questions such as: Where does it make sense to apply machine learning, and where should we stick with classic searches? Can we detect meaningful anomalies in system behavior? Is it possible to cluster our account activities and find unusual patterns? This is a practical session of security monitoring use cases, deep diving into the ideas, concepts and the SPL behind them.\nDo you want to use machine learning to enhance your datacenter security monitoring, but you don’t know where to start? Then this is the talk for you. Come learn how high secure datacenter operations benefit from operationalizing machine learning. With the help of the Splunk's Machine Learning Toolkit, your security analysts can take different approaches to use case creation and gain new insight into what's going on in your environment. We'll detail the challenges, benefits and use cases of using machine learning for datacenter security monitoring, and we'll answer questions such as: Where does it make sense to apply machine learning, and where should we stick with classic searches? Can we detect meaningful anomalies in system behavior? Is it possible to cluster our account activities and find unusual patterns? This is a practical session of security monitoring use cases, deep diving into the ideas, concepts and the SPL behind them.Do you want to use machine learning to enhance your datacenter security monitoring, but you don’t know where to start? Then this is the talk for you. Come learn how high secure datacenter operations benefit from operationalizing machine learning. With the help of the Splunk's Machine Learning Toolkit, your security analysts can take different approaches to use case creation and gain new insight into what's going on in your environment. We'll detail the challenges, benefits and use cases of using machine learning for datacenter security monitoring, and we'll answer questions such as: Where does it make sense to apply machine learning, and where should we stick with classic searches? Can we detect meaningful anomalies in system behavior? Is it possible to cluster our account activities and find unusual patterns? This is a practical session of security monitoring use cases, deep diving into the ideas, concepts and the SPL behind them.Do you want to use machine learning to enhance your datacenter security monitoring, but you don’t know where to start? Then this is the talk for you. Come learn how high secure datacenter operations benefit from operationalizing machine learning. With the help of the Splunk's Machine Learning Toolkit, your security analysts can take different approaches to use case creation and gain new insight into what's going on in your environment. We'll detail the challenges, benefits and use cases of using machine learning for datacenter security monitoring, and we'll answer questions such as: Where does it make sense to apply machine learning, and where should we stick with classic searches? Can we detect meaningful anomalies in system behavior? Is it possible to cluster our account activities and find unusual patterns? This is a practical session of security monitoring use cases, deep diving into the ideas, concepts and the SPL behind them.Do you want to use machine learning to enhance your datacenter security monitoring, but you don’t know where to start? Then this is the talk for you. Come learn how high secure datacenter operations benefit from operationalizing machine learning. With the help of the Splunk's Machine Learning Toolkit, your security analysts can take different approaches to use case creation and gain new insight into what's going on in your environment. We'll detail the challenges, benefits and use cases of using machine learning for datacenter security monitoring, and we'll answer questions such as: Where does it make sense to apply machine learning, and where should we stick with classic searches? Can we detect meaningful anomalies in system behavior? Is it possible to cluster our account activities and find unusual patterns? This is a practical session of security monitoring use cases, deep diving into the ideas, concepts and the SPL behind them."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1479 - Automate Forensic Investigations in AWS with Splunk",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Technology",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1479.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1479.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Alerts in cloud environments require your team to quickly and precisely gather evidence and isolate affected environments. The GE Digital Predix Incident Response (IR) team found an abundance of content for analyzing forensic evidence from Windows environments, but they noticed a gap in content built for performing investigations on Linux-based hosts. The Predix IR team will discuss the tools they have built to contain a compromised Linux-based hosts, gather evidence, and analyze that evidence in Splunk. Utilizing splunk searches, lookups, and visualization components to look both narrowly into the data set as well as broadly across the rest of the environmental data in splunk to identify known bad or potentially suspicious activities that may warrant further investigation by an analyst. \n"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1128 - Automate Your Phishing Response with Splunk Enterprise Security, Splunk Phantom, and Machine Learning",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1128.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1128.pdf",
    "SkillLevel": "Advanced",
    "Description": "We developed an automation framework that classifies and mitigates emails reported to the SOC. The framework acts as an engine that consumes multiple data sources, including a supervised machine learning model and a risk scoring algorithm to assess with high confidence if an email is phishing, spam, or benign. We will discuss the benefits of our approach to phishing mitigation, such as enhancing our SOC's ability to automatically identify, prioritize, and mitigate malicious phishing attempts against employees before any damage is done.  The session will outline the overall design of the framework, detail the primary components that are used within Splunk Phantom and Splunk Enterprise Security, and will outline the supervised machine learning model that we trained to aide the automation engine.  "
  },
  {
    "Event": ".conf19",
    "Title": "FN1655 - Auto-Monitoring of search workloads with Workload Management",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1655.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1655.pdf",
    "SkillLevel": "Intermediate",
    "Description": "How do you ensure that business-critical searches are not impacted by non-critical searches? How do you divvy up Splunk search capacity among internal teams? How do you assign appropriate resources for ingestion and search? Do you want to monitor runaway/rogue searches in real time and prevent adverse impact on the rest of your users? If you are struggling with these questions, this is the session for you! Splunk Workload Management puts you in control. It allows you to define resource pools, access to them, limit the maximum number of concurrent searches in a pool, and monitor rogue searches. Using Linux cgroups to allocate CPU and memory to different pools, Workload Management allows you to create separate pools for ingestion and search. You can create sub-pools within search to isolate incoming searches. The rules can be defined based on search type, user, roles, application etc. Even better, you can monitor for rogue searches and automatically abort or throttle them."
  },
  {
    "Event": ".conf19",
    "Title": "SECS2979 - AWS and Splunk “Take Action” Playbook-  Identify & Resolve Security Incidents with Splunk, Phantom, AWS Security Hub and AWS Services",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Cloud",
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SECS2979.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Learn how AWS and Splunk have “better together” solutions for Security risk identification and response. See the demo showing how to identify disparate security risks, aggregate them, and send them to Splunk Phantom to remediate. \n\nThe Splunk platform provides real-time, end-to-end visibility into your AWS environment to help you organize, display, and take action on your security alerts. AWS Security Hub makes this possible by aggregating security findings from AWS and third-party sources and exporting them to Splunk through a single point of integration—even as you continue to add new data sources. \n\nSecurity findings can then be sent to Splunk Phantom to automate responses, handling repetitive tasks to focus your attention on mission-critical decisions.  The integration between AWS and Splunk makes it easy for you to head off future security threats and free up your security personnel for higher value activities."
  },
  {
    "Event": ".conf19",
    "Title": "FN1886 - Best Practice of EMM deployment for Splunk Mobile",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Mobile"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1886.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1886.pdf",
    "SkillLevel": "Intermediate",
    "Description": "This session is for customers who would like to extend their Splunk usage to mobile devices and understand how Enterprise Mobility Management (EMM) works. It also will benefit customers who have existing mobility management solutions, including mobile device management (MDM) and mobile application management (MAM) systems, and want to enroll the Splunk mobile apps. The session will include an introduction and live demos showing detailed integration with the most popular mobile device management (MDM) solutions: AirWatch, MobileIron, and Blackberry. We also will have live demos to illustrate how to configure different Splunk mobile apps with the enterprise mobility management (EMM) solutions from scratch. Topics to be discussed will include profile configuration, authorized devices, app wrapping, and MDM/MAM-specific features."
  },
  {
    "Event": ".conf19",
    "Title": "FN1054 - Best Practices and Better Practices for Admins",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "SkillLevel": "Advanced",
    "Description": "Up level your Splunk Fu by learning best practices for administrators that will make you a Splunk Ninja in no time! An updated version of the popular session from .conf2015 through .conf2018, this session will review all of the tips and tricks you've regretted not knowing. As always, attendees are encouraged to support each other by sharing their own best practices, tips and tricks, and love for all things Splunk! This session will explore topics relevant for admins, such as effective Splunk resources, common pitfalls, monitoring consoles, and admin ideas to make your Splunk deployment a palace!"
  },
  {
    "Event": ".conf19",
    "Title": "SECS2841 - Best Practices for Rapid Containment of Incidents",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SECS2841.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SECS2841.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Prevention and detection solutions are vital to maintain a healthy network but not sufficient.\n\nWhen a security incident occurs, the ability to investigate rapidly and recover is crucial but is manually intensive, especially when dealing with networks spanning on premise, public, and private cloud environments.\n\nOnce an incident is detected, then what?\n\nLearn how RedSeal integrates within Splunk Enterprise Security and Phantom framework to provide you with immediate answers to burning questions."
  },
  {
    "Event": ".conf19",
    "Title": "FN1402 - Best practises for forwarder hierarchies",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence"
    ],
    "SkillLevel": "Advanced",
    "Description": "Splunk is like an iceberg, on the surface we see the major components: indexers, search heads, license master, cluster master but under the water line we have a huge number of forwarders collecting and aggregating data streams. These forwarders are the foundations of any installation and configuration issues translate into problems with alerts, search performance, cluster stability and scaling out. This talk shows you to various ways to measure the efficiency of data collection and then how to improve it by the correct application autoLBVolume, EVENT_BREAKER, forceTimeBasedAutoLB, INGEST_EVALS and multiple pipelines. Prepare for lots of complex searches to identify common problems and charts that show good and bad. The talk aims to revolutionize how you think about data collection in Splunk and turbo charge your platform performance and improve stability.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1749 - Beyond the Desktop- Making the Most out of Mobile",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1749.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1749.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "With Splunk Mobile and Splunk TV, the power of Splunk now extends beyond your desktop. These apps unlock new ways to get the most out of your data and the most out of Splunk. In this session we’ll show you how easy it is to get started with these new products. We’ll discuss strategies for taking advantage of the new form factors while navigating some or their limitations. We’ll give you best practices for designing mobile-friendly dashboards, configuring alerts, and managing groups of devices. We’ll also equip you with what you need to diagnose issues you may have when transitioning to our mobile products. We’ll show you how to put the power of Splunk in your pocket (and on your TV)."
  },
  {
    "Event": ".conf19",
    "Title": "SEC2205 - Beyond Tier 1 Automation- The Hidden Value of Splunk Phantom Automation for Security Operations ",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Higher Education",
    "Products": [
      "Splunk Enterprise",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2205.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2205.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "You've probably heard examples of Splunk Phantom automating 90% of Tier 1 processes, but did you know that Phantom improves human-lead processes too? Come learn about the hidden value of validation and utility playbooks from Penn State University’s Enterprise Security Manager and Splunk’s Lead Technologist for Higher Education. Validation playbooks are automated tests run to validate a human judgement or request. Utility playbooks are short easy-to-create playbooks in Phantom that an analyst  runs during an investigation.  We’ll cover when to use validation and utility playbooks, how to get started creating them, and ideas for other playbooks you can use to improve your daily operations. "
  },
  {
    "Event": ".conf19",
    "Title": "IT2009 - Big DevOps in Small Packages- Building a scalable cloud monitoring platform with SmartStore, Docker and AWS Services at UK Ministry of Defence",
    "Track": "IT Operations",
    "Industry": "Public Sector",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT2009.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT2009.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Last year, at the Container Strikes Back session, Brent Boe and Brian Bingham introduced the new Splunk official container. Today, we’re talking about the benefits as well as the challenges we encountered adopting the “Splunk in containers!” strategy at the Ministry of Defence for both our testing and our production environments. Our small DevOps team tasked with deploying Splunk did not have the bandwidth to build a resilient and scalable Splunk deployment in a traditional way at the pace required. By consuming the official Splunk Docker image, taking advantage of SmartStore backed by AWS S3, making a liberal use of other AWS services, and with support from Splunk PS architects as well as communities around Splunk’s official GitHub repos, we were able to rapidly deploy a complex Splunk cluster in AWS with minimal overhead. This has allowed our project to keep to our tight deadlines around cloud migration, while also providing a monitoring platform that can be scaled out over time as adoption increases."
  },
  {
    "Event": ".conf19",
    "Title": "FN1352 - Blockchain is entering the enterprise, see what Splunk is doing and how you can leverage it.",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1352.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1352.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Blockchain applications and infrastructure are new, complex, and generate a variety of data. Splunk is a perfect match for ingesting, analyzing, and gaining insights from data on-chain, off-chain, and even cross-chain. Organizations can now monitor the health, performance, and security of blockchain infrastructure as well gain insights by analyzing transactions and correlate with external data. In this session you will get to see it all in action (live demos!) and even participate yourself, there might even be some free cryptocurrency given away.  Blockchain applications and infrastructure are new, complex, and generate a variety of data. Splunk is a perfect match for ingesting, analyzing, and gaining insights from data on-chain, off-chain, and even cross-chain. Organizations can now monitor the health, performance, and security of blockchain infrastructure as well gain insights by analyzing transactions and correlate with external data. In this session you will get to see it all in action (live demos!) and even participate yourself, there might even be some free cryptocurrency given away. Blockchain applications and infrastructure are new, complex, and generate a variety of data. Splunk is a perfect match for ingesting, analyzing, and gaining insights from data on-chain, off-chain, and even cross-chain. Organizations can now monitor the health, performance, and security of blockchain infrastructure as well gain insights by analyzing transactions and correlate with external data. In this session you will get to see it all in action (live demos!) and even participate yourself, there might even be some free cryptocurrency given away. "
  },
  {
    "Event": ".conf19",
    "Title": "SEC1781 - BotS the Missing Link",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "BOTS has become the most popular capture the flag event in the world, and many of our contestants are gunning for first place. How do you get the edge to beat out the competition? Two words: link analysis. Link analysis has helped authorities and security teams track down drug cartel leaders, terrorists, top criminals, and threat actors for years. In this session we'll walk through the principles of link analysis and how we can apply the technique across the BOTS datasets.\n"
  },
  {
    "Event": ".conf19",
    "Title": "SEC2280 - Break Down Silos By Ingesting Multi-Purpose Data Sources into Splunk",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2280.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2280.pdf",
    "SkillLevel": "Beginner",
    "Description": "We use Splunk data to help previously siloed groups at Qualcomm work better together. We will discuss specific high value, multipurpose data sources that we ingest, and how we use them to foster collaboration across teams. You will leave this session with a better sense of data sources that you analyze for security that can also help you work better with everyone from developers, to help desk staff, to executive management."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1930 - Break Free From Legacy GRC to Achieve Real-Time Integrated Risk Visibility",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1930.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1930.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "As organizations shift away from legacy Governance, Risk, and Compliance (GRC) approaches towards an integrated risk management (IRM) strategy, cyber risk management paradigms must also shift. This presentation will address why firms are shifting to IRM and how the shift to IRM will affect security organizations globally. We will showcase strategies used by forward-leaning peers and thought leaders to operationalize integrated risk management programs in their organizations.\n"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1597 - Build a JARVIS for Your SOC",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1597.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1597.pdf",
    "SkillLevel": "Intermediate",
    "Description": "The boss saw Ironman and wanted to create a JARVIS-like assistant for our SOC...so we built him one using Splunk. In this session we will share how we developed a Splunk virtual assistant to improve SOC efficiency and support the SOC 2.0 model of continual improvement. SOC JARVIS solves problems such as: How does a SOC manage its attack detection ideas and knowledge? How does an analyst understand the impact of their search changes on alert volumes? How does the SOC manage feedback between analysts and search authors? Learn how to use Splunk in a novel way to address these problems so that you can make your SOC workflows more efficient and let analysts spend more time threat hunting and improving how they detect attacks."
  },
  {
    "Event": ".conf19",
    "Title": "DEV1476 - Build Apps Fast with SDC Developer Tools",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "AI/ML",
      "Splunk Developer Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1476.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV1476.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Learn how to build powerful apps with the Splunk Developer Cloud.  "
  },
  {
    "Event": ".conf19",
    "Title": "SEC2189 - Build a Successful Vulnerability Management Program on Splunk",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "Organizations struggle to keep up with the increasing number of vulnerabilities that they face. In this session, we will demonstrate how to use Splunk to cut through the noise and focus on those vulnerabilities that pose the greatest threats your organization. We will show you how to construct a framework that assesses risk by key risk factors, such as asset or identity criticality, threat intelligence, asset or application exposure, vulnerability campaign IDs, and others. We'll also show you how to inventory vulnerabilities and integrate them with workflow software so you can monitor their remediation. Lessons learned from this session will help you improve your vulnerability management program going forward."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1446 - Build Automated Decisions for Incident Response with Splunk Phantom",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1446.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1446.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Incident response (IR) analysts are required to make multiple decisions on every alert and incident. Whether the decision is to escalate, respond, or to discard the alert, each one of those decisions is critical to protecting their environment. With the integration of SOAR platforms like Splunk Phantom into IR teams, many of those decisions can now be automated for analysts. These decisions can save hours of work for analysts and allow for focus on more critical alerts. However, there are still questions to answer before implementing these decisions. What data is needed to make confident decisions? Where in the process should these decisions be made? How can existing decisions be improved? How should new decisions be integrated? The General Electric IR team has worked to answer these questions by using Splunk Enterprise and Splunk Phantom. In this session, we will show how our team approached these questions, implemented solutions, and integrated decisions for our analysts to save time and focus their efforts."
  },
  {
    "Event": ".conf19",
    "Title": "FND1981 - Building a Kickass LGBTQ+ Employee Resource Group",
    "Track": "Foundations/Platform",
    "Industry": "Diversity & Inclusion",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FND1981.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FND1981.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Do you want to promote productivity and camaraderie at the same time? Do you want to spearhead events that are about community and inclusion? If so, join us as we talk about what it takes to create an Employee Resource Group (ERG) that celebrates diversity. In this session, we’ll share the grassroots origins of Pride@Splunk, focusing on the transition from an unofficial club to an established ERG. More importantly, we’ll draw on our experience to provide tips for starting your own ERG in any sized company. Come learn how to lead an ERG that supports diversity, advocacy, and inclusion."
  },
  {
    "Event": ".conf19",
    "Title": "DEV1141 - Building applications with Splunk UI and Splunk React Visualizations ",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Developer Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1141.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV1141.pdf",
    "SkillLevel": "Intermediate",
    "Description": "In this session we will walk you through the process of creating a highly customized application experience using React and Splunk's UI and visualization libraries. In this session we will walk you through the process of creating a highly customized application experience using React and Splunk's UI and visualization libraries.In this session we will walk you through the process of creating a highly customized application experience using React and Splunk's UI and visualization libraries.In this session we will walk you through the process of creating a highly customized application experience using React and Splunk's UI and visualization libraries.In this session we will walk you through the process of creating a highly customized application experience using React and Splunk's UI and visualization libraries."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1391 - Building a Security Monitoring Strategy 2.0",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security",
      "Splunk Machine Learning Toolkit",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1391.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1391.pdf",
    "SkillLevel": "Intermediate",
    "Description": "So you have a SIEM with security data, e.g. firewalls, proxy, endpoint data, etc. Now what? How do you effectively operationalize your investment? This session provides recipes, principles, patterns, and strategies for using Splunk and data-driven analytics to move your security monitoring and compliance effectiveness up the maturity curve. This session will cover how to identify key mixes of data sources, core OOTB content to use, and how to layer capabilities aligned with your maturity. We will help you go beyond the endless alerts and investigations and start creating value by reducing the impact of potential security events. We're excited to show you that there's no need for a PhD in security assurance and operations—just Splunk and a solid plan."
  },
  {
    "Event": ".conf19",
    "Title": "FNS2529 - Building a Splunk SmartStore data platform with FlashBlade for high-volume datasets",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FNS2529.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2529.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Splunk’s new, cloud native architecture SmartStore simplifies Indexer maintenance by 10X by disaggregating compute from storage. Learn how SmartStore with Pure Storage FlashBlade delivers dynamic scaling, storage efficiency and high performance to accelerate operational intelligence and security management. This session will cover new capabilities enabled by SmartStore, performance considerations and deployment best practices."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1556 - Building Behavioral Detections- Cross-Correlating Suspicious Activity with the MITRE ATT&CK™ Framework",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1556.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1556.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Advanced attackers that live off your land add insult to what can be very serious injury. In this session we'll show you how to use behavioral analysis to identify advanced attackers that evade traditional signature-based detection methods. We do so in our organization by using Splunk to combine insights from traditional data sources to detect activity across multiple phases of the MITRE ATT&CK™ framework. We'll focus on how to build queries  tune them for your environment, and start catching these threat actors with behavioral detections as soon as you get back from .conf.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN2195 - Building scalable AWS based Splunk Architectures using Cloud Formation in 30 minutes or less",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2195.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2195.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Using AWS cloud formation we will demonstrate creation of a full blown Splunk Enterprise system in 30 minutes or less. While the system is spinning up, we will discuss the decision points and process that was taken while creating a best-of-breed, AWS-based Splunk deployment. You will be introduced to the core AWS Components (EC2, Storage, VPC, security), Splunk Enterprise Architecture (multi-site clustered index and multi-site search head cluster), Base and CLI Configurations, Cloud Formation Automation, GIT Configuration management, and best practices surrounding those technologies."
  },
  {
    "Event": ".conf19",
    "Title": "IoT1509 - Building the “smartest factory on planet earth” – Accenture partnering with leading crystal maker innovating with Splunk for Industrial IoT",
    "Track": "Internet of Things",
    "Industry": "Manufacturing",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML",
      "Splunk for Industrial IoT"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1509.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IoT1509.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Production of crystal and gemstones requires high-class, top-quality output and unmatched quality and accuracy in the E2E production line. Based on in-depth experience, our joint client is one of the leading providers of production line machinery, serving its own business units as well as industry customers with precision optical instruments, grinding, sawing, drilling, and dressing tools. The new technical innovations in the area of Industrial Internet of Things (IIoT) offer completely new options to improve smart production lines. Thus Accenture is partnering with Splunk on creating a roadmap to build a fully digital, smart factory that will become a world-leading lighthouse facility. This session will provide insights into how the power of data enabled by Splunk can realize a quantum step in modern production line environments. It also will help you understand the value of data science for predictive quality, digital twin scenarios, reduced lot size, and closed loop R&D processes. "
  },
  {
    "Event": ".conf19",
    "Title": "SECS2797 - Building threat-driven use cases for the real world with iDefense intelligence",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SECS2797.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SECS2797.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Where did you come up with the idea for your last use case? Traditional approaches to use case ideation focus on identifying new use cases based on the data already available to the security operations center. However, the threat landscape is constantly changing, and attackers are constantly getting more sophisticated. To detect these advanced threats, our use cases must be based on both business and threat context. In this session, we will share our approach to building innovative use cases based on real-world threats. Starting with industry-specific threat intelligence, we identify the threat actors and their specific tactics, techniques, and procedures. With these insights, we identify use cases relevant to the business, map them to both existing and new data sources, and prioritize implementation based on the specific threats."
  },
  {
    "Event": ".conf19",
    "Title": "ITS2199 - Build intelligent self-assuring data center with Cisco ACI, NAE and Splunk Enterprise",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/ITS2199.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/ITS2199.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Do you spend most of your valuable time analyzing and troubleshooting connectivity or performance issues in your data center? Did you wish your networks were smarter in alerting you about potential issues before it actually impacts your SLA? Join us to learn how Cisco ACI, NAE(Network Assurance Engine) & Splunk can help you achieve this goal. Cisco ACI, the industry-leading SDN solution, enables scalable multi-cloud networks with a consistent policy model, and gain the flexibility to move applications seamlessly to any location or any cloud while maintaining security and high availability. Cisco NAE uses mathematical models to continuously verify that policy implemented in your data center is inline with the intent. Splunk Apps for ACI, NAE bring health/performance metrics, faults and policy deviations together to help you accelerate your root cause analysis, thereby improving the operational efficiency of your data center. Session covers use cases and demo of these Cisco Data Center Networking product integrations with Splunk Enterprise.\nDo you spend most of your valuable time analyzing and troubleshooting connectivity or performance issues in your data center? Did you wish your networks were smarter in alerting you about potential issues before it actually impacts your SLA? Join us to learn how Cisco ACI, NAE(Network Assurance Engine) & Splunk can help you achieve this goal. Cisco ACI, the industry-leading SDN solution, enables scalable multi-cloud networks with a consistent policy model, and gain the flexibility to move applications seamlessly to any location or any cloud while maintaining security and high availability. Cisco NAE uses mathematical models to continuously verify that policy implemented in your data center is inline with the intent. Splunk Apps for ACI, NAE bring health/performance metrics, faults and policy deviations together to help you accelerate your root cause analysis, thereby improving the operational efficiency of your data center. Session covers use cases and demo of these Cisco Data Center Networking product integrations with Splunk Enterprise.Do you spend most of your valuable time analyzing and troubleshooting connectivity or performance issues in your data center? Did you wish your networks were smarter in alerting you about potential issues before it actually impacts your SLA? Join us to learn how Cisco ACI, NAE(Network Assurance Engine) & Splunk can help you achieve this goal. Cisco ACI, the industry-leading SDN solution, enables scalable multi-cloud networks with a consistent policy model, and gain the flexibility to move applications seamlessly to any location or any cloud while maintaining security and high availability. Cisco NAE uses mathematical models to continuously verify that policy implemented in your data center is inline with the intent. Splunk Apps for ACI, NAE bring health/performance metrics, faults and policy deviations together to help you accelerate your root cause analysis, thereby improving the operational efficiency of your data center. Session covers use cases and demo of these Cisco Data Center Networking product integrations with Splunk Enterprise.Do you spend most of your valuable time analyzing and troubleshooting connectivity or performance issues in your data center? Did you wish your networks were smarter in alerting you about potential issues before it actually impacts your SLA? Join us to learn how Cisco ACI, NAE(Network Assurance Engine) & Splunk can help you achieve this goal. Cisco ACI, the industry-leading SDN solution, enables scalable multi-cloud networks with a consistent policy model, and gain the flexibility to move applications seamlessly to any location or any cloud while maintaining security and high availability. Cisco NAE uses mathematical models to continuously verify that policy implemented in your data center is inline with the intent. Splunk Apps for ACI, NAE bring health/performance metrics, faults and policy deviations together to help you accelerate your root cause analysis, thereby improving the operational efficiency of your data center. Session covers use cases and demo of these Cisco Data Center Networking product integrations with Splunk Enterprise.Do you spend most of your valuable time analyzing and troubleshooting connectivity or performance issues in your data center? Did you wish your networks were smarter in alerting you about potential issues before it actually impacts your SLA? Join us to learn how Cisco ACI, NAE(Network Assurance Engine) & Splunk can help you achieve this goal. Cisco ACI, the industry-leading SDN solution, enables scalable multi-cloud networks with a consistent policy model, and gain the flexibility to move applications seamlessly to any location or any cloud while maintaining security and high availability. Cisco NAE uses mathematical models to continuously verify that policy implemented in your data center is inline with the intent. Splunk Apps for ACI, NAE bring health/performance metrics, faults and policy deviations together to help you accelerate your root cause analysis, thereby improving the operational efficiency of your data center. Session covers use cases and demo of these Cisco Data Center Networking product integrations with Splunk Enterprise.Do you spend most of your valuable time analyzing and troubleshooting connectivity or performance issues in your data center? Did you wish your networks were smarter in alerting you about potential issues before it actually impacts your SLA? Join us to learn how Cisco ACI, NAE(Network Assurance Engine) & Splunk can help you achieve this goal. Cisco ACI, the industry-leading SDN solution, enables scalable multi-cloud networks with a consistent policy model, and gain the flexibility to move applications seamlessly to any location or any cloud while maintaining security and high availability. Cisco NAE uses mathematical models to continuously verify that policy implemented in your data center is inline with the intent. Splunk Apps for ACI, NAE bring health/performance metrics, faults and policy deviations together to help you accelerate your root cause analysis, thereby improving the operational efficiency of your data center. Session covers use cases and demo of these Cisco Data Center Networking product integrations with Splunk Enterprise.Do you spend most of your valuable time analyzing and troubleshooting connectivity or performance issues in your data center? Did you wish your networks were smarter in alerting you about potential issues before it actually impacts your SLA? Join us to learn how Cisco ACI, NAE(Network Assurance Engine) & Splunk can help you achieve this goal. Cisco ACI, the industry-leading SDN solution, enables scalable multi-cloud networks with a consistent policy model, and gain the flexibility to move applications seamlessly to any location or any cloud while maintaining security and high availability. Cisco NAE uses mathematical models to continuously verify that policy implemented in your data center is inline with the intent. Splunk Apps for ACI, NAE bring health/performance metrics, faults and policy deviations together to help you accelerate your root cause analysis, thereby improving the operational efficiency of your data center. Session covers use cases and demo of these Cisco Data Center Networking product integrations with Splunk Enterprise."
  },
  {
    "Event": ".conf19",
    "Title": "DEV2178 - Build your own custom data visualization on dashboard",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Developer Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV2178.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV2178.pdf",
    "SkillLevel": "Advanced",
    "Description": "Ever had the necessity to have fine-grain control over visualizations on your Splunk dashboards? This talk will show you everything you need to know about how to build your own custom data visualization experience. Work through real-world examples by customizing the very popular Buttercup games dashboard. By the end of this talk, you will be inspired to have your dashboard with your own visualizations and share them with the Splunk community. Ever had the necessity to have fine-grain control over visualizations on your Splunk dashboards? This talk will show you everything you need to know about how to build your own custom data visualization experience. Work through real-world examples by customizing the very popular Buttercup games dashboard. By the end of this talk, you will be inspired to have your dashboard with your own visualizations and share them with the Splunk community.Ever had the necessity to have fine-grain control over visualizations on your Splunk dashboards? This talk will show you everything you need to know about how to build your own custom data visualization experience. Work through real-world examples by customizing the very popular Buttercup games dashboard. By the end of this talk, you will be inspired to have your dashboard with your own visualizations and share them with the Splunk community.Ever had the necessity to have fine-grain control over visualizations on your Splunk dashboards? This talk will show you everything you need to know about how to build your own custom data visualization experience. Work through real-world examples by customizing the very popular Buttercup games dashboard. By the end of this talk, you will be inspired to have your dashboard with your own visualizations and share them with the Splunk community.Ever had the necessity to have fine-grain control over visualizations on your Splunk dashboards? This talk will show you everything you need to know about how to build your own custom data visualization experience. Work through real-world examples by customizing the very popular Buttercup games dashboard. By the end of this talk, you will be inspired to have your dashboard with your own visualizations and share them with the Splunk community."
  },
  {
    "Event": ".conf19",
    "Title": "BA1428 - Business Flow vs The Real World. Cutting Through Complexity in Big Retail to Deliver Broad and Deep Assurance",
    "Track": "Business Analytics",
    "Industry": "Retail",
    "Products": [
      "Splunk Business Flow"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BA1428.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BA1428.pdf",
    "SkillLevel": "Intermediate",
    "Description": "At a major UK retail organization, we developed so many great dashboards over the years for every conceivable business area. This once sufficed, but it fuelled greater expectation in an ever harsher trading environment. Enter Peak Trade periods and the need for outcome-centered visibility across the order fulfillment process \"just got real\".\nCould Splunk Business Flow help us to uncover the actual flow, given the teams who built it are long disbanded? Could we talk \"flow\" when support teams align to \"steps\"?  \nWhat we found was data that cannot agree what an Order is. Processes which tackle different aspects of an order, sometimes in parallel or batch. Processes which fork and then merge later on. Processes which go into planned deep-freeze for days or weeks before reawakening. Data formats which are hostile. The list goes on.\nWe take you through the thought process, share mistakes and learnings, and outline considerations/techniques to help you win in the arena of real-world data.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN2156 - Can’t we just have a bot run our deployments- Yes, yes we can.",
    "Track": "Foundations/Platform",
    "Industry": "Healthcare",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2156.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2156.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Can’t we just have a bot run our deployments? Yes we can. Here at Optum, our Splunk team has developed a hands-off method for deploying the Splunk Universal Forwarder on thousands of hosts in just minutes. With our automation we have been able to take advantage of bot integration via ChatOps to take care of our business needs, all while keeping our executives happy. Oh, and the backend configs? Yeah, we have tips on those too."
  },
  {
    "Event": ".conf19",
    "Title": "BA1959 - Case Study- Call Center Agent Performance Monitoring",
    "Track": "Business Analytics",
    "Industry": "Communications",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BA1959.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BA1959.pdf",
    "SkillLevel": "Advanced",
    "Description": "Do you want to monitor call-center employee performance in Splunk? We all know Splunk does a great job with web logs and IoT devices, but could it be used to provide a meaningful Scorecard for 3000+ employees and their management? We think it can, and we would like to share our successful implementation of a Splunk Employee Metrics Dashboard. Some of the concepts include: data preparation, summary indexes, tokens, custom CSS and JavaScript, drilldowns to the details, security, and more!"
  },
  {
    "Event": ".conf19",
    "Title": "SEC2083 - Catch exfiltration from cloud file stores early!",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk User Behavior Analytics"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2083.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2083.pdf",
    "SkillLevel": "Advanced",
    "Description": "In this session, we tackle data breaches and information exfiltration from cloud file stores. Beyond the attacks that make headlines and result in millions of stolen personal records, we will also focus on the far less publicized risks related to exposure of intellectual property, infrastructure details or finances. We will share our experience in building a defensive strategy that now detects highly-covert exfiltration attempts.\n\nTo this end, we first shed a lot of light on how companies use general-purpose file stores, such as Box, Office365 or Google Drive. We cover the types of files that commonly get stored in the cloud, file sharing practices, access properties, as well as uses of cloud stores by various departments. There are a lot of unexpected insights which eventually invalidate common security assumptions.\n\nAs the boundary between good and bad gets blurred, we will provide you with a peek into how to design an effective data-driven defense. This approach helped us hone our detection to just tens of validly suspicious exfiltration files in a massive cloud store.\nIn this session, we tackle data breaches and information exfiltration from cloud file stores. Beyond the attacks that make headlines and result in millions of stolen personal records, we will also focus on the far less publicized risks related to exposure of intellectual property, infrastructure details or finances. We will share our experience in building a defensive strategy that now detects highly-covert exfiltration attempts.In this session, we tackle data breaches and information exfiltration from cloud file stores. Beyond the attacks that make headlines and result in millions of stolen personal records, we will also focus on the far less publicized risks related to exposure of intellectual property, infrastructure details or finances. We will share our experience in building a defensive strategy that now detects highly-covert exfiltration attempts.In this session, we tackle data breaches and information exfiltration from cloud file stores. Beyond the attacks that make headlines and result in millions of stolen personal records, we will also focus on the far less publicized risks related to exposure of intellectual property, infrastructure details or finances. We will share our experience in building a defensive strategy that now detects highly-covert exfiltration attempts.To this end, we first shed a lot of light on how companies use general-purpose file stores, such as Box, Office365 or Google Drive. We cover the types of files that commonly get stored in the cloud, file sharing practices, access properties, as well as uses of cloud stores by various departments. There are a lot of unexpected insights which eventually invalidate common security assumptions.To this end, we first shed a lot of light on how companies use general-purpose file stores, such as Box, Office365 or Google Drive. We cover the types of files that commonly get stored in the cloud, file sharing practices, access properties, as well as uses of cloud stores by various departments. There are a lot of unexpected insights which eventually invalidate common security assumptions.To this end, we first shed a lot of light on how companies use general-purpose file stores, such as Box, Office365 or Google Drive. We cover the types of files that commonly get stored in the cloud, file sharing practices, access properties, as well as uses of cloud stores by various departments. There are a lot of unexpected insights which eventually invalidate common security assumptions.As the boundary between good and bad gets blurred, we will provide you with a peek into how to design an effective data-driven defense. This approach helped us hone our detection to just tens of validly suspicious exfiltration files in a massive cloud store.As the boundary between good and bad gets blurred, we will provide you with a peek into how to design an effective data-driven defense. This approach helped us hone our detection to just tens of validly suspicious exfiltration files in a massive cloud store.As the boundary between good and bad gets blurred, we will provide you with a peek into how to design an effective data-driven defense. This approach helped us hone our detection to just tens of validly suspicious exfiltration files in a massive cloud store."
  },
  {
    "Event": ".conf19",
    "Title": "DEV1293 - Check-Out SPL Rehab- A new way to debug your searches",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk Developer Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1293.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/Dev1293.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Wouldn't it be great if SPL had a debug mode?! We think so too, which is why we created SPL Rehab. This new app allows you to step through your search on a per-command basis, visualize key figures from the job inspector and search log, and show you how your overall output is affected! We will also show you how the tool works under the covers and how you can apply some useful dashboarding tricks to your own apps! Wouldn't it be great if SPL had a debug mode?! We think so too, which is why we created SPL Rehab. This new app allows you to step through your search on a per-command basis, visualize key figures from the job inspector and search log, and show you how your overall output is affected! We will also show you how the tool works under the covers and how you can apply some useful dashboarding tricks to your own apps!Wouldn't it be great if SPL had a debug mode?! We think so too, which is why we created SPL Rehab. This new app allows you to step through your search on a per-command basis, visualize key figures from the job inspector and search log, and show you how your overall output is affected! We will also show you how the tool works under the covers and how you can apply some useful dashboarding tricks to your own apps!"
  },
  {
    "Event": ".conf19",
    "Title": "BAS2793 - Classifying Evil – Combating Human Trafficking with the power of Data & Analytics",
    "Track": "Business Analytics",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BAS2793.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BAS2793.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Human trafficking is one of the most egregious and lucrative crimes of modern society. Traffickers use both lawful and illicit infrastructure to conduct business, making it difficult for defenders to identify trafficking on their services.\n\nIn our next innovative leap, Global Emancipation Network partnered with Splunk4Good and Accenture to develop the world’s first trafficking content classifier. We are using a Splunk powered data & analytics platform tocreate a risk scoring engine that assigns values to indicators and patterns and provides action options on flagged results to allow better decision-making for law enforcement, government, and private sector users. Learn how Splunk can turn data into robust classifiers to help save lives."
  },
  {
    "Event": ".conf19",
    "Title": "FNS2896 - Cloud-scale On-Prem- SmartStore Best Practices with Dell EMC",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FNS2896.mp4",
    "SkillLevel": "Good for all skill levels",
    "Description": "The potential cost-savings and operational benefits of SmartStore are impressive, but they are not reserved only for those organizations deploying Splunk in the cloud.  Learn how Dell EMC delivers cloud-scale object storage for on-premises deployments of SmartStore and how you can leverage best practices learned from some of the world's largest global Splunk deployments."
  },
  {
    "Event": ".conf19",
    "Title": "IoT2128 - ControlWatch- Cybersecurity Monitoring for Operational Technology (OT) and Industrial Control System (ICS) Environments",
    "Track": "Internet of Things",
    "Industry": "Manufacturing",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT2128.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IOT2128.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Today’s industrial, manufacturing, and building management systems rely on an array of on-demand, uninterruptable technologies, like Industrial Control Systems (ICS), for their day-to-day operations. Escalations in cyber-related attacks have made increased visibility into these often overlooked systems paramount. Over the past year, Booz Allen has built an OT cybersecurity monitoring solution called ControlWatch that provides enhanced visibility and anomaly detection with a focus on OT environments. By aggregating data from within and around the process, the solution provides a critical view for plant managers, C-level decision-makers, or the boots on the ground. We’ll highlight the context into, detection of, and alerting on a myriad of malicious and misaligned activities. We have implemented unique OT-centric use cases and will walk-through a day-in-the-life scenario to show you how the solution increases cybersecurity awareness and resilience in a production organization at all levels."
  },
  {
    "Event": ".conf19",
    "Title": "DEV1667 - Converting a Traditional Splunk App to a Splunk Cloud App with Splunk Developer Cloud",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Developer Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1667.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV1667.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Unveiled at .conf2018, Splunk Developer Cloud (SDC) gives developers the ability to integrate Splunk data services into their own applications. If you’ve been curious about getting started with SDC, this session is for you. Attendees will discover how to convert a traditional Splunk App to an SDC App, eliminating potential infrastructure resource roadblocks, leveraging more flexible scaling options, and building better visualizations with a modern, React-based framework. We'll also take a deep dive into the differences in the application design and development process between a traditional Splunk App and SDC as we walk you through our internal process of converting one of our homegrown Splunk Apps."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1619 - Cops and Robbers II- Paint the Town Purple! ",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "SkillLevel": "Intermediate",
    "Description": "Building on last year's .conf talk, we're making security monitoring great again by adding more functionality and flair to the concept of a MITRE ATT&CK™ navigator interface in Splunk. The result is an easier way to execute tests and validate your detections.  Our main goal is not only improving your security posture but also fostering cross-team collaboration through a methodology and tools that simulate adversary style actions, validate your existing detections, and build new detections. These enhancements make it easier for you to prove to yourself and your management that you have (or don't have) the visibility you need.  We will showcase one way to tangibly measure your capabilities against the popular ATT&CK™ framework, and we'll again use Atomic Red Team for our adversary actions, because it's awesome and leverages a common framework popular with Splunk customers.  In addition to apps and playbooks, this year's collateral will include detailed guides to help you implement these tactics in your own SOC."
  },
  {
    "Event": ".conf19",
    "Title": "SECS2831 - Couples Therapy- Real-World Mistakes and Lessons Learned in Business Relationships",
    "Track": "Security, Compliance and Fraud",
    "Products": [],
    "Description": "Being a good security executive is like being married to all of your counterparts. If you want the relationship to work, it has to be a team sport. Discuss real-world scenarios with Joel Fulton, CISO of Splunk, and Colin O’Connor, COO of ReliaQuest, around times that relationships went bad and hear about ways you can work better within your organization and partnerships to avoid these types of situations.\nBeing a good security executive is like being married to all of your counterparts. If you want the relationship to work, it has to be a team sport. Discuss real-world scenarios with Joel Fulton, CISO of Splunk, and Colin O’Connor, COO of ReliaQuest, around times that relationships went bad and hear about ways you can work better within your organization and partnerships to avoid these types of situations.Being a good security executive is like being married to all of your counterparts. If you want the relationship to work, it has to be a team sport. Discuss real-world scenarios with Joel Fulton, CISO of Splunk, and Colin O’Connor, COO of ReliaQuest, around times that relationships went bad and hear about ways you can work better within your organization and partnerships to avoid these types of situations."
  },
  {
    "Event": ".conf19",
    "Title": "FN1315 - Cover Your Assets- Protect Your Knowledge Objects from Yourself (and Others) - A Paychex story",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence",
      "Splunk Machine Learning Toolkit"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1315.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1315.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "\"Did we just lose ALL our knowledge objects? Do you know how much time and energy that was?\" After a destructive resync, Paychex lost two months of its knowledge object creations/modifications. We learned to be prepared if it were to ever happen again. How? It's easier than you might think, and you don't have to be an admin. You’ll learn how to proactively save your work (dashboards, reports, data models, MLTK experiments, ITSI glass tables, macros, views, etc.) and audit changes when they occur. You will leave the session knowing how to manage the ever-increasing amount of things you create. You'll also have solutions that can save you time and effort from having to recreate lost/modified objects, including how to restore service faster. You also will come away with peace of mind knowing that you can take control of safeguarding and protecting your work, thereby covering your assets when a disaster happens."
  },
  {
    "Event": ".conf19",
    "Title": "IT1670 - Customer Service is as Easy as Pie with ITSI",
    "Track": "IT Operations",
    "Industry": "Higher EducationTechnologyNot industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1670.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1670.pdf",
    "SkillLevel": "BeginnerIntermediateAdvancedGood for all skill levels",
    "Description": "Do you need to support thousands of people every day logging into your organization’s web applications? Customer service is important to Penn State University (PSU), and Splunk ITSI offered the perfect solution for health monitoring and data sharing between executive management and subject matter experts within the WebAccess team. At PSU, WebAccess is the Single Sign-On (SSO) solution supporting over 140,000 students, facility, and staff. ITSI’s Service Analyzer showcases the relationships among various SSO components: Active Directory, CoSign, Duo Two-Factor Authentication (2FA), Lightweight Directory Access Protocol (LDAP), Kerberos, and Shibboleth. Join us and learn how ITSI pioneered the recipe for PSU to isolate root cause slicing deep dives and allowed management to analyze performance on a glass table. Splunk’s ITSI continues to deliver valuable insights from back end systems to front end user interfaces, improving customer service. "
  },
  {
    "Event": ".conf19",
    "Title": "FN2124 - Data Fabric Search(DFS) - Under the Hood",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2124.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2124.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Data Fabric Search (DFS) is the next generation of Splunk’s search platform. DFS executes the following vision: Splunk should be able to leverage compute assets from anywhere and access and execute on data regardless of type and origin. Inspired by the above mantra DFS scales Splunk searches both in terms of volume and cardinality. In this session you will learn how DFS searches scale to trillion scale event volume or billion scale cardinality - capabilities previously impossible. DFS is not limited to local but by supporting scaled federated executions also powers remote splunk deployments. The Search Pipeline of DFS has been build grounds up based on lambda architecture which provides massive scale, high throughput and performance gains. At the end some of the performance and scale numbers which has been achieved internally will be shared.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN2030 - Data Fabric Search- Opening doors to unprecedented levels of scale and performance",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2030.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2030.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Data Fabric Search (DFS) is one of Splunk's newest technologies built to handle the ever increasing rate of growth in data. Combining the rich functionality of Splunk with the parallel execution of Spark, DFS enables searching data at scales previously unheard of. This session will be decidedly for a technical audience as we deep-dive into DFS architecture and demonstrate the capabilities DFS offers to execute searches that scan more than one trillion events at a time. We will also share our experience as performance engineers with tips and tricks on how to tune Splunk deployments to take full advantage of this exciting new feature. Having additional capability to search at massive scale opens doors to use cases that no longer need to make the trade-off between depth of analysis and timeliness of results. Come learn more about Data Fabric Search and see real-world examples of its power put to use. The impossible is possible."
  },
  {
    "Event": ".conf19",
    "Title": "FN2276 - Data Fabric Search Use Cases- Real World Applications",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2276.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2276.pdf",
    "SkillLevel": "Intermediate",
    "Description": "See Data Fabric Search (DFS) in action! We will walk through real-world customer stories and demonstrate how DFS was used to make effective business decisions. With DFS, organizations can quickly weave together insights across the enterprise to get a deeper and more comprehensive view of customer behavior, organizational threats, or business opportunities. Data Fabric Search excels in speed and scale with use cases primarily falling into two categories: 1) High performance and high cardinality searches processing large volumes of data, and 2) Queries that run operations across multiple deployments. For each customer story, we will demonstrate how DFS is able to run these queries successfully and show how they created powerful business results.\n"
  },
  {
    "Event": ".conf19",
    "Title": "ITS2579 - Data Models- Bridging the Knowledge Gap to Work with Complex Data … Even Mainframe Data! ",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/ITS2579.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT2579.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Do you have complex or obscure data?  Do users, without the necessary domain knowledge, struggle to work with this data?  Perhaps a Data Model can help bridge the knowledge gap.  This session will use valuable (but complex…and even scary) mainframe metrics as a working example of how to get users up and running fast by leveraging the power of a data model.  But, these principles are not just for mainframe.  They can equally apply to other data types and scenarios.  Not all users are well-versed in complex data structures and fields, but some are…so combine Subject Matter Expert (SME) knowledge with this great Splunk facility to make everyone’s life simpler.  Deliver real results in a very short time with little overhead or burden on users."
  },
  {
    "Event": ".conf19",
    "Title": "FN1200 - Data Onboarding Methodologies",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security"
    ],
    "SkillLevel": "Advanced",
    "Description": "Onboarding data into Splunk can be a challenge if you don’t know where to start. Come to this session to review our standard Data Onboarding Process so you can ensure you are bringing in data using the best way possible approach every time. This session will cover the process and provide some examples and best practices for various methods of data ingestion."
  },
  {
    "Event": ".conf19",
    "Title": "FN1561 - Data Onboarding- Where do I begin-",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1561.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1561.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "How do I get data into Splunk? What is a sourcetype? Does Splunk already know how to handle my data? What app do I use? What if all my data is syslog? If you are asking these questions, then this session is for you. After all, data quality is the foundation of becoming a data-driven organization. This session will walk through onboarding fundamentals. We will discuss the importance of a timestamp and what to do if your data may not have one. We will explain when to use an existing or create a new sourcetype. We will review the process of examining an app from Splunkbase and determining what sourcetype the app expects. By the end of this session you will no longer use syslog as a sourcetype, but as a means of collecting data."
  },
  {
    "Event": ".conf19",
    "Title": "DEV3288 - DataPaaS – Simplifying Complexity with Automation and CI-CD",
    "Track": "Developer",
    "Products": [],
    "Description": "kl"
  },
  {
    "Event": ".conf19",
    "Title": "DEV1317 - Data Stream Processor- Architecture and SDKs",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Data Fabric Search and Data Stream Processor"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1317.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV1317.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Popular stream processing frameworks (such as Apache Spark Streaming, Apache Flink, and Apache Kafka Streams) make stream processing accessible to developers with language bindings typically in Java, Scala, and Python. These frameworks also include some variant of streaming SQL support to further expand the accessibility of large-scale, low-latency, high-throughput stream processing. What's missing is bringing the world of stream processing to the Business Intelligence user. At Splunk we've built a tool called Splunk Data Stream Processor (DSP) to fill this gap.  In this session, Max and Sharon will present the design and architecture of DSP. We will compare it with other stream processing frameworks to show you how DSP allows users to visually author and preview stream processing pipelines and instantly deploy them at scale. We will also present our developer SDKs, allowing third-party custom functions to be developed and integrated for data processing. With its high level abstractions for business users and extensible framework for developers, Data Stream Processor makes stream processing accessible to the widest possible audience. Popular stream processing frameworks (such as Apache Spark Streaming, Apache Flink, and Apache Kafka Streams) make stream processing accessible to developers with language bindings typically in Java, Scala, and Python. These frameworks also include some variant of streaming SQL support to further expand the accessibility of large-scale, low-latency, high-throughput stream processing. What's missing is bringing the world of stream processing to the Business Intelligence user. At Splunk we've built a tool called Splunk Data Stream Processor (DSP) to fill this gap.  In this session, Max and Sharon will present the design and architecture of DSP. We will compare it with other stream processing frameworks to show you how DSP allows users to visually author and preview stream processing pipelines and instantly deploy them at scale. We will also present our developer SDKs, allowing third-party custom functions to be developed and integrated for data processing. With its high level abstractions for business users and extensible framework for developers, Data Stream Processor makes stream processing accessible to the widest possible audience.Popular stream processing frameworks (such as Apache Spark Streaming, Apache Flink, and Apache Kafka Streams) make stream processing accessible to developers with language bindings typically in Java, Scala, and Python. These frameworks also include some variant of streaming SQL support to further expand the accessibility of large-scale, low-latency, high-throughput stream processing. What's missing is bringing the world of stream processing to the Business Intelligence user. At Splunk we've built a tool called Splunk Data Stream Processor (DSP) to fill this gap.  In this session, Max and Sharon will present the design and architecture of DSP. We will compare it with other stream processing frameworks to show you how DSP allows users to visually author and preview stream processing pipelines and instantly deploy them at scale. We will also present our developer SDKs, allowing third-party custom functions to be developed and integrated for data processing. With its high level abstractions for business users and extensible framework for developers, Data Stream Processor makes stream processing accessible to the widest possible audience.Popular stream processing frameworks (such as Apache Spark Streaming, Apache Flink, and Apache Kafka Streams) make stream processing accessible to developers with language bindings typically in Java, Scala, and Python. These frameworks also include some variant of streaming SQL support to further expand the accessibility of large-scale, low-latency, high-throughput stream processing. What's missing is bringing the world of stream processing to the Business Intelligence user. At Splunk we've built a tool called Splunk Data Stream Processor (DSP) to fill this gap.  In this session, Max and Sharon will present the design and architecture of DSP. We will compare it with other stream processing frameworks to show you how DSP allows users to visually author and preview stream processing pipelines and instantly deploy them at scale. We will also present our developer SDKs, allowing third-party custom functions to be developed and integrated for data processing. With its high level abstractions for business users and extensible framework for developers, Data Stream Processor makes stream processing accessible to the widest possible audience."
  },
  {
    "Event": ".conf19",
    "Title": "FN2062 - Data Stream Processor- How to get the most out of your data!​",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Data Fabric Search and Data Stream Processor"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2062.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2062.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Have you ever been asked to create a resilient petabyte scale data collection and distribution architecture? Do you need to transform data before it is indexed to remove unnecessary or sensitive data or even enrich the data with a lookup before writing the data to your index? Do you need to detect specific patterns to identify the event line break, event timestamp, or assign the appropriate sourcetype? Do you need to control where to send the data including the specific Splunk Index(es) or even a non-Splunk Sink?If so, we will show you how Splunk’s Data Stream Processor (DSP) can be used to address these requirements to meet both current and future demands. We will walk through the scenarios that customers are dealing with today for these requirements. Finally we will talk about how Universal Forwarder, Heavy Weight Forwarder, and HTTP Event Collector fit into this new data ingestion architecture.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1325 - DB Connect- Automating the H-E-Double Hockey Sticks Out of it",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1325.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1325.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Have you ever thought to yourself, \"Man, I love manually inputting new connections and inputs in DB Connect. It makes my life so much more fulfilling!\" Yeah, neither have we. We will show you some simple ways to automate this process by utilizing cron schedules and bash scripts. We will focus on the technical side of automating DB Connect using real world examples to show you how we were able to overcome this hurdle, and how you can become the next Ninja Warrior of DB Connect."
  },
  {
    "Event": ".conf19",
    "Title": "IoT1629 - Deep Dive- How to use Splunk to protect the environment",
    "Track": "Internet of Things",
    "Industry": "ManufacturingNon-Profit",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1629.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IOT1629.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Can Splunk help eliminate environmental risks and save the company money at the same time? The Alabama Department of Environmental Management (ADEM) sets standards defining the amount of Volatile Organic Compounds (VOCs) that can be safely released. Honda Manufacturing of Alabama (HMA) must adhere to those standards and track our compliance within the parameters set forth by the ADEM. Failure to comply with these standards will result in potentially damaging the environment and large fines. HMA uses a Regenerative Thermal Oven (RTO) to burn off pollutants from our paint department, which is controlled by an Allen Bradley Programmable Logic Controller (PLC). Using this PLC and the sensors built into the RTO, we collect individual chamber temperatures, fan speeds, exhaust temperatures, motor winding temperatures, motor vibrations, and fan vibrations. Using these readings and the Splunk Machine Learning Toolkit we try to predict if the temperature will be within the set range. "
  },
  {
    "Event": ".conf19",
    "Title": "DEV2165 - Deep Dive on The New Dashboarding & Content Export Experience",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk Developer Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV2165.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV2165.pdf",
    "SkillLevel": "Intermediate",
    "Description": "This session provides detailed guidance on how to use the new dashboard framework into Splunk apps. It first goes over the basic get started tutorial, which helps developers to build a dashboard in just a few minutes. Then it dives deep into the overall architecture, technology stacks, and individual components that can be customized, including layouts, visualizations, data sources, inputs, event handlers. By attending this session, Splunk app developers will be able to integrate dashboards into the apps flexibly and reliably. This session will also walk through the best practices that can help developers to build the optimal dashboards. This session is targeted to both new Splunk app developers and existing Splunk app developers. For people who already know about the existing Splunk technology stack such as Backbone, SimpleXML, SplunkJS, this session will also go through how to migrate to the new framework. As a bonus, this session will also talk about how to export dashboards as beautiful images and PDFs that 100% matches the original ones! "
  },
  {
    "Event": ".conf19",
    "Title": "SEC1178 - Defend Against Malicious Insiders Using Splunk Enterprise Security, Splunk's Machine Learning Toolkit, and Statistics",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1178.pdf",
    "SkillLevel": "Advanced",
    "Description": "Identifying insider threats is always difficult, and the challenge grows exponentially as the size of a workforce increases and the diversity of normal employee behavior expands. As a result, analysts must get creative to separate routine from malicious activity. This presentation will detail how Lockheed Martin uses machine learning and anomaly detection to identify insider threats. We'll discuss real-world examples of how we leverage Splunk' Machine Learning Toolkit with Summary Indexes, Lookups, Alerts, and Splunk Enterprise Security. "
  },
  {
    "Event": ".conf19",
    "Title": "SEC2129 - Defense Against the Dark Arts- Splunk Edition",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2129.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2129.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Malware infection, lateral movement, data exfiltration, oh my! If you’ve spent any time around the wizarding world of security, you know how much effort goes into preventing dark magic from happening. What if you could use machine learning to stay one step ahead of the adversary? Fasten your seatbelts, because in this talk we will show you how Splunk can utilize machine learning models to take your security detections to the next level. We’ll demonstrate how Splunk's Machine Learning Toolkit can be used to train, validate, and then deploy models to identify anomalies and discover clusters of bad behavior via user-friendly guided workflows—all this while training your models with more data then you’ve ever been able to before. Prepare to leave Las Vegas equipped to incorporate machine learning in your organization’s security detections and jump from reactive to proactive. Mischief managed! "
  },
  {
    "Event": ".conf19",
    "Title": "BA1868 - Delivering real-time sales journey analytics and IT operational health with IT Service Intelligence",
    "Track": "Business Analytics",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BA1868.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BA1868.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Splunk and ITSI are already loved by IT Operations and Application Support teams - but can you help your Business Operations teams love them too?\n\nSplunk’s unique approach to AIOps and monitoring empowers every organization in your enterprise – from IT to Apps to Business – to utilise the same data for multiple use cases and user communities.  Through this session, you’ll learn how to implement a logging framework that delivers across multiple channels.  Key concepts will include ingesting AWS data, ITSI KPI base searches, entity filtering, and APM techniques.  Join this session to learn how to extend your Splunk usage with business KPIs and perform sales journey analytics to grow your list of stakeholders.\nSplunk and ITSI are already loved by IT Operations and Application Support teams - but can you help your Business Operations teams love them too?Splunk and ITSI are already loved by IT Operations and Application Support teams - but can you help your Business Operations teams love them too?Splunk’s unique approach to AIOps and monitoring empowers every organization in your enterprise – from IT to Apps to Business – to utilise the same data for multiple use cases and user communities.  Through this session, you’ll learn how to implement a logging framework that delivers across multiple channels.  Key concepts will include ingesting AWS data, ITSI KPI base searches, entity filtering, and APM techniques.  Join this session to learn how to extend your Splunk usage with business KPIs and perform sales journey analytics to grow your list of stakeholders.Splunk’s unique approach to AIOps and monitoring empowers every organization in your enterprise – from IT to Apps to Business – to utilise the same data for multiple use cases and user communities.  Through this session, you’ll learn how to implement a logging framework that delivers across multiple channels.  Key concepts will include ingesting AWS data, ITSI KPI base searches, entity filtering, and APM techniques.  Join this session to learn how to extend your Splunk usage with business KPIs and perform sales journey analytics to grow your list of stakeholders."
  },
  {
    "Event": ".conf19",
    "Title": "BA1130 - Demonstrating The Value Of A Business Flow Use Case",
    "Track": "Business Analytics",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk Business Flow"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BA1130.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BA1130.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "As organizations look to extend Splunk to new datasets for business process insights, it is crucial to have the right data to work with. However Splunk teams can get stuck demonstrating the value of new solutions without having data available. Some very intelligent approaches to data sampling, data preparation, and query optimization can dramatically accelerate your ability to create value from more data.  In this session, you will learn some of these techniques so you can quickly make a business case and demonstrate the value for Splunk Business Flow in your organization.\nAs organizations look to extend Splunk to new datasets for business process insights, it is crucial to have the right data to work with. However Splunk teams can get stuck demonstrating the value of new solutions without having data available. Some very intelligent approaches to data sampling, data preparation, and query optimization can dramatically accelerate your ability to create value from more data.  In this session, you will learn some of these techniques so you can quickly make a business case and demonstrate the value for Splunk Business Flow in your organization.As organizations look to extend Splunk to new datasets for business process insights, it is crucial to have the right data to work with. However Splunk teams can get stuck demonstrating the value of new solutions without having data available. Some very intelligent approaches to data sampling, data preparation, and query optimization can dramatically accelerate your ability to create value from more data.  In this session, you will learn some of these techniques so you can quickly make a business case and demonstrate the value for Splunk Business Flow in your organization.As organizations look to extend Splunk to new datasets for business process insights, it is crucial to have the right data to work with. However Splunk teams can get stuck demonstrating the value of new solutions without having data available. Some very intelligent approaches to data sampling, data preparation, and query optimization can dramatically accelerate your ability to create value from more data.  In this session, you will learn some of these techniques so you can quickly make a business case and demonstrate the value for Splunk Business Flow in your organization.As organizations look to extend Splunk to new datasets for business process insights, it is crucial to have the right data to work with. However Splunk teams can get stuck demonstrating the value of new solutions without having data available. Some very intelligent approaches to data sampling, data preparation, and query optimization can dramatically accelerate your ability to create value from more data.  In this session, you will learn some of these techniques so you can quickly make a business case and demonstrate the value for Splunk Business Flow in your organization.As organizations look to extend Splunk to new datasets for business process insights, it is crucial to have the right data to work with. However Splunk teams can get stuck demonstrating the value of new solutions without having data available. Some very intelligent approaches to data sampling, data preparation, and query optimization can dramatically accelerate your ability to create value from more data.  In this session, you will learn some of these techniques so you can quickly make a business case and demonstrate the value for Splunk Business Flow in your organization.As organizations look to extend Splunk to new datasets for business process insights, it is crucial to have the right data to work with. However Splunk teams can get stuck demonstrating the value of new solutions without having data available. Some very intelligent approaches to data sampling, data preparation, and query optimization can dramatically accelerate your ability to create value from more data.  In this session, you will learn some of these techniques so you can quickly make a business case and demonstrate the value for Splunk Business Flow in your organization."
  },
  {
    "Event": ".conf19",
    "Title": "SEC2233 - Deploying Splunk Enterprise Security and Splunk Phantom At Scale",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2233.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2233.pdf",
    "SkillLevel": "Advanced",
    "Description": "Ever wondered how to integrate or scale Splunk Enterprise Security (ES) and Splunk Phantom?  Join us as we explore best practices involved in setting up clustered environments for ES and Phantom that yield a highly available and scalable security platform. You will leave this session better able to create scalable ES and Phantom deployments, tools, commands, cheat sheets, and troubleshooting methods at your own organizations."
  },
  {
    "Event": ".conf19",
    "Title": "IoTIOTS2759 - Deployment Models for OT Security",
    "Track": "Internet of Things",
    "Industry": "Energy & Utilities",
    "Products": [
      "Splunk Cloud",
      "Splunk Enterprise Security",
      "Splunk User Behavior Analytics"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoTS2759.mp4",
    "SkillLevel": "Good for all skill levels",
    "Description": "The role of cloud based solutions in traditional IT environments is well understood. Benefits of lower TCO, scalability, reliability, on demand services, and security are all contributing to the rapid adoption of “cloud”.\nBut where does cloud fit in an OT or Industrial environment?\nHow can one reconcile the notion of being “air gapped” for critical infrastructure vs “cloud”?\nThis session will detail the architecture of a typical OT environment and show the potential integration points for how cloud can be safely incorporated into an Industrial environment. Attendees will learn how they can capitalize on their investments in cloud within IT and apply that learning and best practices to their OT needs."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1305 - Detect and Mitigate Insider Threats Using Splunk's Machine Learning Toolkit and Splunk Enterprise Security",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1305.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1305.pdf",
    "SkillLevel": "Advanced",
    "Description": "When is a 20MB email to an external Gmail account dangerous? It all depends on context. Understanding what normal behavior is will reveal whether specific behavior is malicious or ordinary. We’ll walk you through how using Splunk’s Machine Learning Toolkit and Splunk Enterprise Security together provides actionable insight for analysts to improve security. We'll also detail how we caught insider threats in our environment with these tools. "
  },
  {
    "Event": ".conf19",
    "Title": "DEV1139 - Detecting Anomalies in DSP Pipelines Using Real Time Machine Learning",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1139.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV1139.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Machine Learning on the stream is useful for a few important reasons: scenarios where we want to dramatically reduce the resource utilization while providing high fidelity results and in use cases where we need algorithms to adapt to changing patterns and drifts in distributions real time.  In this talk, we will discuss ongoing work in the area of streaming machine learning and show how we leverage Flink and DSP to build real time machine learning systems that allow us to perform adaptive thresholding and anomaly detection online.  As an application of these principles, we will showcase how real time machine learning is used to detect anomalies in DSP pipelines.  The talk will introduce relevant background in streaming machine learning as well as the problem of anomaly detection on Kubernetes logs. Machine Learning on the stream is useful for a few important reasons: scenarios where we want to dramatically reduce the resource utilization while providing high fidelity results and in use cases where we need algorithms to adapt to changing patterns and drifts in distributions real time.Machine Learning on the stream is useful for a few important reasons: scenarios where we want to dramatically reduce the resource utilization while providing high fidelity results and in use cases where we need algorithms to adapt to changing patterns and drifts in distributions real time.Machine Learning on the stream is useful for a few important reasons: scenarios where we want to dramatically reduce the resource utilization while providing high fidelity results and in use cases where we need algorithms to adapt to changing patterns and drifts in distributions real time.Machine Learning on the stream is useful for a few important reasons: scenarios where we want to dramatically reduce the resource utilization while providing high fidelity results and in use cases where we need algorithms to adapt to changing patterns and drifts in distributions real time.In this talk, we will discuss ongoing work in the area of streaming machine learning and show how we leverage Flink and DSP to build real time machine learning systems that allow us to perform adaptive thresholding and anomaly detection online.In this talk, we will discuss ongoing work in the area of streaming machine learning and show how we leverage Flink and DSP to build real time machine learning systems that allow us to perform adaptive thresholding and anomaly detection online.In this talk, we will discuss ongoing work in the area of streaming machine learning and show how we leverage Flink and DSP to build real time machine learning systems that allow us to perform adaptive thresholding and anomaly detection online.In this talk, we will discuss ongoing work in the area of streaming machine learning and show how we leverage Flink and DSP to build real time machine learning systems that allow us to perform adaptive thresholding and anomaly detection online.As an application of these principles, we will showcase how real time machine learning is used to detect anomalies in DSP pipelines.As an application of these principles, we will showcase how real time machine learning is used to detect anomalies in DSP pipelines.As an application of these principles, we will showcase how real time machine learning is used to detect anomalies in DSP pipelines.As an application of these principles, we will showcase how real time machine learning is used to detect anomalies in DSP pipelines.The talk will introduce relevant background in streaming machine learning as well as the problem of anomaly detection on Kubernetes logs.The talk will introduce relevant background in streaming machine learning as well as the problem of anomaly detection on Kubernetes logs.The talk will introduce relevant background in streaming machine learning as well as the problem of anomaly detection on Kubernetes logs.The talk will introduce relevant background in streaming machine learning as well as the problem of anomaly detection on Kubernetes logs."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1620 - Develop Endpoint Detection Superpowers with Sysmon + Splunk",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "Threat actors often enter organizations via the endpoint, yet enterprises continue to suffer the consequences from not logging their endpoints. This session will show you how to pair using Sysmon in your environment to monitor endpoints with the MITRE ATT&CK™ framework to detect more threats. Through a threat hunting demonstration created from real scenarios, this session will contextualize and guide your future investigations along all covered ATT&CK™ techniques."
  },
  {
    "Event": ".conf19",
    "Title": "IT1296 - DevSplunkOps- Making Splunk the Single View for the SDLC",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1296.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1296.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Are you tired of looking in multiple areas for different parts of the DevOps cycle? Learn how to use Splunk to gather and display all your metrics in a single place. We will show you how we Splunked Azure DevOps (TFS), SonarQube, GitLab, Service Now, and Slack to provide a single view for the Plan, Build, and Run steps of a team, and how that compares to other teams."
  },
  {
    "Event": ".conf19",
    "Title": "SECS2917 - Differentiating Evil from Benign in the Normally Abnormal World",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security",
      "Splunk IT Service Intelligence",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SECS2917.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SECS2917.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Have you ever been positive you had found evil, only to realize it was normal after hours of triage and work? We have all heard and love “KNOW NORMAL FIND EVIL,” but how hard is it to actually know normal? The MITRE ATT&CK Framework gives defenders a better map to “find evil,” but how can this framework be used to “know normal”?\n\nRick will discuss how knowing normal in a world of abnormal is harder than one thinks, and how addressing the actual root cause of evil can improve the technology industry as a whole.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FNC2750 - Digging Deep into Disk Diagnoses",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "This talk offers Splunk admin insights into the impact of storage constraints on Splunk search performance. A performant environment relies on sufficient resources for CPU, RAM and storage. In the field, many medium to large Splunk environments are throttled by poor storage performance to support search, indexing and SmartStore loads. We'll use the  Monitoring Console to gather and analyze metrics external to Splunk that indicate storage contention and guides the admin on remediation options."
  },
  {
    "Event": ".conf19",
    "Title": "IT2095 - Distributed Tracing in Splunk- Get end-to-end visibility into application performance with Splunk and OpenTracing ",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT2095.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT2095.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Are you addressing the challenges of gaining visibility into a distributed microservices environment? Is your organization considering using distributed tracing to augment your APM capabilities? Have you heard of OpenTracing and want to learn what capabilities it gives you and how to get started? Come learn about the OpenTracing project and how you can use it with Splunk to get a complete picture of your application environment using logs, metrics, and traces. We'll go from the basics of what the project is to how to get started integrating with Splunk. We'll also review an example of a large telco customer to see how they got started with OpenTracing and how they rolled it out in their application environments."
  },
  {
    "Event": ".conf19",
    "Title": "IT1339 - Distributed Tracing via Structured Logging Using Splunk",
    "Track": "IT Operations",
    "Industry": "Technology",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1339.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1339.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Is your team spending too much time investigating common business and engineering questions due to a lack of transparency into your rapidly growing distributed system? Is logging for your microservices a haphazard mess? Is isolating a single event workflow throughout the pipeline just TOO MUCH WORK? Learn how we utilized Splunk to simplify troubleshooting distributed systems at Bloomberg. In this session, we will: Walk through some custom components that provide transparency into complex distributed microservices; show how to utilize them to find and isolate a single event workflow; explain how to write highly-optimized Splunk SPL queries to answer various business workflow questions; and demonstrate how to present the results using advanced visualizations.\n"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1705 - Diving into Splunk Phantom's Overlooked Features",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1705.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1705.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Whether you're a new or experienced Splunk Phantom user, you'll learn from the high-value, often overlooked features we discuss in this session. We'll showcase some of Phantom's most overlooked valuable features, as well as experienced users' top ranked features. Join us to learn more about how you can optimize your use of Splunk’s SOAR (Security Orchestration Automation & Response) platform."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1104 - Don't Blow Your Budget Fighting Fraud; Orchestrate and Automate Instead",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Financial ServicesNot industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1104.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1104.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Manual sorting through spreadsheets, disparate applications, and scattered data sources to conduct link analysis for a fraud investigation is both painful and ineffective. There must be a better way, right? In this session we'll use Splunk Enterprise and Splunk Phantom to automate repeatable fraud investigation tasks, which will save your team time and better protect your assets from the bad guys."
  },
  {
    "Event": ".conf19",
    "Title": "IT1433 - Down in the Weeds, Up in the Cloud- IT Ops",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1433.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1433.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Analytics Workspaces, Application Insights, Azure Monitor, O365 Admin Centers, just a few of the many Microsoft tools required to monitor and interrogate information from Azure & Office 365. Getting the valuable intel and insights from your Azure and Office 365 environments should be easy, and effortless. Throw on your Splunk hoodie and join Ryan as we Splunk our way through all things IT in Azure, Office365 as part of the Microsoft-as-a-Service world! Infrastructure management, real-time billing, SLA monitoring, capacity planning, all quick, all easy, all powerful with Splunk!"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1432 - Down in the Weeds, Up in the Cloud- Security",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1432.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1432.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "This is one of multiple sessions in a series at .conf this year focused on getting valuable intel and insights from your Azure and Office 365 environments. Throw on your hoodie and join Ryan as we Splunk our way through all things Azure, Office365, security, compliance, and visibility in the Microsoft-as-a-Service world.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1373 - Drive effective Splunk enablement and adoption & build user cohorts using internal logs and machine learning ",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1373.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1373.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Wouldn’t it be a great if we had Splunk’s version of Moneyball; an application where everyone comes out ahead by leveraging data to drive effective Splunk enablement and adoption? Splunk’s internal logs have a wealth of information about how Splunk is being used within your organization. Let’s take drinking the “Splunk Champagne” to the next level by applying statistics and machine learning to Splunk’s internal logs! This session will cover segmenting users based on their search profiles - number of searches run, average response times, and recency of searches executed, among other criteria. We’ll use techniques such as clustering to classify users from novice to experts, and use TF-IDF and text analytics techniques to understand commands used in search strings. Enriching this data with completed and planned Splunk Education courses, lunch & learn sessions, and other training activities will enable your users to achieve the Splunk Ninja status they’re looking for! "
  },
  {
    "Event": ".conf19",
    "Title": "FN1298 - Driving Adoption of Splunk in Your Enterprise",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1298.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1298.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "So, you have Splunk in your enterprise but people aren't using it. What do you do? After working with many organizations in financial services, we have heard the response \"yes, we use Splunk.\" However, there is a large gap between the organizations that have a Splunk license and those that are truly using Splunk. What is the key to successful adoption of Splunk as an enterprise tool? In this session hear how FIS has driven an uptake\\in Splunk adoption across multiple business lines and external clients. Using Splunk, FIS have supported off-prescription solutions including system parameter inquiry, operator fraud, financial invoicing, client conference data analytics, staff resource allocations, and status reporting."
  },
  {
    "Event": ".conf19",
    "Title": "BA1191 - Driving More Sales at Deutsche Bahn-  DB Systel’s Journey Mapping With Splunk Business Flow",
    "Track": "Business Analytics",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Business Flow"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BA1191.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BA1191.pdf",
    "SkillLevel": "Beginner",
    "Description": "As one of the largest transportation providers in the world, Deutsche Bahn is eager to understand the customer experience of its passengers interacting with its agents. Driving greater sales requires highly efficient engagement with call center agents. However, efficiencies can be hard to understand and measure with so many call center interactions. DB Systel Gmbh, the IT services arm of Deutsche Bahn, uses Splunk Business Flow to quickly map thousands of customer experiences to understand opportunities for improving efficiencies. In this session, learn how Deutsche Bahn is able to operate a more efficient call center and drive greater passenger sales using the fast and flexible insights of Splunk Business Flow."
  },
  {
    "Event": ".conf19",
    "Title": "SEC2179 - Dude, Where’s My Log- The Unknown Logging Gaps in Your Environment, Why You Didn't Detect that Pentest, and How Splunk Can Help ",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2179.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2179.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Failure to log everything needed for maximum visibility in your environment can leave huge gaps in your ability to remediate threats. But running an enterprise-level logging program can be difficult: how do you know if you're logging everything necessary to detect threats? Are all of your technologies configured to send the right logs? Are they all logging to Splunk? In this session we will help you answer these and other critical questions of your logging program, which will ultimately help you remediate issues and better use log analysis to mitigate threats."
  },
  {
    "Event": ".conf19",
    "Title": "IT2951 - Effective Strategies for Monitoring Docker and Kubernetes Environments",
    "Track": "IT Operations",
    "Products": [],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT2951.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT2951.pdf",
    "Description": "Containerization and the adoption of microservices have dramatically accelerated software innovation. However, containerized environments behave so differently compared to traditional ones that they present new observability challenges, and require new monitoring strategies and tools. What are those challenges? In this talk, we will discuss three – the explosion in scale, massive component churn caused by continuous delivery, and the need for infrastructure-to-application correlation –  then share effective strategies for overcoming them in a number of practical examples.\nContainerization and the adoption of microservices have dramatically accelerated software innovation. However, containerized environments behave so differently compared to traditional ones that they present new observability challenges, and require new monitoring strategies and tools. What are those challenges? In this talk, we will discuss three – the explosion in scale, massive component churn caused by continuous delivery, and the need for infrastructure-to-application correlation –  then share effective strategies for overcoming them in a number of practical examples.Containerization and the adoption of microservices have dramatically accelerated software innovation. However, containerized environments behave so differently compared to traditional ones that they present new observability challenges, and require new monitoring strategies and tools. What are those challenges? In this talk, we will discuss three – the explosion in scale, massive component churn caused by continuous delivery, and the need for infrastructure-to-application correlation –  then share effective strategies for overcoming them in a number of practical examples.Containerization and the adoption of microservices have dramatically accelerated software innovation. However, containerized environments behave so differently compared to traditional ones that they present new observability challenges, and require new monitoring strategies and tools. What are those challenges? In this talk, we will discuss three – the explosion in scale, massive component churn caused by continuous delivery, and the need for infrastructure-to-application correlation –  then share effective strategies for overcoming them in a number of practical examples.Containerization and the adoption of microservices have dramatically accelerated software innovation. However, containerized environments behave so differently compared to traditional ones that they present new observability challenges, and require new monitoring strategies and tools. What are those challenges? In this talk, we will discuss three – the explosion in scale, massive component churn caused by continuous delivery, and the need for infrastructure-to-application correlation –  then share effective strategies for overcoming them in a number of practical examples.Containerization and the adoption of microservices have dramatically accelerated software innovation. However, containerized environments behave so differently compared to traditional ones that they present new observability challenges, and require new monitoring strategies and tools. What are those challenges? In this talk, we will discuss three – the explosion in scale, massive component churn caused by continuous delivery, and the need for infrastructure-to-application correlation –  then share effective strategies for overcoming them in a number of practical examples."
  },
  {
    "Event": ".conf19",
    "Title": "FN1455 - ELK vs. Splunk",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1455.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1455.pdf",
    "SkillLevel": "Advanced",
    "Description": "This session will present a side-by-side, high-level comparison of the features, assumptions, and architectures of a Splunk and ELK deployment."
  },
  {
    "Event": ".conf19",
    "Title": "FND1956 - Empathy- The Forgotten Data Point",
    "Track": "Foundations/Platform",
    "Industry": "Diversity & Inclusion",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FND1956.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FND1956.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "For those of us who work in the technology sector and adjacent disciplines, our technical skills are largely seen as our most valuable; the programming languages we know, what we’ve accomplished with those languages and so on. While our technical skills are usually our ticket in, they have little or no impact on how happy we are at work. For example, the correlation between your proficiency in Splunk and your commitment to your team is negligible. As life at work evolves, we see the rising need for an alternative data point: empathy. With the help of sentiment analysis, our talk will highlight how the presence or absence of empathy impacts our working lives. And, better yet, how we can change the course of a team, project, and our own perception by leveraging empathy in a conscious way."
  },
  {
    "Event": ".conf19",
    "Title": "IoT2863 - Energy Insights- Developing an Energy Transformation Framework In Manufacturing with GrayMatter and Splunk at Continental Automotive",
    "Track": "Internet of Things",
    "Products": [],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT2863.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IOT2863.pdf",
    "Description": " Intersecting energy asset data with production and operational data opens new doors to better visibility, energy savings and meeting green standards. In this session you’ll learn how Continental Automotive eliminated energy waste, reduced costs and lowered emissions levels by breaking down energy consumption and operational data silos on the factory floor.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1366 - Enhanced Anomaly Detection- Join T-Mobile and Splunk as we Deep Dive an Enterprise-IT Operational Use Case",
    "Track": "Foundations/Platform",
    "Industry": "Technology",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1366.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1366.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Is confidence in your ability to disassociate real alerts from noise at an all-time low? Are alerts becoming ineffective because they’re excessive? Many organizations across many industries wrestle with these questions daily. Even so, the prevalence of alert noise persists. Join us to understand how T-Mobile is using Splunk Enterprise with the Splunk Machine Learning Toolkit to identify and narrow the point of origin of service-impacting events across our suite of enterprise applications. Using the DensityFunction algorithm to highlight anomalous behavior, we’re able to focus an investigation on a small subset of the applications which in turn leads to faster resolution of the issues we’re confronted with.    Is confidence in your ability to disassociate real alerts from noise at an all-time low? Are alerts becoming ineffective because they’re excessive? Many organizations across many industries wrestle with these questions daily. Even so, the prevalence of alert noise persists. Join us to understand how T-Mobile is using Splunk Enterprise with the Splunk Machine Learning Toolkit to identify and narrow the point of origin of service-impacting events across our suite of enterprise applications. Using the DensityFunction algorithm to highlight anomalous behavior, we’re able to focus an investigation on a small subset of the applications which in turn leads to faster resolution of the issues we’re confronted with.Is confidence in your ability to disassociate real alerts from noise at an all-time low? Are alerts becoming ineffective because they’re excessive? Many organizations across many industries wrestle with these questions daily. Even so, the prevalence of alert noise persists. Join us to understand how T-Mobile is using Splunk Enterprise with the Splunk Machine Learning Toolkit to identify and narrow the point of origin of service-impacting events across our suite of enterprise applications. Using the DensityFunction algorithm to highlight anomalous behavior, we’re able to focus an investigation on a small subset of the applications which in turn leads to faster resolution of the issues we’re confronted with.Is confidence in your ability to disassociate real alerts from noise at an all-time low? Are alerts becoming ineffective because they’re excessive? Many organizations across many industries wrestle with these questions daily. Even so, the prevalence of alert noise persists. Join us to understand how T-Mobile is using Splunk Enterprise with the Splunk Machine Learning Toolkit to identify and narrow the point of origin of service-impacting events across our suite of enterprise applications. Using the DensityFunction algorithm to highlight anomalous behavior, we’re able to focus an investigation on a small subset of the applications which in turn leads to faster resolution of the issues we’re confronted with.Is confidence in your ability to disassociate real alerts from noise at an all-time low? Are alerts becoming ineffective because they’re excessive? Many organizations across many industries wrestle with these questions daily. Even so, the prevalence of alert noise persists. Join us to understand how T-Mobile is using Splunk Enterprise with the Splunk Machine Learning Toolkit to identify and narrow the point of origin of service-impacting events across our suite of enterprise applications. Using the DensityFunction algorithm to highlight anomalous behavior, we’re able to focus an investigation on a small subset of the applications which in turn leads to faster resolution of the issues we’re confronted with.Is confidence in your ability to disassociate real alerts from noise at an all-time low? Are alerts becoming ineffective because they’re excessive? Many organizations across many industries wrestle with these questions daily. Even so, the prevalence of alert noise persists. Join us to understand how T-Mobile is using Splunk Enterprise with the Splunk Machine Learning Toolkit to identify and narrow the point of origin of service-impacting events across our suite of enterprise applications. Using the DensityFunction algorithm to highlight anomalous behavior, we’re able to focus an investigation on a small subset of the applications which in turn leads to faster resolution of the issues we’re confronted with.  "
  },
  {
    "Event": ".conf19",
    "Title": "FND2072 - Enhancing Diversity in Tech through Employee Resource Groups - Our BEAMs Journey",
    "Track": "Foundations/Platform",
    "Industry": "Diversity & Inclusion",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FND2072.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FND2072.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Splunk’s support and promotion of diversity and inclusion led to the establishment of their most recent Employee Resource Group (ERG) for black employees called BEAMS. Black Employees and Mentors, BEAMS, officially launched February 2019, and has provided a platform for underrepresented employees to make an impact within the Splunk community and beyond. Join members of BEAMS for this discussion about changing the narrative for Black employees in the tech industry, and learn how you can thoughtfully provoke the same within your organization. Come learn from the experiences of BEAMS and their open and bold mindset, how BEAMS navigates challenges associated with creating change, promoting equality within a large organization, and going beyond the office walls, and how they support partnerships with youth education organizations. Be prepared to leave with thoughtful anecdotes and an action item list for your organization.\n"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1544 - Enterprise Security Biology III- Dissecting the Incident Management Framework",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1544.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1544.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Splunk's Incident Management Framework is used extensively in support of the notable event creation, and it serves as a bridge that associates the Risk, Asset & Identity, and Threat frameworks together. In this session we will discuss how incident management functions, what occurs behind the scenes to prepare events that are correlated, and how to present correlated events to analysts. Attendees will leave this talk with a greater understanding of the Incident Management Framework and methods to work more effectively with it within Splunk Enterprise Security."
  },
  {
    "Event": ".conf19",
    "Title": "SEC2274 - Enterprise Security for an Internet Service Provider- How Splunk Enterprise Security Changed the Way We Work and Changed the Organization",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Communications",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk Machine Learning Toolkit"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2274.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2274.pdf",
    "SkillLevel": "Intermediate",
    "Description": "20+ million subscribers, 290PB network traffic daily, and tens of millions of IoT, IPTV and ICT devices—a bigger network means more attacks from all over the world. Learn how SK Broadband, the biggest telco/ISP provider in South Korea, leverages Splunk Enterprise Security (ES) to protect their subscribers from countless DDoS and malware attacks. We will cover detailed use cases for analyzing a high volume of data—500 million security events over 7 billion logs per day—as well as how we met a high bar of operational efficiency by customizing our ES deployment.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT1697 - Every Minute Counts- Integrating Splunk and VictorOps to Accelerate Incident Response",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence",
      "VictorOps"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1697.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1697.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Splunk/VictorOps allows your data to talk to people, and your people to talk to data, all while improving MTTR/MTTI. You’re already using Splunk today to get cool dashboards, ask questions of your data with search, and trigger important alerts based on data that’s of interest to you. But, just because alerts are important doesn’t mean they are for everyone. And for those who should respond to the alert, it’s important to get the alert into their hands immediately. Transitioning to a modern NOC, or “New Operations Center,” Splunk and VictorOps enable teams to collaboratively solve issues by providing on-call users with actionable alerts that allow for smarter investigation, faster mobilization, and lower mean time to resolution. In this session we’ll take a look under the hood at different Splunk and VictorOps integrations."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1709 - Examining Splunk Phantom's Architecture",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1709.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1709.pdf",
    "SkillLevel": "Advanced",
    "Description": "Want to learn more about Splunk Phantom's platform architecture? Join us in this session for an in-depth technical review of all key processes, including ingestion, automation, action execution, health monitoring, the data store, and more.  This session will give experienced users a much deeper understanding of the technology behind Splunk’s SOAR (Security Orchestration Automation & Response) platform."
  },
  {
    "Event": ".conf19",
    "Title": "IT1589 - “Excuse Me! Your Microphone isn’t on” – supporting 400 teaching spaces with Splunk & ITSI – A case study",
    "Track": "IT Operations",
    "Industry": "Higher Education",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1589.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1589.pdf",
    "SkillLevel": "Beginner",
    "Description": "Do you remember the time when you had to attend lectures in person? Monash University has extended teaching hours and stopped repeating lectures. To provide flexibility to students and release pressure on finite campus facilities, lectures are now live streamed and are guaranteed to cover 100% of the classroom content. We are using Splunk and IT Service Intelligence (ITSI) to help monitor live streaming of lectures. It is impossible to manually monitor hundreds of teaching spaces in real-time and keep track of lecture schedules, the status of AV devices, microphones, and audio signals. Come and see how we at Monash University use Splunk ITSI glass tables to rule them all! Using ITSI we get real-time insights into service health and entity-level alerting based on ITSI thresholds."
  },
  {
    "Event": ".conf19",
    "Title": "FN2103 - Exploratory Data Analysis on Aviation Safety Data",
    "Track": "Foundations/Platform",
    "Industry": "Aerospace & DefenseNon-Profit",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2103.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2103.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "It’s a bird, it’s a plane. Yes, it’s a plane! Let’s go for a flight into the skies of aviation data and the concepts and tools that make aviation data analytics easy. I’ve been capturing this open data for nearly two years, and I’ve been able to unravel some insights based on four projects. Two projects focus on processing Canadian/U.S. safety reports, while the other two are processing data captured and logged from a radio receiver made from a Raspberry Pi. Data Science is offering very exciting careers and providing an important competitive differentiator, which is why we’ll be reviewing several statistical processing techniques made possible by the power of the Machine Learning Toolkit. We also will cover exploratory data analysis tools built into the Search Processing Language. Before we approach for landing, we’ll show everything under the hood so that everybody can understand the things that make it fly. This is both a technical deep dive as-well-as a practical usage walkthrough.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN2514 - Fast and scalable replication of knowledge bundles in Splunk Enterprise",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2514.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2514.pdf",
    "SkillLevel": "Intermediate",
    "Description": "This session will highlight a new strategy for fast and effective replication of knowledge bundles, resulting in up-to-date search results and a significant reduction in WAN bandwidth usage. Knowledge bundles include the knowledge objects that a search-head distributes to search peers so that they can process a distributed search. Replication of these bundles to search peers can be slow for large deployments and also consumes significant WAN bandwidth, especially in multi-site deployments. This new replication option in Splunk Enterprise accelerates search access to recent data and reduces WAN bandwidth consumption."
  },
  {
    "Event": ".conf19",
    "Title": "SECS2655 - Fast, Forward and Focused ",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Cloud",
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "Technology is constantly transforming, creating new possibilities and risks in the business world. Shark Tank star, HG’s Founder & CEO, Robert Herjavec, will explore the evolving cyber landscape and explain how to best position your business risks at the Board of Directors level. There is no light at the end of the tunnel when it comes to cyber warfare. Robert will share his recommendations on how to adapt, transform and scale your business strategies and Splunk for Security operations to face this battle head on."
  },
  {
    "Event": ".conf19",
    "Title": "FN1588 - Feeding the Beast- The fine art of self-service knowledge object and search management",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1588.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1588.pdf",
    "SkillLevel": "Intermediate",
    "Description": "The joys, freedoms, and advantages of running a Splunk “data democracy” at scale are numerous and well documented. However, as with many aspects of life, where there is huge upside a dark downside often lurks. How do I mange thousands of knowledge objects, or help users make their searches faster and more efficient, or deal with a myriad of small, but still time-consuming operational challenges? For answers, come and listen to this presentation by Atlassian, the world leader in software collaboration tools. Hear how the Atlassian team not only tackled such problems head on, but also how they showed users how to manage these issues in the most effective and innovative ways. From clearing out unused dashboards, to scheduling searches and lookups, to exposing the performance of searches in a developer-friendly way, we have tips, tricks, and advice for all comers, irrespective of where you are on your Splunk journey.\n"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1250 - Feed the Beast! Use Splunk to Address APTs At Speed and Scale By Utilizing Endpoint-Centric Threat Hunting Uses Cases ",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1250.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1250.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Going beyond basic perimeter defense, Threat Hunting cuts through the noise of endpoint telemetry and anti-virus data to find nation-state level Advanced Persistent Threats (APTs) that hide below the alert threshold. We will demonstrate, through 4 hunt analytic use cases, how to overcome the legacy challenge of relying on Packet Capture (PCAP) data to detect adversaries, highlighting the need to transform Hunt operations by combining Endpoint Detection and Response (EDR) telemetry data with knowledge of APT behavior to find hidden adversaries. This talk will provide a framework for planning and executing hunts, demonstrate why focusing on EDR telemetry data can add additional value over and beyond traditional network data, and how to strengthen hunting through a Purple Team approach.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1003 - Fields, Indexed Tokens and You",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "SkillLevel": "Advanced",
    "Description": "Splunk software does many things to make your searches run fast. Most importantly, Splunk has to narrow down the set of potentially matching events. The fewer events Splunk has to scan, the faster your search will run. In this session we will explore how Splunk software uses fields and indexed tokens to achieve this, and how you can leverage them to your advantage. You will learn how to detect optimization potential in your searches and how to make meaningful changes. Additionally, we will cover how common configurations can have great impact on search performance. "
  },
  {
    "Event": ".conf19",
    "Title": "IT2219 - Fighting Fraud at Mastercard Thousands of Times a Second – A Use Case of Application Monitoring",
    "Track": "IT Operations",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT2219.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT2219.pdf",
    "SkillLevel": "Intermediate",
    "Description": "At Mastercard, we assist our banks in making sure their cardholders are protected from fraud attacks on their cards. We process tens of thousands of transactions per second all within tens of milliseconds. When our platform has outages (big and little), we need to understand how the problem occurred so we can prevent future issues and provide the best service possible. I will provide a detailed use case of a past incident, our challenges and how Splunk helped us find the culprit and make changes to our platform and processes to prevent the issue in the future."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1952 - Finding Evil Is Never An Accident- How to Hunt in BOTS",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1952.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1952.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "To secure the modern endpoint, you need sufficient data, the right visibility and analysis, and the technology necesary to stop an intrusion. We will leverage BOTSv4 data in this session to help you test and validate Splunk use cases related to hunting threats using endpoint data. We’ll cover several real world case studies as described in MITRE ATT&CK™, and we will simulate adversary groups by executing a single Atomic test and building an elaborate chain reaction. We will then show you in Splunk how to confirm your data quality and confirm you have what you need to detect and evict an adversary from your environment. We will demonstrate practical hunt techniques using BOTSv4 data and how to raise the flag when data is missing or is not required."
  },
  {
    "Event": ".conf19",
    "Title": "2623 - First Timer Orientation",
    "Products": [],
    "Description": "For new .conf attendees (or curious alumni), this informational session provides insight and best practices on how to get the most out of your .conf experience, from navigating the physical campus and virtual tools to scheduling those four perfect days in Las Vegas. Add this session to your personal agenda and join your fellow .conf n00bs on Monday just before the Welcome Soiree. We look forward to meeting you! "
  },
  {
    "Event": ".conf19",
    "Title": "IT2184 - Fiserv Turns ITSI Up to 11 – with Powerful KPIs, Better Performance, & Custom Alerts",
    "Track": "IT Operations",
    "Industry": "Financial ServicesTechnologyNot industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT2184.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT2184.pdf",
    "SkillLevel": "Intermediate",
    "Description": "ITSI provides a platform to turn your never-ending stream of app and infrastructure data into manageable KPIs that you can act on, but how do you manage the number of events ITSI detects? What if you need to extend ITSI with additional capabilities in order to merge your alerts with your support team's response processes? We will turn ITSI up to 11 by adding the concept of \"platforms\" as a collection of related services, support reoccurring maintenance windows, and generate rich alerts that include information such as which entities are impacted and what the KPI values are. These concepts are combined so that all the problems in your platform generate a single alert with alert text rich enough that that your support teams can respond without opening ITSI, and we will do it all with just SPL."
  },
  {
    "Event": ".conf19",
    "Title": "FN1137 - Forecasting Disk Usage with Machine Learning – So easy, even a cave-person can do it!",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1137.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1137.pdf",
    "SkillLevel": "Intermediate",
    "Description": "This presentation will walk users through how to use the machine learning toolkit to accurately forecast disk usage across their entire environment, giving them the exact day, month, and year when a server will run out of disk space. No more being awakened at 3:00 am for a bridge call due to a drive running out of disk. This process also can be used by capacity planning teams to select a future date and get a clear view of capacity across the business for all servers. Using machine learning to remove tech debt in an organization does not require a data scientist. You can do it if you have the right server metrics and the MLTK installed. "
  },
  {
    "Event": ".conf19",
    "Title": "FN1922 - From 0 to 6 Terabytes Real Quick- Cox Automotive’s Fast and Furious Splunk Adoption",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1922.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1922.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "How do you control a race car with unlimited horsepower and make sure it doesn’t go off the track? With over 25 unique business affiliates within our enterprise and hundreds of application, engineering, and business teams driving rapid adoption, Cox Automotive’s Splunk Cloud deployment is our very own race car. Come sit in the passenger seat as we recap our ride, which is filled with explosive data ingest rates, unpredictable search slowdowns, out of control data drifts, and rapidly increasing end user requirements. Let our experiences and in-house solutions help you navigate the potholes you will encounter when dealing with the seemingly unstoppable growth rate of an enterprise-level Splunk distributed deployment."
  },
  {
    "Event": ".conf19",
    "Title": "BA1853 - From a dataful life to unlocking difference- building a data fabric to drive customer and business value in just 60 days",
    "Track": "Business Analytics",
    "Industry": "Communications",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "In the big wide world of possibilities with Splunk, are you ever wondered how to make Splunk a business and customer experience insight and action engine, fast? See how we prioritized, integrated, and enabled our organization to make Splunk the data fabric to drive significant outcomes in customer experience and business outcomes across Belong in just 60 days. Who are we? Belong is a digital-first telco delivering broadband and mobile services across Australia. As the company launched into a new brand and new offerings powered by cloud-native platforms, agile ways of working, highly automated delivery, and a diverse team, Belong is using Splunk Enterprise and ITSI to create end-to-end and top-to-bottom customer and business insights and actions.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1628 - From hell to heaven- growing Splunk Cloud from 1TB to 13TB a day",
    "Track": "Foundations/Platform",
    "Industry": "Technology",
    "Products": [
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1628.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1628.pdf",
    "SkillLevel": "Intermediate",
    "Description": "The true story of how Paddy Power Betfair, the international, multi-channel sports betting and gaming operator went from sending two terabytes to 13 terabytes of data to Splunk each day. The massive, self-inflicted performance issues we encountered originated from not tuning to Splunk to keep pace. This talk explains all the great work that was undertaken to properly fine tune Splunk back into shape."
  },
  {
    "Event": ".conf19",
    "Title": "FNS2586 - From Zero to Fully Online with Nutanix HCI & App Management",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FNS2586.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FNS2586.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "You want to focus on gathering insights, not setting up your Splunk deployment. Nutanix Enterprise Cloud, built on industry-leading hyperconverged infrastructure (HCI), ensures you can be up and running fast, easily adapt to growth, and removes the risks of infrastructure challenges when moving from pilot to production. Attend this session to see how easy it is to have the cloud-experience for your on-prem Splunk infrastructure and flex to the public cloud when needed. You’ll see a demo of how you can use built-in application lifecycle management and blueprints to simplify and speed managing Splunk Enterprise."
  },
  {
    "Event": ".conf19",
    "Title": "IoT1632 - Gatwick Airport- From Road to Runway, how we embarked more users on Splunk",
    "Track": "Internet of Things",
    "Industry": "Travel & Transportation",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1632.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IOT1632.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Gatwick Airport has become a name synonymous with Splunk, and since we first embraced the world of Splunking there has been no slowing down with our innovative ways of using every data feed and technique available to gain insightful business analytics to improve operation. Join us as we take you through our new and exciting use cases. We will show you how we provide our key operations, from road to runway, with all the data insights they could ever need, adding key resilience to existing systems and combining our Cloud and Enterprise environments. All while we are utilizing inventive CSS techniques to give the impression of bespoke individual systems. We are getting more end users onto the Splunk platform without them even realizing their using Splunk at all."
  },
  {
    "Event": ".conf19",
    "Title": "FNS2585 - Get Both High Performance & Ease of Management for Splunk with Nutanix",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FNS2585.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FNS2585.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Explore the benefits of running Splunk on Nutanix Enterprise Cloud, both taking the complexity out of managing the infrastructure and ensuring the performance you need so that your experts can spend more time extracting insight from data. Attend this webinar to gain valuable insights into how to architect the best environment for Splunk Enterprise with deployments that can scale to handle increasing data ingest rates and ever-increasing expectations for high performance. See a demo and learn useful tips on how Nutanix hyperconverged infrastructure (HCI) based solutions allow you to take full advantage of server virtualization without the limitations of other approaches and add in the speed and flexibility of the public cloud."
  },
  {
    "Event": ".conf19",
    "Title": "FN2192 - Get those spreadsheets into Splunk!",
    "Track": "Foundations/Platform",
    "Industry": "Non-ProfitNot industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2192.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2192.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Unfortunately not everyone is living the data dream of using connected toaster to send webhooks to Splunk in nicely formatted JSON. Believe it or not, there are still organizations, especially nonprofits, using spreadsheets to manually record their important data. Spreadsheets aren't going to disappear overnight, so Splunk shouldn't ignore the value of spreadsheet data. Learn good practices on how to store and index spreadsheets to Splunk using the Google Drive app.\n"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1538 - Getting Started with Risk-Based Alerting and MITRE",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1538.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1538.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Risk-based alerting is gaining traction in the SOC: by using multiple-lower fidelity searches to yield higher-fidelity investigations, it allows analysts to rapidly prioritize investigations, correlate “risk objects” between alerts, identify gaps in monitoring, and generally understand attack narratives. We'll discuss the first steps needed to transition from the traditional one-to-one ticket investigation model to this holistic approach, i.e. how risk-based alerting works, a description of prerequisites, and dashboard optimization.  We will also discuss how to start building a comprehensive search inventory based on Splunk analytics, MITRE, and your own threat intelligence."
  },
  {
    "Event": ".conf19",
    "Title": "FN1746 - Getting the Most Out of Splunk Natural Language Platform",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1746.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1746.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Do you get overwhelmed with writing Splunk queries for your users all the time? Wouldn’t a world where your executives and other casual Splunk users could get the answers they want without knowing SPL be nice? With Splunk Natural Language Platform (NLP), your users can talk to their data–literally–and get their answers right from their smartphones or AppleTV.\nIn this session, we’ll walk through how to set up NLP, create user intents, and optimize the accuracy of the results. We’ll also show you how to re-create the NLP experiences seen onstage and other tips for how to get the most out of Splunk NLP for your organization.\n"
  },
  {
    "Event": ".conf19",
    "Title": "DEV1252 - Getting your FIX- Developing an add-on for the Financial Information eXchange Protocol",
    "Track": "Developer",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1252.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV1252.pdf",
    "SkillLevel": "Intermediate",
    "Description": "The Financial Information eXchange (FIX) Protocol is one of the most pervasive electronic communications protocols used for real-time exchange of information related to securities transaction and market data. The protocol is used to move massive quantities of money per day.\n\n With over 1600 fields (tags) and 115 message types, the protocol presents some unique challenges to consider when developing an add-on for Splunk. Come see and discuss the protocol and how Splunk can help you make sense of the data it contains to improve your trading, business analytic, security, fraud, and compliance operations.\nThe Financial Information eXchange (FIX) Protocol is one of the most pervasive electronic communications protocols used for real-time exchange of information related to securities transaction and market data. The protocol is used to move massive quantities of money per day.The Financial Information eXchange (FIX) Protocol is one of the most pervasive electronic communications protocols used for real-time exchange of information related to securities transaction and market data. The protocol is used to move massive quantities of money per day.The Financial Information eXchange (FIX) Protocol is one of the most pervasive electronic communications protocols used for real-time exchange of information related to securities transaction and market data. The protocol is used to move massive quantities of money per day. With over 1600 fields (tags) and 115 message types, the protocol presents some unique challenges to consider when developing an add-on for Splunk. Come see and discuss the protocol and how Splunk can help you make sense of the data it contains to improve your trading, business analytic, security, fraud, and compliance operations. With over 1600 fields (tags) and 115 message types, the protocol presents some unique challenges to consider when developing an add-on for Splunk. Come see and discuss the protocol and how Splunk can help you make sense of the data it contains to improve your trading, business analytic, security, fraud, and compliance operations. With over 1600 fields (tags) and 115 message types, the protocol presents some unique challenges to consider when developing an add-on for Splunk. Come see and discuss the protocol and how Splunk can help you make sense of the data it contains to improve your trading, business analytic, security, fraud, and compliance operations."
  },
  {
    "Event": ".conf19",
    "Title": "DEV1396 - Get your head into the clouds with Splunk Cloud Platform",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Developer Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1396.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV1396.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "What is Splunk Cloud Platform (SCP), and how can it be leveraged in new ways current Splunk Cloud cannot? In this session you'll learn what Splunk Cloud Platform has to offer, the core concepts to be successful, and how to use Splunk Developer Cloud (SDC) tools to explore the services and features. Getting started couldn't be easier, and we'll show you how to go from sign-up to running in just a few clicks. You'll be dropped right in the middle of Splunk Investigate where you can access the services and features SCP has to offer (like ingest and search and collaboration tools). But that's not all, because using SDC tools you'll see how easy it is to create your own app to utilize the same SCP features as Splunk Investigate for your own use cases. Come join us for this end-to-end look at Splunk Cloud Platform, and get your head into the clouds!   "
  },
  {
    "Event": ".conf19",
    "Title": "DEV1270 - ‘Git’ Splunk App Management with Visual Studio Code",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1270.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV1270.pdf",
    "SkillLevel": "Beginner",
    "Description": "How would you like a lightweight, fast, cross-platform source code editor that provides syntax highlighting for all Splunk configuration files? Want to edit your remote Splunk configuration files directly from your laptop or desktop? Ever make changes to your Splunk configuration files you wish you could easily undo? All of this can be done from Microsoft Visual Studio (VS) Code installed locally on your favorite operating system. This hands on lab will teach you how to use VS Code for Splunk App Management, including remote file editing, Splunk application best practices, and using GitLab for configuration file version control."
  },
  {
    "Event": ".conf19",
    "Title": "FND2209 - Going Beyond Inclusion in the Workplace",
    "Track": "Foundations/Platform",
    "Industry": "Diversity & Inclusion",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FND2209.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FND2209.pdf",
    "SkillLevel": "Beginner",
    "Description": "Technology has enveloped our lives - Smart TVs at home, FortNite on your morning commute, to communication with our colleagues… even when they’re the next cubicle over. Before the Internet, people who are blind had to rely on others to read the news to them, unless they were fortunate enough to have access to a braille copy. Today, a staggering amount of information is at their fingertips, thanks to the Internet and screen readers. People with limited mobility can now work home if they choose to. One disability group still faces significant challenges. Over 70% of Deaf sign language users are under- or un-employed. \n\nMany people have implicit bias assuming that communication with Deaf and Hard of Hearing employees is difficult, costly, challenging.\n\nIn this presentation, Davin and Jarlath discuss how offices can not only become accessible, but go beyond creating a culture of inclusion, awareness and equity.\nTechnology has enveloped our lives - Smart TVs at home, FortNite on your morning commute, to communication with our colleagues… even when they’re the next cubicle over. Before the Internet, people who are blind had to rely on others to read the news to them, unless they were fortunate enough to have access to a braille copy. Today, a staggering amount of information is at their fingertips, thanks to the Internet and screen readers. People with limited mobility can now work home if they choose to. One disability group still faces significant challenges. Over 70% of Deaf sign language users are under- or un-employed. Technology has enveloped our lives - Smart TVs at home, FortNite on your morning commute, to communication with our colleagues… even when they’re the next cubicle over. Before the Internet, people who are blind had to rely on others to read the news to them, unless they were fortunate enough to have access to a braille copy. Today, a staggering amount of information is at their fingertips, thanks to the Internet and screen readers. People with limited mobility can now work home if they choose to. One disability group still faces significant challenges. Over 70% of Deaf sign language users are under- or un-employed. Technology has enveloped our lives - Smart TVs at home, FortNite on your morning commute, to communication with our colleagues… even when they’re the next cubicle over. Before the Internet, people who are blind had to rely on others to read the news to them, unless they were fortunate enough to have access to a braille copy. Today, a staggering amount of information is at their fingertips, thanks to the Internet and screen readers. People with limited mobility can now work home if they choose to. One disability group still faces significant challenges. Over 70% of Deaf sign language users are under- or un-employed. Many people have implicit bias assuming that communication with Deaf and Hard of Hearing employees is difficult, costly, challenging.Many people have implicit bias assuming that communication with Deaf and Hard of Hearing employees is difficult, costly, challenging.Many people have implicit bias assuming that communication with Deaf and Hard of Hearing employees is difficult, costly, challenging.In this presentation, Davin and Jarlath discuss how offices can not only become accessible, but go beyond creating a culture of inclusion, awareness and equity.In this presentation, Davin and Jarlath discuss how offices can not only become accessible, but go beyond creating a culture of inclusion, awareness and equity.In this presentation, Davin and Jarlath discuss how offices can not only become accessible, but go beyond creating a culture of inclusion, awareness and equity."
  },
  {
    "Event": ".conf19",
    "Title": "FNS2584 - Handling Expanding Data Sources & Datacenter Migration While Strengthening the Nutanix Security Posture with Splunk and HCI",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FNS2584.mov",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FNS2584.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Nutanix implemented Splunk to improve operations and security. Attend this session to learn how we started small and grew our Splunk footprint, going from 80 GB/day to 700GB-1.8TB/day, to satisfy key IT and business needs. You will also learn how we leveraged Splunk and our own Nutanix infrastructure for a successful data center migration that involved over 2000 clients and 80+TB of data. We’ll share best practices and insights into running virtualized Splunk Enterprise on hyperconverged infrastructure (HCI). You’ll also learn about an app for Phantom, which we’ll demo, we built to provide security operations teams the ability to quickly contain a VM by stopping or suspending it, then safely starting it, plus the other workloads, like firewall, Docker (incl. Splunk Docker), ETL, etc., we run alongside Splunk on the same infrastructure stack. Whether you’re a Splunk user or own the infrastructure that supports your Splunk team, you’ll get details to help you in your job."
  },
  {
    "Event": ".conf19",
    "Title": "ITS2796 - Harnessing AIOps for a Retailer’s Digital Transformation at Pace and Scale Using ITSI",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence"
    ],
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/ITS2796.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "How can a rapidly-innovating retailer match the pace of delivery with comprehensive alerting? And how can support teams visualise complex end-to-end flows in a microservice architecture? In partnership with Accenture, our clients implement Splunk to match innovation with world-class monitoring and analytics. Our clients innovate to change the way they deliver systems using 100% Agile delivery, configuring leading software on a private “platform as a service”. We will demonstrate how one can address the challenges of pace and scale by focusing on tight integration and configuration of increasingly complex AIOps ecosystems.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1098 - Harnessing Natural Curiosity for exploring data",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1098.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1098.pdf",
    "SkillLevel": "Intermediate",
    "Description": "In a world full of AI, wouldn't it be nice to foster some Natural Curiosity?  Coming from a Humanities background, I tend to look at data a little differently than most engineers and data scientists. Using Splunk I'm able to freely navigate through the data and explore different use cases. "
  },
  {
    "Event": ".conf19",
    "Title": "FND1364 - Harnessing the Next Generation of Tech Talent",
    "Track": "Foundations/Platform",
    "Industry": "Non-ProfitDiversity & Inclusion",
    "Products": [
      "Splunk Business Flow"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FND1364.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FND1364.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Generation Z is our largest generation ever. They are over 2 billion strong and are predicted to make up nearly 30% of the workforce by 2025. Fifty percent of them are likely to have a university degree that will carry them through 17 jobs, spanning five different careers in their lifetime. So how can we harness the energy of this huge pool of \"think differently,\" \"act differently\" resources? With an internship program. Learn how the Splunk ANZ team focused on addressing the staffing challenges many tech companies face, particularly with regard to promoting diversity and including underrepresented groups, such as female students studying STEM subjects. This session will review how they built an internship program that not only delivered huge value to the business, including Gen Z diversity, but also focused on transferring technical and soft skills to facilitate their transition into full-time employment.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1263 - Harnessing the Power of Splunk and Google Cloud- Deploy, Ingest, and Beyond",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1263.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1263.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Want to run Splunk on Google Cloud Platform (GCP)? Have a GCP environment you aren’t monitoring in Splunk? In this joint Google & Splunk session, you’ll learn how to architect, build and scale a Splunk environment on GCP according to best practices for availability, performance and cost. We’ll walk you through the setup of a real-world Splunk environment on GCP, as well as the various options available for ingesting valuable Google Cloud data in any Splunk environment. We’ll leverage streaming services from Google Cloud like Pub/Sub and Dataflow to capture in near real-time operational, audit, billing, inventory data, and more. We’ll then demonstrate how to analyze this wealth of data and get immediate insights for several use cases from IT Ops to Security.\nWant to run Splunk on Google Cloud Platform (GCP)? Have a GCP environment you aren’t monitoring in Splunk? In this joint Google & Splunk session, you’ll learn how to architect, build and scale a Splunk environment on GCP according to best practices for availability, performance and cost. We’ll walk you through the setup of a real-world Splunk environment on GCP, as well as the various options available for ingesting valuable Google Cloud data in any Splunk environment. We’ll leverage streaming services from Google Cloud like Pub/Sub and Dataflow to capture in near real-time operational, audit, billing, inventory data, and more. We’ll then demonstrate how to analyze this wealth of data and get immediate insights for several use cases from IT Ops to Security.Want to run Splunk on Google Cloud Platform (GCP)? Have a GCP environment you aren’t monitoring in Splunk? In this joint Google & Splunk session, you’ll learn how to architect, build and scale a Splunk environment on GCP according to best practices for availability, performance and cost. We’ll walk you through the setup of a real-world Splunk environment on GCP, as well as the various options available for ingesting valuable Google Cloud data in any Splunk environment. We’ll leverage streaming services from Google Cloud like Pub/Sub and Dataflow to capture in near real-time operational, audit, billing, inventory data, and more. We’ll then demonstrate how to analyze this wealth of data and get immediate insights for several use cases from IT Ops to Security."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1550 - Have No Fear, WMI Is Here- Identify Lateral Movement and Malicious Backdoors with Windows Management Instrumentation",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1550.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1550.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Attackers are increasingly using a 'living off the land' approach, often using crypto mining malware, EternalBlue, timing, or other attacks that leverage the Windows Management Instrumentation Command Line. These attacks typically don't generate any events via conventional Sysmon and PowerShell, so even if you're pulling in those logs you likely won't see them. Join this session to learn how to detect and protect your organization from these advanced WMI-based attacks."
  },
  {
    "Event": ".conf19",
    "Title": "BA2082 - Healthcare and Splunk – Leading the Way in Patient Care!",
    "Track": "Business Analytics",
    "Industry": "HealthcareNon-Profit",
    "Products": [
      "Splunk Enterprise"
    ],
    "SkillLevel": "Beginner",
    "Description": "Memorial Sloan Kettering Cancer Center in New York City is a leader in cancer care. MSKCC has embraced Splunk as a must-have tool in our software arsenal, helping us maintain excellence in patient care. With Splunk, MSKCC can create a multitude of applications, dashboards, and alerts that improve the way we conduct business. Paramount is the ability to ingest disparate, traditional DBMS data sources via hundreds of DBInputs. This clinical data comes from SQL Server, Oracle, MySQL, Cache, and DB2 from databases as large as 7 terabytes. Creating continuous, data-driven and predictive dashboards helps clinicians serve patients in the best possible way. In order to maintain proper privacy and compliance, we leverage Splunk’s powerful role capabilities. Some of the dashboard features to be discussed in this session: o Opioid Inventory Monitoring o Patient-Controlled Analgesia Predictive Usage o Predicting Surgery Durations o Care Team Determination o Blood Transfusion Checks\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN2087 - Health Insights in One-click with New Splunk Monitoring Tools",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2087.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2087.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Wouldn’t it be great if you can just be proactively told when your Splunk deployment needs your attention? Wouldn’t it be simply awesome to go to one place and know exactly what the problem is and how to resolve it? At Splunk we understand that every organization suffers the pain of throwing resources to keep the lights on for their infrastructure environment. Fortunately the new version of Splunk Monitoring helps you know when things are not performing as expected. You can now see health of deployment wide without affecting your search or indexing latency and go through guided set of checks curated from years of support experience to solve issues first hand.\n\n \nWouldn’t it be great if you can just be proactively told when your Splunk deployment needs your attention? Wouldn’t it be simply awesome to go to one place and know exactly what the problem is and how to resolve it? At Splunk we understand that every organization suffers the pain of throwing resources to keep the lights on for their infrastructure environment. Fortunately the new version of Splunk Monitoring helps you know when things are not performing as expected. You can now see health of deployment wide without affecting your search or indexing latency and go through guided set of checks curated from years of support experience to solve issues first hand.Wouldn’t it be great if you can just be proactively told when your Splunk deployment needs your attention? Wouldn’t it be simply awesome to go to one place and know exactly what the problem is and how to resolve it? At Splunk we understand that every organization suffers the pain of throwing resources to keep the lights on for their infrastructure environment. Fortunately the new version of Splunk Monitoring helps you know when things are not performing as expected. You can now see health of deployment wide without affecting your search or indexing latency and go through guided set of checks curated from years of support experience to solve issues first hand.Wouldn’t it be great if you can just be proactively told when your Splunk deployment needs your attention? Wouldn’t it be simply awesome to go to one place and know exactly what the problem is and how to resolve it? At Splunk we understand that every organization suffers the pain of throwing resources to keep the lights on for their infrastructure environment. Fortunately the new version of Splunk Monitoring helps you know when things are not performing as expected. You can now see health of deployment wide without affecting your search or indexing latency and go through guided set of checks curated from years of support experience to solve issues first hand.Wouldn’t it be great if you can just be proactively told when your Splunk deployment needs your attention? Wouldn’t it be simply awesome to go to one place and know exactly what the problem is and how to resolve it? At Splunk we understand that every organization suffers the pain of throwing resources to keep the lights on for their infrastructure environment. Fortunately the new version of Splunk Monitoring helps you know when things are not performing as expected. You can now see health of deployment wide without affecting your search or indexing latency and go through guided set of checks curated from years of support experience to solve issues first hand.Wouldn’t it be great if you can just be proactively told when your Splunk deployment needs your attention? Wouldn’t it be simply awesome to go to one place and know exactly what the problem is and how to resolve it? At Splunk we understand that every organization suffers the pain of throwing resources to keep the lights on for their infrastructure environment. Fortunately the new version of Splunk Monitoring helps you know when things are not performing as expected. You can now see health of deployment wide without affecting your search or indexing latency and go through guided set of checks curated from years of support experience to solve issues first hand.Wouldn’t it be great if you can just be proactively told when your Splunk deployment needs your attention? Wouldn’t it be simply awesome to go to one place and know exactly what the problem is and how to resolve it? At Splunk we understand that every organization suffers the pain of throwing resources to keep the lights on for their infrastructure environment. Fortunately the new version of Splunk Monitoring helps you know when things are not performing as expected. You can now see health of deployment wide without affecting your search or indexing latency and go through guided set of checks curated from years of support experience to solve issues first hand.Wouldn’t it be great if you can just be proactively told when your Splunk deployment needs your attention? Wouldn’t it be simply awesome to go to one place and know exactly what the problem is and how to resolve it? At Splunk we understand that every organization suffers the pain of throwing resources to keep the lights on for their infrastructure environment. Fortunately the new version of Splunk Monitoring helps you know when things are not performing as expected. You can now see health of deployment wide without affecting your search or indexing latency and go through guided set of checks curated from years of support experience to solve issues first hand."
  },
  {
    "Event": ".conf19",
    "Title": "FND1502 - Helping Women in Technology to Boost Their Careers by Getting Public Recognition for Intellectual Property that They Create",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specificDiversity & Inclusion",
    "Products": [
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FND1502.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FND1502.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "This session will illuminate the world of Intellectual Property (IP) so that women are more empowered to gain recognition. The underrepresentation of women in STEM has meant that, historically, men have had a reputational advantage, but women can gain ground. IP rights provide an avenue for increased acknowledgement, both inside and outside the company. This session will cover the issues of ownership as well as the different types of IP, and which IP may be most valuable to the company. It also will explain the role of an IP group in a company, and how women can take advantage of their efforts to see their contributions being acknowledged. It also will discuss the business pressures at play, with different company groups and products vying for limited resources. The goal of this session is to give women the information and tools they need to take advantage of opportunities, gaining the much-deserved recognition they deserve, and boosting their professional career in technology!"
  },
  {
    "Event": ".conf19",
    "Title": "SECS3061 - High-Impact Strategies to Close the Visibility Gap in Security",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security"
    ],
    "SkillLevel": "Intermediate",
    "Description": "As security threats multiply, so do the number of tools and technologies to address them. However, many companies are struggling to make sense of the data from their SIEM, EDR, and other investments, resulting in an inability to integrate the data in centralized, actionable ways. This presentation will explain what security teams should do to increase visibility into, and control over, escalating threats to their enterprises."
  },
  {
    "Event": ".conf19",
    "Title": "IT1642 - How 3M is Transforming SAP ERP Operations through AIOPS",
    "Track": "IT Operations",
    "Industry": "Aerospace & DefenseCommunicationsEnergy & UtilitiesHealthcareManufacturingTechnologyTravel & TransportationOil & GasNot industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1642.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1642.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Discover how 3M is using Splunk to get more value and insights from their mission-critical SAP deployment and its complex legacy environment. You can see how far we have we come and where  our vision will take us. Managing SAP is a complex and mission-critical challenge. With its proprietary and often-customized inner workings, SAP has become synonymous with complexity and has long been a difficult challenge for IT departments around the globe to manage. With even short outages carrying the potential of large impacts, it is more important than ever for large ERP customers to bring their systems management and monitoring practices into the data-driven world. In this session, learn what transformational outcomes have been and are being achieved as this 116-year-old global manufacturer partners with Splunk to embrace and overcome ERP and legacy operational data chaos."
  },
  {
    "Event": ".conf19",
    "Title": "IT1878 - How a team of 4 Site Reliability Engineers can manage 100's of team’s data and 10,000 VM's through automation",
    "Track": "IT Operations",
    "Industry": "CommunicationsOnline ServicesTechnology",
    "Products": [
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1878.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1878.pdf",
    "SkillLevel": "Advanced",
    "Description": "This talk will tell how Paddy Power Betfair’s development teams onboard data to Splunk using pipeline deployments from QA right through to Production. We’ll discuss the large scale of our stack, but how a small team manages Splunk across the organization with the help of automation. We’ll go into detail to demonstrate the business value of Splunk and how it provides efficiencies across the organization."
  },
  {
    "Event": ".conf19",
    "Title": "IT1753 - How Kronos Consolidated Logging and Infrastructure Monitoring with the Splunk App for Infrastructure",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1753.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1753.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Monitoring a microservice-based and globally distributed SaaS solution in the public cloud is challenging alone, but doing it with a variety of monitoring and logging tools makes it even harder. Learn how the Kronos Workforce Dimensions team consolidated logging, infrastructure monitoring, and cloud monitoring all within Splunk to facilitate seamless issue detection and correlation in an effort to minimize time to resolution. In this session, we will share our approach and lessons learned with respect to deployment in the cloud, alerting and visualization (dashboards) in our journey of consolidation of tools and leveraging the new Splunk App for Infrastructure app along the way.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT1918 - How S&P Global Migrated to the Cloud with Splunk",
    "Track": "IT Operations",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1918.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1918.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "S&P Global is undergoing a cloud transformation. As we continue to mature in the  cloud, we are  partnering with Splunk to enable monitoring that is accurate, timely, and actionable. The cloud transformation project was aggressive, and we have exited our primary data centers and have nearly migrated 100% of applications to the cloud within eight months. We are building in the cloud using infrastructure automation, DevSecOps, and continuous monitoring. The cloud environment is by no means simple. It continues to be complex because some applications have migrated to be cloud-native while others are lift and shift. In addition, we are moving to containers and multi-cloud. All those teams use Splunk to monitor for issues and investigate root causes. By integrating and automating our IT services, we have reduced MTTI and MTTR, and increased our productivity. We will share details about our successful implementation story, and provide steps to help you achieve monitoring maturity in changes."
  },
  {
    "Event": ".conf19",
    "Title": "DEV1440 - How Splunkbase identifies, resolves, and reviews incidents using Splunk-Investigate.",
    "Track": "Developer",
    "Industry": "Technology",
    "Products": [
      "Splunk Developer Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1440.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV1440.pdf",
    "SkillLevel": "Beginner",
    "Description": "As a service that has many different integration points, Splunkbase needs to ensure as much uptime as possible. This means that when an incident occurs the root cause needs to be identified, resolved, reviewed and communicated to all relevant parties in a timely manner. Fortunately, Splunk>Investigate has served us very well in achieving these objectives. In this session, we’ll demonstrate how the use of the Splunk>Investigate app on Splunk Cloud Platform (SCP) enables teams to access the same data pool with appropriate authorization to collaborate using shared workbooks. This workflow enables teams to quickly reach a solution when an incident occurs."
  },
  {
    "Event": ".conf19",
    "Title": "FN2041 - How Splunk Uses Product Telemetry Data to Improve our Products and Services",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2041.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2041.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "At Splunk, we listen to our data for insights on how to improve our products and provide better services for our customers. We collect aggregated product usage, configuration and performance data from customers who share this information, and use it for benchmarking product performance and building more realistic test environments, developing sizing calculators and best practice configuration guidelines, prioritizing what products and features to develop on various platforms, improving user experience and workflows, and more proactively support our customers. Come hear from a panel of Splunkers about how they have been able to deliver valuable outcomes using telemetry data.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN2515 - How the world largest sovereign wealth fund moved to SPLUNK CLOUD​",
    "Track": "Foundations/Platform",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2515.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2515.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "NBIM decided in 2018 to move all solutions to cloud within 18 months project. That included also our SPLUNK solutions. What now? As NBIM security lead engineer, I would like to share our experiences from a NBIM project moving SPLUNK Enterprise Security SOC solutions to SPLUNK CLOUD. "
  },
  {
    "Event": ".conf19",
    "Title": "IoT1830 - How to boost operational readiness and situational awareness for law enforcement ",
    "Track": "Internet of Things",
    "Industry": "Non-ProfitPublic Sector",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1830.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IoT1830.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Can law enforcement protect citizens in this age of digital transformation? Are they using technology in way that promotes insights and outcomes to help them become more proactive? Do those insights provide officers with increased situational awareness to ensure officer safety and effectiveness? We’ll show you how the connected vehicle provides data to unlock insights into the operational readiness of the digital officer while also providing improved situational awareness for their safety.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN2143 - How to effectively run high cardinality and federated searches using Data Fabric Search. ",
    "Track": "Foundations/Platform",
    "Industry": "Technology",
    "Products": [
      "Splunk Data Fabric Search and Data Stream Processor"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2143.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2143.pdf",
    "SkillLevel": "Intermediate",
    "Description": "How would you go about exploring your data assets using Splunk’s newly available Data Fabric Search? What should you expect when you adopt Data Fabric Search for your Splunk deployments? We will show you how to go about enriching your Splunk searches and navigating through the different search phases to effectively utilize your resources."
  },
  {
    "Event": ".conf19",
    "Title": "SEC3334 - How to redesign Security Operations powered by Splunk-Phantom",
    "Track": "Security, Compliance and Fraud",
    "Products": [],
    "Description": "Learn about an approach we use to streamline Security Operations for clients while dramatically improving security response times. "
  },
  {
    "Event": ".conf19",
    "Title": "FN1570 - How to troubleshoot blocked ingestion pipeline queues with Indexers and Forwarders",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1570.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1570.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "How many of you have experienced pipeline queues blocking? Almost everyone. This session will provide an overview of Splunk Indexing pipelines and a deep dive into detecting blocked queues, troubleshooting tips to identify the root cause and how to resolve them. How many of you have experienced pipeline queues blocking? Almost everyone. This session will provide an overview of Splunk Indexing pipelines and a deep dive into detecting blocked queues, troubleshooting tips to identify the root cause and how to resolve them.How many of you have experienced pipeline queues blocking? Almost everyone. This session will provide an overview of Splunk Indexing pipelines and a deep dive into detecting blocked queues, troubleshooting tips to identify the root cause and how to resolve them.How many of you have experienced pipeline queues blocking? Almost everyone. This session will provide an overview of Splunk Indexing pipelines and a deep dive into detecting blocked queues, troubleshooting tips to identify the root cause and how to resolve them."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1554 - How We Scaled Splunk Enterprise Security to 100TB with Search Head Clustering",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1554.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1554.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Want to scale Splunk Enterprise Security to 100TB/day? We've done it! In Splunk labs, we built workloads that closely simulate our customers' usage patterns, and we scaled beyond a 100TB per day ingest rate with search head clustering. In this session we'll share key aspects of our Splunk Enterprise Security workload design: diverse source types, major data models, search scenarios, data enrichment, and hardware choices for search head and indexer.  We will also share how different configurations impact search performance and how to tune Splunk Enterprise Security effectively with parameters such as max_searches_per_cpu, acceleration.max_concurrent, allow_skew, and maxBundleSize to name a few. Come see how we scaled to large volumes while efficiently utilizing hardware capacity for maximum performance.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1716 - How we understand your intent - Splunk Natural Language Platform (NLP)",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk Mobile"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1716.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1716.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Understanding the intent of a natural language search is a major component of any natural language processing system. Although there are numerous machine learning and deep learning techniques to solve complex problems in natural language processing, such as understanding intent also known as natural language understanding, very few of them discuss the challenges in production. In this session we take you on a journey from development to production of natural language understanding component of Splunk Natural Language Platform. We will discuss several engineering and data science challenges, and also provide holistic approaches to overcome those challenges. Attendees of the session should walk away with a deeper understanding of different state-of-the-art techniques, such as understanding the intent of a natural language search query, that can help them build their own applications in the natural language processing domain. "
  },
  {
    "Event": ".conf19",
    "Title": "SEC2056 - Hunting in the Dark- Profiling Encrypted Network Traffic",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2056.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2056.pdf",
    "SkillLevel": "Advanced",
    "Description": "It's not easy to detect malicious patterns within encrypted network traffic. JA3, a method of fingerprinting Secure Sockets Layer (SSL) traffic developed by SalesForce, aims to address this by profiling client (JA3) and server (JA3s) SSL connections. Since these fingerprints are unique and persistent, they provide a way to discover applications, fingerprint Operating Systems, and even discover malware. This presentation showcases how to use Splunk to streamline JA3 event data gathered from Bro/Zeek, use that in combination with host-level visibility provided by Carbon Black, and ultimately correlate network signatures with endpoint telemetry. You will learn how to use this method to get a better understanding of what processes are causing benign or malicious SSL connections on your network and how to hunt for unknown threats."
  },
  {
    "Event": ".conf19",
    "Title": "SEC2109 - Hunting Threats with Splunk UBA",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk User Behavior Analytics"
    ],
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2109.pdf",
    "SkillLevel": "Advanced",
    "Description": "We're excited to provide a deep, step-by-step process for how to hunt threats using Splunk User Behavior Analytics (UBA). Our global professional services lead for Splunk UBA will go over how to build custom threats, hunt down rogue employees, build advanced watchlists, and get better at finding the needle in the haystack. "
  },
  {
    "Event": ".conf19",
    "Title": "BAS2765 - Ideas to Data to Outcomes",
    "Track": "Business Analytics",
    "Industry": "Public Sector",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BAS2765.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BAS2765.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "WWT integrates Splunk in enterprise architectures for our joint customers.  Rick Pina from WWT is going to describe how Splunk, when part of a larger architecture, is providing true mission and operational outcomes."
  },
  {
    "Event": ".conf19",
    "Title": "FNC2051 - I Deleted a Critical Knowledge Object…. Now What-",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FNC2051.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FNC2051.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "For many organizations, Splunk is considered critical infrastructure. But Splunk itself does not provide any full backup solutions. Several first and third-party options exist, each with their own positives and negatives. Join us as we talk about what options exist today, why they didn’t work for us, and what we’ve built to solve this problem - right from Splunk - using the open source tool Duplicity. Users of all OS’s welcome, but the app is currently only supported for Linux.\nFor many organizations, Splunk is considered critical infrastructure. But Splunk itself does not provide any full backup solutions. Several first and third-party options exist, each with their own positives and negatives. Join us as we talk about what options exist today, why they didn’t work for us, and what we’ve built to solve this problem - right from Splunk - using the open source tool Duplicity. Users of all OS’s welcome, but the app is currently only supported for Linux.For many organizations, Splunk is considered critical infrastructure. But Splunk itself does not provide any full backup solutions. Several first and third-party options exist, each with their own positives and negatives. Join us as we talk about what options exist today, why they didn’t work for us, and what we’ve built to solve this problem - right from Splunk - using the open source tool Duplicity. Users of all OS’s welcome, but the app is currently only supported for Linux."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1518 - I Have Authority to Operate in the Cloud...Now How Do I Secure It-",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Public Sector",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1518.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1518.pdf",
    "SkillLevel": "Beginner",
    "Description": "You finally got Authority To Operate (ATO) in the Cloud, and you're feeling the budgetary and political pressure to transition your workloads to AWS. But how do you actually transition a workload securely? This session covers the essentials of using Splunk to quickly increase your security posture and awareness in the Cloud. Learn from our experiences and leave with more confidence that you're asking smart questions of your data, monitoring and alerting on the right things, assigning responsibilities to your team appropriately, and have an actionable security plan in place to protect your Cloud assets."
  },
  {
    "Event": ".conf19",
    "Title": "IoT2020 - Image indexing framework for image search and deep learning applications ",
    "Track": "Internet of Things",
    "Industry": "Manufacturing",
    "Products": [
      "Splunk Enterprise",
      "Splunk for Industrial IoT"
    ],
    "SkillLevel": "Intermediate",
    "Description": "Image processing and computer vision applications are increasingly being put to use, especially in manufacturing and other fast-paced environments. However, there are too few frameworks for storing and retrieving images, appending other metadata to the images, and providing a user with an interface to retrieve and view both processed and unprocessed images. In this talk, we will demonstrate such a framework using a manufacturing use case."
  },
  {
    "Event": ".conf19",
    "Title": "IT2133 - Improved Methods for Using Selenium with Splunk to Monitor Web Apps",
    "Track": "IT Operations",
    "Industry": "Public SectorTechnologyNot industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT2133.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT2133.pdf",
    "SkillLevel": "Advanced",
    "Description": "Using data that you probably already have in Splunk, you can gain tremendous insight into the performance of your web applications. However, this requires someone to visit your application, and ideally you don't want your customers discovering your problems for you. What happens when nobody is looking? What if a part of the site is broken but users just haven't tripped over it yet?  We'll demonstrate the methods that we developed at the Pacific Northwest National Laboratory using Splunk and open source tools like Selenium to monitor web applications with synthetic interactions that happen in a real browser to verify everything is performing as expected. We'll share how to put the pieces together, including configuring Selenium Grid, creating monitoring that thoroughly tests your apps, dashboards that create and test your Selenium interactions for you, getting screenshots and network waterfall data, and our monitoring dashboards that combine test results with existing Splunk data."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1545 - Improve Your Cyber Monitoring & Response Strategy with Splunk Enterprise Security and Splunk Phantom",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1545.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1545.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "How do you know if your alerting and response processes adequately cover the tactics and techniques that your adversaries will use against you? If you're not sure, then how do to you continuously improve to adapt to ever-evolving threats? This session will provide practical guidance on leveraging models like the diamond model, MITRE ATT&CK™, and OODA to deconstruct your monitoring and response program so that you can make strategic improvements and mature it on a strong foundation. Using these frameworks will help your team recognize its own bias in developing use cases, understand how its alerting and response coverage maps to adversary tactics/techniques, and develop and prioritize new use cases. The session will wrap up discussing practical tips for creating a continuous improvement program that helps you leverage Splunk Enterprise Security and Splunk Phantom to maintain a strong security posture."
  },
  {
    "Event": ".conf19",
    "Title": "IoT1893 - Improving cost-efficiency of inland cargo shipping within the Port of Hamburg – powered by Splunk IoT capabilities",
    "Track": "Internet of Things",
    "Industry": "Travel & Transportation",
    "Products": [
      "Splunk for Industrial IoT"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1893.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IoT1893.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "The Port of Hamburg is the main entrance to the German market and the third largest European port by transit volume, passing nearly 9 million containers each year. Although it has witnessed several stages of automation, there is still vast, untapped potential to preserve resources, both economic and ecological. By introducing IoT with Splunk, we move away from the docks and onto the vessels, equipping one tugboat and one inland cargo ship with IoT technology to measure movement and gather data about energy consumption from their 1950s diesel engines. The goal of this PoC environment is to automate the communication between ship and port authority, as well as visualize their field of operation to evenly distribute load and optimize the travel distances. On a broader spectrum, we also built a simulation model within our dashboard to investigate the benefits of adapting the vessels with engines powered to natural gas or electricity based on their consumption patterns."
  },
  {
    "Event": ".conf19",
    "Title": "FN1380 - Improving Live Musical Performances with Splunk",
    "Track": "Foundations/Platform",
    "Industry": "Technology",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "Creativity is at the heart of a musicians life.  As expression is released in the form of music, logical analysis of the output becomes possible.  Notes are expressed through instruments.  When recording, these notes can be analyzed for accuracy in pitch, timing, and velocity.  Come and see how Splunk can help musicians approach their craft more holistically by augmenting their creative expression with logical note analysis.   In doing so, Splunk then becomes a means by which musicians can improve live performances through machine data analysis.   "
  },
  {
    "Event": ".conf19",
    "Title": "IoT1621 - Improving the asset reliability for manufacturing the world’s most advanced fighter jet, Lockheed Martin’s F-35 Lightning II with Splunk for Industrial IoT",
    "Track": "Internet of Things",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk for Industrial IoT"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1621.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IOT1621.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Join us to learn how Lockheed Martin utilizes Splunk for IoT to bridge the gap between Operations and Maintenance (O&M), and how they took a multi-vendor, best-of-breed approach to solving their most critical O&M challenges. If you attend this session you will see how real-time monitoring, and condition-based and predictive maintenance for critical assets can reduce unplanned downtime, improve maintenance activities, and increase operational efficiencies. We will also show how the Splunk implementation includes data convergence from disparate pieces of equipment and existing enterprise software solutions to create new insights and allow for actions that were once considered very difficult or too costly to realize."
  },
  {
    "Event": ".conf19",
    "Title": "BA1204 - Improving the Medical Claims Process with Splunk",
    "Track": "Business Analytics",
    "Industry": "Healthcare",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BA1204.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BA1204.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "A large healthcare provider can process thousands of medical claims per day and millions of claims per month. The claims process involves multiple steps and many moving parts and can take weeks to complete. With the use of Splunk, a healthcare customer has been able to increase its visibility and insights into the medical claims process. The result is time savings in the claims process, improved customer experience, and increased productivity. Learn how a healthcare customer is leveraging Splunk to perform near real-time and historical analysis to continuously improve the claims process.\nA large healthcare provider can process thousands of medical claims per day and millions of claims per month. The claims process involves multiple steps and many moving parts and can take weeks to complete. With the use of Splunk, a healthcare customer has been able to increase its visibility and insights into the medical claims process. The result is time savings in the claims process, improved customer experience, and increased productivity. Learn how a healthcare customer is leveraging Splunk to perform near real-time and historical analysis to continuously improve the claims process.A large healthcare provider can process thousands of medical claims per day and millions of claims per month. The claims process involves multiple steps and many moving parts and can take weeks to complete. With the use of Splunk, a healthcare customer has been able to increase its visibility and insights into the medical claims process. The result is time savings in the claims process, improved customer experience, and increased productivity. Learn how a healthcare customer is leveraging Splunk to perform near real-time and historical analysis to continuously improve the claims process."
  },
  {
    "Event": ".conf19",
    "Title": "FN1081 - Index Impasse- Limiting Data Access on a Per Event Basis",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1081.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1081.pdf",
    "SkillLevel": "Advanced",
    "Description": "Is managing your growing list of indexes like herding cattle? Is your Master Node struggling against a stampede of buckets? Are you ducking bullets to satisfy data access requirements at the expense of usability or search performance? This session will demonstrate the advanced, role-based data access approach being used by several large Splunk customers. You will learn alternative methods to satisfy complex data access requirements without having to allocate hundreds or thousands of indexes. This method will allow you to focus on usability and search performance as your primary criteria for designing and managing your indexing strategy."
  },
  {
    "Event": ".conf19",
    "Title": "IoT1066 - Industrial Cyber Security In A Converging IT-OT World",
    "Track": "Internet of Things",
    "Industry": "Manufacturing",
    "Products": [
      "Splunk Enterprise Security",
      "Splunk for Industrial IoT"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1066.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IOT1066.pdf",
    "SkillLevel": "Advanced",
    "Description": "The digital convergence of IT and OT infrastructures promises huge efficiencies, cost savings and opportunities; but it is not without risk. OT was primarily built to run all types of manufacturing & critical infrastructure processes while IT was built to store, transmit and manipulate data in order to conduct business. The two worlds could not be more different in purpose or design; and this can expose even the most secure organizations to new types of cyber threats. In this session we will discuss the current challenges we face in the drive to convergence and how to secure your industrial or critical infrastructure organization from the clear and present threat.\nThe digital convergence of IT and OT infrastructures promises huge efficiencies, cost savings and opportunities; but it is not without risk. OT was primarily built to run all types of manufacturing & critical infrastructure processes while IT was built to store, transmit and manipulate data in order to conduct business. The two worlds could not be more different in purpose or design; and this can expose even the most secure organizations to new types of cyber threats. In this session we will discuss the current challenges we face in the drive to convergence and how to secure your industrial or critical infrastructure organization from the clear and present threat.The digital convergence of IT and OT infrastructures promises huge efficiencies, cost savings and opportunities; but it is not without risk. OT was primarily built to run all types of manufacturing & critical infrastructure processes while IT was built to store, transmit and manipulate data in order to conduct business. The two worlds could not be more different in purpose or design; and this can expose even the most secure organizations to new types of cyber threats. In this session we will discuss the current challenges we face in the drive to convergence and how to secure your industrial or critical infrastructure organization from the clear and present threat."
  },
  {
    "Event": ".conf19",
    "Title": "IT1258 - Infrastructure and System Monitoring with Splunk and Telegraf",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1258.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1258.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Telegraf is a open source tool that is used for collecting metrics from a variety of inputs, including system data (CPU, memory, disk, and network), docker, MySQL, etc. Telegraf (as of v1.8) supports a \"splunkmetric\" serializer for native ingest into Splunk's metric store using a variety of Telegraf's output modules, including file outputs, and HTTP with the ability to include HEC-required fields. Telegraf can be deployed as a stand-alone daemon or as a Splunk application that can be pushed out from deployers, masters, and the like. We'll investigate the various integrations with Telegraf and Splunk including using Telegraf as the system to feed Splunk's App for Infrastructure (in lieu\n of collectd), custom dashboards, as well as integrations with ITSI. With Telegraf's multitude of inputs, outputs, and a nearly universal run-time, it's a fantastic tool to add to your system monitoring workflows."
  },
  {
    "Event": ".conf19",
    "Title": "IT1602 - Infrastructure Insights- Reducing Time by Shining a Light on Siloed IT Landscapes",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1602.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1602.pdf",
    "SkillLevel": "Intermediate",
    "Description": "In a highly virtualized environment, getting a clear picture of the infrastructure landscape can be challenging. Because data is siloed between each layer of IT, discovering their relationships is often a manual task involving multiple teams. But using Splunk, building those relationships becomes a breeze. Join us as we discuss how we combine data from the Configuration Management Database (CMDB), Virtual Infrastructure, and Storage Environment to present an easy-to-use interface that allows our users to explore their infrastructure. Troubleshooting issues that used to take a user hours of downloading and parsing data from multiple sources now takes seconds with a click of a button. From finding all applications running on one host to determining which teams to notify for upcoming storage work, we can now quickly show all meaningful relationships between applications, servers, hosts, and storage."
  },
  {
    "Event": ".conf19",
    "Title": "IoT2134 - Innovation, Automation, and Orchestration for Collaboration Technology with Splunk",
    "Track": "Internet of Things",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk for Industrial IoT",
      "Splunk Mobile"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT2134.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IOT2134.pdf",
    "SkillLevel": "Intermediate",
    "Description": "AV and conference technology may seem trivial to the traditional IT OPS engineer, but it is one of the most visible and expensive parts of an enterprise. Everyone from the CEO to even the new hires on their first day use the technology. At PSE we were able to leverage Splunk to help us build, maintain, and manage our enterprise AV program (and save a lot of money in the process). In this session you will learn how we used Splunk to make data-driven decisions that influence and shape a multi-million dollar AV program."
  },
  {
    "Event": ".conf19",
    "Title": "DEV1881 - Insider Guidance For Approaching Splunk Cloud Vetting Process",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1881.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV1881.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "If you are a customer, when you want an app installed in Splunk Cloud, it is required for the app to pass cloud vetting process. Why does Splunk enforce this? What is in it for you as our customer? Who triggers cloud vetting process for an app and how are cloud vetting requests prioritized?  If you are an app developer, it is frustrating receiving messages from customers saying that the app that you developed fails Splunk cloud vetting, and they want you to help to fix. How to fix? How to develop an app that has the biggest chance of passing cloud vetting? What are the common failures that Splunk cloud vetting engineers saw in history and what are the best practices?  Come to this session, you will get answers to all the questions above. If you are a customer, when you want an app installed in Splunk Cloud, it is required for the app to pass cloud vetting process. Why does Splunk enforce this? What is in it for you as our customer? Who triggers cloud vetting process for an app and how are cloud vetting requests prioritized?If you are a customer, when you want an app installed in Splunk Cloud, it is required for the app to pass cloud vetting process. Why does Splunk enforce this? What is in it for you as our customer? Who triggers cloud vetting process for an app and how are cloud vetting requests prioritized?If you are a customer, when you want an app installed in Splunk Cloud, it is required for the app to pass cloud vetting process. Why does Splunk enforce this? What is in it for you as our customer? Who triggers cloud vetting process for an app and how are cloud vetting requests prioritized?If you are an app developer, it is frustrating receiving messages from customers saying that the app that you developed fails Splunk cloud vetting, and they want you to help to fix. How to fix? How to develop an app that has the biggest chance of passing cloud vetting? What are the common failures that Splunk cloud vetting engineers saw in history and what are the best practices?If you are an app developer, it is frustrating receiving messages from customers saying that the app that you developed fails Splunk cloud vetting, and they want you to help to fix. How to fix? How to develop an app that has the biggest chance of passing cloud vetting? What are the common failures that Splunk cloud vetting engineers saw in history and what are the best practices?If you are an app developer, it is frustrating receiving messages from customers saying that the app that you developed fails Splunk cloud vetting, and they want you to help to fix. How to fix? How to develop an app that has the biggest chance of passing cloud vetting? What are the common failures that Splunk cloud vetting engineers saw in history and what are the best practices?Come to this session, you will get answers to all the questions above.Come to this session, you will get answers to all the questions above.Come to this session, you will get answers to all the questions above."
  },
  {
    "Event": ".conf19",
    "Title": "SECS2656 - Inside the Shark Tank with Herjavec Group ",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Cloud",
      "Splunk Enterprise Security",
      "Splunk User Behavior Analytics"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "Don’t miss your chance to see the Shark in action. Watch as Robert Herjavec puts his Herjavec Group Splunk Experts in the hot seat when they pitch their expertise in Splunk Security Operations using live demos of their proprietary tooling. Hear this panel’s unique perspectives on Managed User Behavior Analytics, Threat Modelling and the future of Splunk Managed Security Services.  As an added bonus, you’ll get to play “Shark” as the panel will close with open forum Q&A from the audience."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1673 - Integrating the Analyst, the Logic, and the Machine",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1673.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1673.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Are your analysts spending too much time clearing through notable events? Ours were too, but today our analysts are living the dream: they have all the details they want right there on the Incident Review screen, all while our alerts fine-tune themselves (with workflow action human input). Come and see how we achieved Incident Review Screen 2.0. by using Splunk's Machine Learning Toolkit to transition to smarter correlation searches. "
  },
  {
    "Event": ".conf19",
    "Title": "ITS2617 - Integrating with improved visibility and insight - with IBM Z in your hybrid multicloud",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/ITS2617.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/ITS2617.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "IBM Z continues to be the cornerstone of the global economy powering the world's largest banks, retailers, airlines and insurance companies.   With the adoption of hybrid multicloud a reality for today's fast moving businesses, the mainframe is no longer isolated from the rest of the IT infrastructure and is now an integral part of your cloud strategy.   As you expose and integrate your data and applications through APIs and look to gain further insight, any lack of visibility impacts your business's ability to deliver key services .  Learn how IBM is not only providing access to the volumes of IBM Z machine data but is also providing valuable insight into that data to both operations and  lines of business. \n"
  },
  {
    "Event": ".conf19",
    "Title": "FN2241 - In Transition- Helping Veterans Navigate the Transition to Technology Careers",
    "Track": "Foundations/Platform",
    "Industry": "Non-ProfitNot industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2241.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2241.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "More than 200,000 military service members return to civilian life each year in the United States alone, with many thousands more around the globe. Many are looking to build on their experiences, leveraging the technical training and skills acquired while working in the service to their country. Thousands more are looking to change direction. Both paths can lead to the Splunk community, where we have provided access to free training for thousands of veterans through our customers, partners, and more. Join us for a discussion with some of our leading partners, including the Wounded Warrior Project, WithYouWithMe Academy, and Npower, about how veterans can navigate the transition to technology careers with help from a growing network of partners, and how Splunk can provide new and exciting career opportunities.\n"
  },
  {
    "Event": ".conf19",
    "Title": "SEC2371 - Introducing Splunk Mission Control",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2371.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2371.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Are you a security analyst? Do you like bright and shiny new things? Attend this session to get the inside scoop on the latest and greatest coming out of our security product and engineering teams.\nAre you a security analyst? Do you like bright and shiny new things? Attend this session to get the inside scoop on the latest and greatest coming out of our security product and engineering teams.Are you a security analyst? Do you like bright and shiny new things? Attend this session to get the inside scoop on the latest and greatest coming out of our security product and engineering teams.Are you a security analyst? Do you like bright and shiny new things? Attend this session to get the inside scoop on the latest and greatest coming out of our security product and engineering teams."
  },
  {
    "Event": ".conf19",
    "Title": "SEC2295 - Introducing Splunk Mission Control",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2295.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2295.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Are you a security analyst? Do you like bright and shiny new things? Attend this session to get the inside scoop on the latest and greatest coming out of our security product and engineering teams.\nAre you a security analyst? Do you like bright and shiny new things? Attend this session to get the inside scoop on the latest and greatest coming out of our security product and engineering teams.Are you a security analyst? Do you like bright and shiny new things? Attend this session to get the inside scoop on the latest and greatest coming out of our security product and engineering teams.Are you a security analyst? Do you like bright and shiny new things? Attend this session to get the inside scoop on the latest and greatest coming out of our security product and engineering teams."
  },
  {
    "Event": ".conf19",
    "Title": "DEV2236 - Introduction to Collect Service",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Data Fabric Search and Data Stream Processor",
      "Splunk Developer Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV2236.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/Dev2236.pdf",
    "SkillLevel": "Beginner",
    "Description": "Collect Service is a new scalable method with high availability to collect data for Splunk Cloud Platform or Splunk Enterprise with Data Stream Processor(DSP). This session will cover the basic principles to show you how the Collect Service operates and why you need to use it, how the service is different from modular inputs, and how to leverage Collect Service’s REST API to automate data collection jobs efficiently.\nCollect Service is a new scalable method with high availability to collect data for Splunk Cloud Platform or Splunk Enterprise with Data Stream Processor(DSP). This session will cover the basic principles to show you how the Collect Service operates and why you need to use it, how the service is different from modular inputs, and how to leverage Collect Service’s REST API to automate data collection jobs efficiently.Collect Service is a new scalable method with high availability to collect data for Splunk Cloud Platform or Splunk Enterprise with Data Stream Processor(DSP). This session will cover the basic principles to show you how the Collect Service operates and why you need to use it, how the service is different from modular inputs, and how to leverage Collect Service’s REST API to automate data collection jobs efficiently.Collect Service is a new scalable method with high availability to collect data for Splunk Cloud Platform or Splunk Enterprise with Data Stream Processor(DSP). This session will cover the basic principles to show you how the Collect Service operates and why you need to use it, how the service is different from modular inputs, and how to leverage Collect Service’s REST API to automate data collection jobs efficiently.Collect Service is a new scalable method with high availability to collect data for Splunk Cloud Platform or Splunk Enterprise with Data Stream Processor(DSP). This session will cover the basic principles to show you how the Collect Service operates and why you need to use it, how the service is different from modular inputs, and how to leverage Collect Service’s REST API to automate data collection jobs efficiently."
  },
  {
    "Event": ".conf19",
    "Title": "BA1623 - Introduction to monitoring business operations with Acceleris’ party dashboard",
    "Track": "Business Analytics",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence",
      "Splunk Machine Learning Toolkit"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BA1623.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BA1623.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Why should running a business feel any different than throwing a party? To demonstrate how Splunk can be used to monitor and manage business operations, the DATA Mavericks team at Acceleris has iteratively perfected its Party Dashboard. It started out as a gimmick at the inauguration party of the company's new headquarters, but now the Party Dashboard demonstrates how Splunk’s dashboarding helps any team get real-time visibility into any operation. Join this session to learn why they chose the relevant metrics, how they collected and fed the data to Splunk, and what meaningful insights were generated as a fun introductory example of using Splunk to get visibility into your business operations."
  },
  {
    "Event": ".conf19",
    "Title": "IT3005 - Introduction to Real-Time Monitoring with SignalFx",
    "Track": "IT Operations",
    "Products": [
      "SignalFx"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT3005.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT3005.pdf",
    "Description": "SignalFx, the latest addition to the Splunk family, is the market leader in real-time cloud monitoring and Observability for infrastructure, microservices, and applications. This session will serve as a primer into the new challenges and solutions involved in effectively monitoring and troubleshooting modern cloud environments built with microservices, containers, Kubernetes, and serverless functions. As you progress on your cloud-native journey and develop your own monitoring and Observability strategy, we will share three valuable insights: 1) best practices for collecting, visualizing, analyzing, alerting, and troubleshooting these complex environments using metrics, traces, and logs; 2) lessons learned from companies that have successfully reduced MTTD and MTTR, improved developer productivity, and improved customer experiences; 3) a live demonstration of the SignalFx platform in action\nSignalFx, the latest addition to the Splunk family, is the market leader in real-time cloud monitoring and Observability for infrastructure, microservices, and applications. This session will serve as a primer into the new challenges and solutions involved in effectively monitoring and troubleshooting modern cloud environments built with microservices, containers, Kubernetes, and serverless functions. As you progress on your cloud-native journey and develop your own monitoring and Observability strategy, we will share three valuable insights: 1) best practices for collecting, visualizing, analyzing, alerting, and troubleshooting these complex environments using metrics, traces, and logs; 2) lessons learned from companies that have successfully reduced MTTD and MTTR, improved developer productivity, and improved customer experiences; 3) a live demonstration of the SignalFx platform in actionSignalFx, the latest addition to the Splunk family, is the market leader in real-time cloud monitoring and Observability for infrastructure, microservices, and applications. This session will serve as a primer into the new challenges and solutions involved in effectively monitoring and troubleshooting modern cloud environments built with microservices, containers, Kubernetes, and serverless functions. As you progress on your cloud-native journey and develop your own monitoring and Observability strategy, we will share three valuable insights: 1) best practices for collecting, visualizing, analyzing, alerting, and troubleshooting these complex environments using metrics, traces, and logs; 2) lessons learned from companies that have successfully reduced MTTD and MTTR, improved developer productivity, and improved customer experiences; 3) a live demonstration of the SignalFx platform in actionSignalFx, the latest addition to the Splunk family, is the market leader in real-time cloud monitoring and Observability for infrastructure, microservices, and applications. This session will serve as a primer into the new challenges and solutions involved in effectively monitoring and troubleshooting modern cloud environments built with microservices, containers, Kubernetes, and serverless functions. As you progress on your cloud-native journey and develop your own monitoring and Observability strategy, we will share three valuable insights: 1) best practices for collecting, visualizing, analyzing, alerting, and troubleshooting these complex environments using metrics, traces, and logs; 2) lessons learned from companies that have successfully reduced MTTD and MTTR, improved developer productivity, and improved customer experiences; 3) a live demonstration of the SignalFx platform in action"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1111 - IOC's- Indicators Of Crap",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1111.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1111.pdf",
    "SkillLevel": "Intermediate",
    "Description": "“You should be looking at Indicators of Compromise!” exclaims your CISO, regulator, vendor, and mom.  No problem, right? You have the most expensive security intelligence vendor and all you have to do is correlate in your expensive SIEM. \n\nIf you've tried this, then you are laughing with me. Come hear my exploration into implementing IOCs at a major US insurance company and a major US bank. I’ll address the differences between Indicators of Compromise vs Indicators of Attack, and I will show you how not to use the MITRE ATT&CK™ framework, plus some tips on how it use it well.  My goal is to save you from falling into the same pitfalls when dealing with Indicators of Crap."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1230 - Is it Normal or Suspicious- Detecting Anomalies via Market Basket Analysis",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk User Behavior Analytics"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1230.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1230.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Detecting abnormal behavior is an important objective in security monitoring, but is extremely challenging as we mostly are expected to detect \"unknown unknowns.\" We can, however, use an entity's past behavior to measure how much of what we observe today deviates from normal behavior. In this way we can detect unknown, hidden and insider threats early on to stay ahead of advanced threats. This talk presents a unified, scalable framework for anomaly detection that is built on the frequent itemset mining technique. The premise is that if we can align an event with more frequent patterns observed in history, then the event is unlikely to be an anomaly. By mining through an extensive set of features and feature co-occurrences, the model can accurately capture the normal behaviors. Any new behaviors can then be scored. At which point, any new rare co-occurrences of events can be detected and sent to analysts and SOC teams for rapid investigation.\nDetecting abnormal behavior is an important objective in security monitoring, but is extremely challenging as we mostly are expected to detect \"unknown unknowns.\" We can, however, use an entity's past behavior to measure how much of what we observe today deviates from normal behavior. In this way we can detect unknown, hidden and insider threats early on to stay ahead of advanced threats. This talk presents a unified, scalable framework for anomaly detection that is built on the frequent itemset mining technique. The premise is that if we can align an event with more frequent patterns observed in history, then the event is unlikely to be an anomaly. By mining through an extensive set of features and feature co-occurrences, the model can accurately capture the normal behaviors. Any new behaviors can then be scored. At which point, any new rare co-occurrences of events can be detected and sent to analysts and SOC teams for rapid investigation.Detecting abnormal behavior is an important objective in security monitoring, but is extremely challenging as we mostly are expected to detect \"unknown unknowns.\" We can, however, use an entity's past behavior to measure how much of what we observe today deviates from normal behavior. In this way we can detect unknown, hidden and insider threats early on to stay ahead of advanced threats. This talk presents a unified, scalable framework for anomaly detection that is built on the frequent itemset mining technique. The premise is that if we can align an event with more frequent patterns observed in history, then the event is unlikely to be an anomaly. By mining through an extensive set of features and feature co-occurrences, the model can accurately capture the normal behaviors. Any new behaviors can then be scored. At which point, any new rare co-occurrences of events can be detected and sent to analysts and SOC teams for rapid investigation.Detecting abnormal behavior is an important objective in security monitoring, but is extremely challenging as we mostly are expected to detect \"unknown unknowns.\" We can, however, use an entity's past behavior to measure how much of what we observe today deviates from normal behavior. In this way we can detect unknown, hidden and insider threats early on to stay ahead of advanced threats. This talk presents a unified, scalable framework for anomaly detection that is built on the frequent itemset mining technique. The premise is that if we can align an event with more frequent patterns observed in history, then the event is unlikely to be an anomaly. By mining through an extensive set of features and feature co-occurrences, the model can accurately capture the normal behaviors. Any new behaviors can then be scored. At which point, any new rare co-occurrences of events can be detected and sent to analysts and SOC teams for rapid investigation."
  },
  {
    "Event": ".conf19",
    "Title": "BA1512 - Just a normal day in the office – Data driven business process improvements for a global supply chain company.",
    "Track": "Business Analytics",
    "Industry": "Travel & Transportation",
    "Products": [
      "Splunk Cloud",
      "Splunk IT Service Intelligence",
      "Splunk Machine Learning Toolkit"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BA1512.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BA1512.pdf",
    "SkillLevel": "Beginner",
    "Description": "“Our IT-powered business processes are too slow.” Does this sound familiar? If so, that is usually the perfect starting point to dig in and start improving them. Unfortunately, specific data that could help with that effort are not available – normally. In this session we will show you how we at Arvato Supply Chain Solutions got the data we needed and used it to improve the collaboration between IT and business. You will learn how we connected different IT systems such as SAP and conveyor line to Splunk Cloud, and how this helped us to analyze business processes with IT Service Intelligence (ITSI). And, as the icing on the cake, we give you a sneak peak of the machine learning algorithm we implemented to continuously improve our business processes."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1357 - Klapp-Back at Attackers- Capturing Data in the Wild to Build Tailored Defenses with Splunk Security Analytics",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1357.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1357.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Splunk's Security Research Team collects attack data in the wild from across the globe and analyzes new and unusual techniques, tactics, and procedures employed by threat actors. We use this data to help customers build tailored defenses—defenses that automatically detect, investigate, and respond to suspicious activities in real time. In this session we will discuss how Splunk security researchers created our own honeypot and data collection framework in response to research demonstrating that honeypots were twice as effective as open-source intelligence feeds at detecting new threats (http://tinyurl.com/y335po8d). We will provide an introduction to honeypots and explain how we architected and built KLAPP-Back, a high-interaction SSH honeypot. We will also discuss how KLAPP-Back helped us build better detection analytics and seed Splunk Enterprise Security, Splunk Phantom, and Splunk User Behavior Analytics use cases with attacker data. "
  },
  {
    "Event": ".conf19",
    "Title": "IT1729 - Kubernetes Observability with Splunk Connect for Kubernetes (SCK)",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1729.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1729.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Kubernetes is the go-to standard for the automation of deployment, management and scaling of containerized applications. From an observability perspective it is extremely difficult to analyze, troubleshoot and gain actionable insights from these containerized applications. We bridge this gap of observability with the open-sourced Splunk Connect for Kubernetes. Splunk Connect for Kubernetes is the Splunk supported integration to ingest logs, metrics and Kubernetes object state information into Splunk. This session is intended for k8s users and developers who want to make their lives easier leveraging Splunk alongside Kubernetes. Come join the developers who built Splunk Connect for Kubernetes and learn how to configure and run Splunk to monitor your Kubernetes environment."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1071 - Large Scale Threat Hunting in Splunk",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk Machine Learning Toolkit"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1071.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1071.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Threat hunting is hard, and threat hunting in an enterprise network with thousands of endpoints is even harder. We will demonstrate how we leveraged Splunk Enterprise to build an Advanced Threat Hunting platform designed for large scale threat hunting of 100,000 or more endpoints. Using Splunk Enterprise allows us to combine analytics, data enrichment, and custom workflows to display in one platform the most important data to analysts. Our threat hunting platform addresses the challenges of data retention and collection, high false positive rates, and analyst fatigue, all while lowering the time to detection of malicious incidents and improving the efficiency of enterprise SOC operations.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT2002 - Leaving the 'Dinosaurs' Behind- Replacing Legacy Monitoring Tools Part I",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "Why do we monitor? Whether it is the sound of branches rustling behind us, the tingling on the back of our neck to tell us something is watching us from the trees, or the brief glimpse of the shadow moving behind the rock, we use the data we have at our disposal to tell us when something is wrong. This is a customer/spelunker led talk discussing enterprise monitoring, the replacement of Netcool Omnibus/Impact, and IBM Tivoli Monitoring. Why do we monitor? Whether it is the sound of branches rustling behind us, the tingling on the back of our neck to tell us something is watching us from the trees, or the brief glimpse of the shadow moving behind the rock, we use the data we have at our disposal to tell us when something is wrong. This is a customer/spelunker led talk discussing enterprise monitoring, the replacement of Netcool Omnibus/Impact, and IBM Tivoli Monitoring.Why do we monitor? Whether it is the sound of branches rustling behind us, the tingling on the back of our neck to tell us something is watching us from the trees, or the brief glimpse of the shadow moving behind the rock, we use the data we have at our disposal to tell us when something is wrong. This is a customer/spelunker led talk discussing enterprise monitoring, the replacement of Netcool Omnibus/Impact, and IBM Tivoli Monitoring.Why do we monitor? Whether it is the sound of branches rustling behind us, the tingling on the back of our neck to tell us something is watching us from the trees, or the brief glimpse of the shadow moving behind the rock, we use the data we have at our disposal to tell us when something is wrong. This is a customer/spelunker led talk discussing enterprise monitoring, the replacement of Netcool Omnibus/Impact, and IBM Tivoli Monitoring.Why do we monitor? Whether it is the sound of branches rustling behind us, the tingling on the back of our neck to tell us something is watching us from the trees, or the brief glimpse of the shadow moving behind the rock, we use the data we have at our disposal to tell us when something is wrong. This is a customer/spelunker led talk discussing enterprise monitoring, the replacement of Netcool Omnibus/Impact, and IBM Tivoli Monitoring.Why do we monitor? Whether it is the sound of branches rustling behind us, the tingling on the back of our neck to tell us something is watching us from the trees, or the brief glimpse of the shadow moving behind the rock, we use the data we have at our disposal to tell us when something is wrong. This is a customer/spelunker led talk discussing enterprise monitoring, the replacement of Netcool Omnibus/Impact, and IBM Tivoli Monitoring."
  },
  {
    "Event": ".conf19",
    "Title": "FN1061 - Lesser Known Search Commands",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1061.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1061.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Come learn some lesser known search commands! Amaze your co-workers, dazzle employers, learn something new! We will cover various commands that you might know about, but have never tried! "
  },
  {
    "Event": ".conf19",
    "Title": "SEC1674 - Lessons Learned From Building a Threat Detection Program",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1674.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1674.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "We will share experiences and best practices for implementing notable events, the various Splunk Enterprise Security frameworks, and adaptive response actions, and we'll share our approach for building a program to consistently develop, measure, and iterate on correlation searches. We will discuss how to integrate lessons learned from incidents, red team engagements, threat intelligence, threat hunting, and requirements from business units into the program. Example tactics we'll cover include leveraging low-fidelity detections to develop higher-fidelity and higher-value ones, managing detection content simply and easily through macros, and building a formula to assess the efficacy of your detection content. "
  },
  {
    "Event": ".conf19",
    "Title": "SEC1490 - Lessons Learned from Deploying Splunk UBA",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Technology",
    "Products": [
      "Splunk User Behavior Analytics",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1490.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1490.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Splunk User Behavioral Analytics (UBA) is a machine learning driven solution that helps organizations find hidden threats and anomalous behavior across users, devices, and applications. In this session we'll answer questions that came up during our large-scale deployment such as, once you've got UBA installed, how do you know if it is working well in your environment? And how long after installation does it take for the system to be operational and produce results? We'll also share best practices for validating outputs and tuning the system. This session will help you jumpstart your understanding of UBA and help you get your UBA deployment into production and detecting threats faster. "
  },
  {
    "Event": ".conf19",
    "Title": "SEC1849 - Lessons Learned From Securing Japan's First Online-Only Bank",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "Japan Net Bank is Japan's first internet-only bank, and as a result we see a high volume of fraud, partcularly as the presence of debit cards increases in the country. This session will cover methods we've developed to reduce losses from fraud and better secure our customers' financial assets. We'll cover use cases such as detecting customer account number and password leakage, fraudulent login attempts, accounts created to perpetrate fraud, and unauthorized use of debit cards, as well as how to integrate and efficiently analyze multiple threat intelligence sources in Splunk."
  },
  {
    "Event": ".conf19",
    "Title": "ITS3009 - Leveraging the AWS ISV Workload Migration Program to Migrate your Splunk Workloads to Cloud ",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/ITS3009.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "In this session, you will learn how AWS and Splunk have come together to help accelerate your Cloud migration. Splunk Cloud delivers a secure, reliable and scalable service, enabling you to get answers from your machine data without the need to purchase, manage and deploy additional infrastructure. Splunk on AWS enables you to bring your current license and run Splunk in a scalable and secure way on the AWS Cloud. \n\nThe AWS ISV Workload Migration Program (WMP) helps accelerate both customer migration paths of Splunk workloads to AWS, by creating a repeatable migration methodology, coupled with incentives to lower the overall cost of migration.\n "
  },
  {
    "Event": ".conf19",
    "Title": "FND1745 - Life in the 3%- A Conversation on Claiming Your Space In the Workplace",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specificDiversity & Inclusion",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FND1745.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FND1745.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "“There’s always a tension between one’s individual self—one’s million data points—and the statistical breakdown of your existence. It’s the awareness of this tension that I navigate each day.” I wrote this in response to the reveal that black employee representation across Splunk’s U.S. offices was only 3%. #RepresentationMatters and having a “seat at the table” are critical frameworks and tactics to improve diversity, and this panel discussion brings together a diverse cross-section of Splunkers to further explore these topics. Specifically, panel participants will use there statistical data points (e.g., 3%) as a foil to reflect on how they’ve leveraged their individual motivations to push for and create change within their particular role. We intend for the audience to leave stirred by the ideas this conversation will open up, and moved to ask themselves what they can do to create change or elevate others who’ve been underrepresented in their workplace.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT1564 - Los Angeles World Airports - Streamlining event management with IT Service Intelligence (ITSI)",
    "Track": "IT Operations",
    "Industry": "Public SectorTechnologyTravel & Transportation",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence",
      "Splunk Machine Learning Toolkit"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1564.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1564.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Los Angeles World Airport has chosen Splunk's ITSI as their centralized event/alert management platform. We’ve consolidated alerts/events from multiple management platforms across the enterprise, reducing help desk churn by grouping similar events, and evaluating the results against smart Key Performance Indicator (KPI) thresholds so that only actionable alerts or events are processed. In addition, we’ve broken down the legacy data siloes through the use of service definitions, glass tables, and deep-dives, providing better insight for all team members. Lastly, we’ve automated ITSI service and dependency creation via the Splunk ServiceNow bi-direction integration App. Plan top attend this session and you will learn how we’ve increased visibility (making data available for everyone); increased efficiency by reducing alert/event noise; improved resolution using ITSI Smart KPIs; and implemented auto service creation via ServiceNow "
  },
  {
    "Event": ".conf19",
    "Title": "FNS2822 - Lost In Translation - The Challenge of Managing Microservices",
    "Track": "Foundations/Platform",
    "Products": [],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FNS2822.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FNS2822.pdf",
    "Description": "Instana CEO Mirko Novakovic explores microservices applications and the performance management challenges created by their complexity. Unique microservice architectures, operational and deployment needs create a large management gap. \nWith so many moving pieces in addition to highly complex architectures, having the raw data won’t meet the challenge. There’s just too many pieces, too many dependencies and too much data for humans (even teams) to understand. New management tools are needed that translate data into information. \n\nOtherwise, each stakeholder must fend for themselves even though the ephemeral application architecture makes it impossible to find bottlenecks. It’s like trying to get directions in a foreign city where nobody speaks your language - every question and answer gets lost in translation. \n\nMirko examines distributed microservice applications, how their complexity creates management problems, and what APM tools should do to automatically meet those challenges and deliver high performing applications.\nInstana CEO Mirko Novakovic explores microservices applications and the performance management challenges created by their complexity. Unique microservice architectures, operational and deployment needs create a large management gap. \nWith so many moving pieces in addition to highly complex architectures, having the raw data won’t meet the challenge. There’s just too many pieces, too many dependencies and too much data for humans (even teams) to understand. New management tools are needed that translate data into information. Otherwise, each stakeholder must fend for themselves even though the ephemeral application architecture makes it impossible to find bottlenecks. It’s like trying to get directions in a foreign city where nobody speaks your language - every question and answer gets lost in translation. Mirko examines distributed microservice applications, how their complexity creates management problems, and what APM tools should do to automatically meet those challenges and deliver high performing applications."
  },
  {
    "Event": ".conf19",
    "Title": "BAS2766 - Machine Data Alchemy- Transforming Digital Exhaust into Campus Gold",
    "Track": "Business Analytics",
    "Industry": "Public Sector",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BAS2766.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BA2766.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Students, faculty and staff increasingly rely on technology for all aspects of campus activity. Universities must learn how to capture campus-wide data and transform it into value for everyone’s benefit. Clemson University will describe how they use data insights to deliver on student experience in a secure and connected setting. Clemson analyzed data from their Learning Management System (Canvas) using Splunk, so student and instructor activity is available in near real time which offers valuable feedback to the University. Learn how they drove additional speed and efficiency into other campus areas providing greater visibility of issues that consumed time and resources. Customer Support, IT security, and IT operations have also leveraged the Splunk platform to significantly expand and improve Clemson’s ability to make impactful decisions. We’ll share more details about these use cases so reserve your seat now for insights of your own!\nStudents, faculty and staff increasingly rely on technology for all aspects of campus activity. Universities must learn how to capture campus-wide data and transform it into value for everyone’s benefit. Clemson University will describe how they use data insights to deliver on student experience in a secure and connected setting. Clemson analyzed data from their Learning Management System (Canvas) using Splunk, so student and instructor activity is available in near real time which offers valuable feedback to the University. Learn how they drove additional speed and efficiency into other campus areas providing greater visibility of issues that consumed time and resources. Customer Support, IT security, and IT operations have also leveraged the Splunk platform to significantly expand and improve Clemson’s ability to make impactful decisions. We’ll share more details about these use cases so reserve your seat now for insights of your own!Students, faculty and staff increasingly rely on technology for all aspects of campus activity. Universities must learn how to capture campus-wide data and transform it into value for everyone’s benefit. Clemson University will describe how they use data insights to deliver on student experience in a secure and connected setting. Clemson analyzed data from their Learning Management System (Canvas) using Splunk, so student and instructor activity is available in near real time which offers valuable feedback to the University. Learn how they drove additional speed and efficiency into other campus areas providing greater visibility of issues that consumed time and resources. Customer Support, IT security, and IT operations have also leveraged the Splunk platform to significantly expand and improve Clemson’s ability to make impactful decisions. We’ll share more details about these use cases so reserve your seat now for insights of your own!Students, faculty and staff increasingly rely on technology for all aspects of campus activity. Universities must learn how to capture campus-wide data and transform it into value for everyone’s benefit. Clemson University will describe how they use data insights to deliver on student experience in a secure and connected setting. Clemson analyzed data from their Learning Management System (Canvas) using Splunk, so student and instructor activity is available in near real time which offers valuable feedback to the University. Learn how they drove additional speed and efficiency into other campus areas providing greater visibility of issues that consumed time and resources. Customer Support, IT security, and IT operations have also leveraged the Splunk platform to significantly expand and improve Clemson’s ability to make impactful decisions. We’ll share more details about these use cases so reserve your seat now for insights of your own! so reserve your seat now for insights of your own!"
  },
  {
    "Event": ".conf19",
    "Title": "FN1470 - Machine Learning & Splunk 2019- The Splunk Machine Learning Toolkit in Action",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1470.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1470.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Anomaly Detection, Predictive Analytics, and Clustering — oh my! Splunk customers want answers from their data, and machine learning is here to help. This session will help demystify the machine learning process, show how common machine learning themes are used for different outcomes at customers around the world, and give you next steps for achieving success at home by implementing machine learning! We aren’t talking about just science projects. We'll be giving examples and public details about Splunk’s Machine Learning Advisory successes over the years. Expect to leave with tangible examples you can implement back in the real world - if you can Escape from Vegas! "
  },
  {
    "Event": ".conf19",
    "Title": "IoT1318 - Maintaining a state of good repair with predictive analytics",
    "Track": "Internet of Things",
    "Industry": "Energy & UtilitiesPublic SectorOil & Gas",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1318.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IOT1318.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Take a deep dive in this enablement focused presentation where we cover the background, data and how to implement 3 Splunk solutions entirely captured in this sessions' companion app that shows how to use Splunk for maintaining a state of good repair, make data-driven decisions to garner rate payer confidence and proactively realize conservation goals.  The use cases covered in this session are: *** Corrosion Analytics - See how to use machine learning combined with ArcGIS, Maximo and Corrosion data to create an interactive map to predict pipe failures and replacement priorities based on proximity to sensitive infrastructure. *** Mobile Work Fleet - see how to use scripted inputs to develop asset management dashboards, make data driven purchasing decisions and optimize routes. *** Water Leak detection - see how Splunk's Machine Learning Toolkit can be used to easily detect anomalous consumption based on user behavior and automate alerting utilities and customers to prevent water waste.\n"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1852 - Make Compliance a Breeze with Splunk Enterprise Security",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1852.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1852.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "This session will give you the tools to tackle compliance with Splunk Enterprise Security. The session will showcase why you might want to grant different compliance views to your teams based on the compliance standard they are responsible for adhering to, and how to do so. We'll also cover how to present the compliance standards that a notable event relates to and how to grant your compliance officers visibility into only the notable events that are relevant to them."
  },
  {
    "Event": ".conf19",
    "Title": "SEC2372 - Make Your Security Tools Work Better Together Using Splunk's Adaptive Operations Framework",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2372.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2372.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Security architectures typically involve many layers of tools and products that are not designed to work together, leaving gaps in how security teams bridge multiple domains to coordinate defense. The Splunk Adaptive Operations Framework (AOF) addresses these gaps by connecting security products and technologies from our partners with Splunk security solutions including Splunk Enterprise Security (ES) and Splunk Phantom. Join this session to learn how the Splunk AOF benefits both users and security technology providers by enabling rich context for all security decisions, collaborative decision-making, and orchestrated actions across diverse security technologies. "
  },
  {
    "Event": ".conf19",
    "Title": "IT1931 - Marcus by Goldman Sachs- Monitoring an Online Banking Startup with Splunk",
    "Track": "IT Operations",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Enterprise",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1931.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1931.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Marcus by Goldman Sachs is an online, consumer lending and savings platform, often referred to as a startup within the 150-year-old company. The Marcus platform was designed and built from the ground up using the latest technologies and following agile software practices. Splunk software is used to monitor application and infrastructure logs and supports not only DevOps but also Development, QA, Production Support, and Security teams. This session will cover the challenges and successes we have experienced during our first years of rapid growth, the products and capabilities that we added to our platform this year, and provide a glimpse at the potential role of Splunk Next products in online retail banking use cases in the future.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FNC2751 - Master Joining Your Datasets Without Using Join",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FNC2751.pdf",
    "SkillLevel": "Advanced",
    "Description": "As you become more comfortable with Splunk, you can build complex searches that draw knowledge from very different sources to create reports that other products just cannot do. However your first instincts here will take you to the \"join\" and \"transaction\" commands. These commands will give you less-than-great performance as well as some confusing problems to work around. Come to this talk to learn how to avoid these commands *almost* entirely. We'll walk through search language examples starting with simple ones, where you would first start playing with 'join', and work our way up. In each case we'll cover why you would gravitate to join/transaction and what the problems are. Then what the techniques and the search syntax looks like to write a better faster report, and push most of the work out to the indexers. Not only will your searches run faster and your users be happier, but your administrators will be happy too with the lower system resources used!\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1407 - Master Search Speed",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1407.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1407.pdf",
    "SkillLevel": "Advanced",
    "Description": "Imagine improving the speed of your searches over 500k times faster and breathe new life into your Splunk environment without more hardware investment.  Learn how to use both time and segmentation with fast subsearches to quickly filter events for fast, advanced data correlation.  Based on the .conf17 talk “Fields, Indexed Tokens, And You\"\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN2069 - Maximizing permissioned blockchain throughput using Samsung SDS Accelerator and Splunk MLTK",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2069.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2069.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Blockchain scalability is one of the main barriers to adoption of this revolutionary new technology. Finance, supply chain, and e-commerce blockchain deployments often have peak throughputs that far exceed their baseline. For example, when tickets for a popular concert go on sale, the peak transaction throughput will result in unacceptable latency for the users. Samsung SDS Accelerator is a layer 2 scaling solution for Hyperledger Fabric that enables up to 10x transaction throughput during this burst of activity. Using Splunk MLTK, we’re able to detect and react to these bursts of activity without compromising the security guarantees of the underlying blockchain. "
  },
  {
    "Event": ".conf19",
    "Title": "SEC2105 - Measure What Matters to Streamline Security Operations with Splunk",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Cloud",
      "Splunk Machine Learning Toolkit"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2105.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2105.pdf",
    "SkillLevel": "Advanced",
    "Description": "To tame an event queue that's ballooning out of control, you need to know first which rules and data sources are generating a disproportionate number of alerts, and second the security value you're getting from those rules and data sources. Any changes made to rules or telemetry analyzed without that knowledge risk making your organization more vulnerable. In this session we'll discuss how Splunk empowers us to perform advanced analytics on everything from alert conversion rates to human time expenditure on alerts so that we can optimize all processes related to alerting. As long as we know what to measure and where to look, Splunk can help us tune our security operations centers to reduce monotony and false positives without diminishing our ability to detect actual threats.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN2268 - Metric Index- Evolution & Internals ",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2268.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2268.pdf",
    "SkillLevel": "Advanced",
    "Description": "Splunk’s metric index has changed a lot since we launched it back in Splunk Enterprise 7.0. In this latest iteration, we have upgraded our data model and metric index to natively ingest and store multiple metrics in a single event to further reduce its storage footprint and lower total cost of ownership. This session with provide a deep-dive into our latest metric index layout, its evolution since introduction in Splunk Enterprise 7.0, and how it varies from a log index storage layout."
  },
  {
    "Event": ".conf19",
    "Title": "FN1266 - Metrics- Past, Present, and Future",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1266.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1266.pdf",
    "SkillLevel": "Beginner",
    "Description": "Curious about how to efficiently onboard and analyze metric data in Splunk? This talk will teach you the basic design and best practices for Splunk's Metric Indexes. Since they were introduced two years ago, Splunk's metric capabilities have quickly evolved. Now there is support for rollups, richer logs-to-metrics conversion capabilities, and a more efficient data representation formats. We also will discuss planned future enhancements and how you may best prepare for them today."
  },
  {
    "Event": ".conf19",
    "Title": "FN1288 - Migrating Splunk to AWS- lessons learned (or how recover from being a victim of your own success)",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security"
    ],
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1288.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "At Red Hat, we’ve been using Splunk since 2014. Since then, usage has grown significantly, from utilizing Splunk Enterprise, to introducing Splunk Enterprise Security, all the way to having more than 600 TB of data residing in Splunk, with the daily data ingestion expected to grow beyond 2.x terabytes per day. At this level, the on-premises infrastructure was reaching its limits, so we started to look for alternatives. Our first reaction was, “Let’s throw some more hardware at it,” but we quickly came to the realization that if we were to continue on the path of implementing traditional infrastructure we wouldn’t solve our scalability problem. What did we do? Attend this talk to find out. We’ll describe our journey as we moved our Splunk environment to Amazon Web Services (AWS), share details about the architecture, review data migration tactics, and overview some of the pain and lessons we learned along the way."
  },
  {
    "Event": ".conf19",
    "Title": "FND1466 - Mind the gap!  The what, why and how of data bias, why you should avoid it and how you can save money or lives if you do.",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specificDiversity & Inclusion",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FND1466.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FND1466.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Why is the queue for the women’s restroom always longer than the men’s at a concert or theater? Data is fundamental to the modern world. From business decisions to economic development and public policy, we rely on data to allocate resources and make critical decisions. However, because so much of the data fails to take into account bias, such as gender and race, bias and discrimination are baked into our systems. This results in missed economic opportunities and tremendous costs in time, in money and, in some cases, lives. In this session we will explore examples of data bias based on studies that are eye-opening, informative, and will change the way you look at the world. We will look at the pitfalls that lead to poor data-driven decisions, and their outcomes. We also will explore the steps you can take to inform your own data driven decisions. And, yes, we will answer the question, “Why is the women’s queue is longer than the men’s?”"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1706 - Mission Control- A Day in the Life of a Security Analyst",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security",
      "Splunk User Behavior Analytics",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1706.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1706.pdf",
    "SkillLevel": "Beginner",
    "Description": "Join us to see the latest developments with Splunk’s Security Operations Suite. We’ll share background on the underlying architecture as well as a showcase of new features. Learn how your security use cases are solved with scale and performance.\n"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1803 - Modernize and Mature Your SOC with Risk-Based Alerting",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1803.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1803.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Today SOCs are in desperate need of a different alerting approach. Texas Instruments (TI) decided to transform its SOC by using risk-based alerting to generate fewer, higher fidelity alerts, and by aligning to the MITRE ATT&CK™ framework, which provides more situational awareness to analysts. This risk-based approach reduces false positives and the situational numbness associated with the legacy whitelisting process. Splunk and TI will walk you through TI's SOC successes as it transitioned to risk-based alerting. TI will detail a few real-life risk-based rule examples, discuss learning curves to fast track your transition, and discuss how MITRE ATT&CK™ fits in with this approach.  After this session, you will have the foundation to embark on your risk-based alerting journey, allowing you to increase detection mechanisms, increase your coverage of the ATT&CK™ techniques, and improve the overall effectiveness of your SOC."
  },
  {
    "Event": ".conf19",
    "Title": "IT2001 - Monitoring and troubleshooting workloads running on public cloud infrastructure made easy",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "Monitoring and troubleshooting infrastructure running on multiple public cloud environments can be a daunting task. In this session, we will demonstrate how Splunk App for Infrastructure can help you bring Metrics, Logs and Cloud Native Events to monitor the health of your environment in near real-time, troubleshoot failing entities and gain complete control over your public cloud infrastructure.\nMonitoring and troubleshooting infrastructure running on multiple public cloud environments can be a daunting task. In this session, we will demonstrate how Splunk App for Infrastructure can help you bring Metrics, Logs and Cloud Native Events to monitor the health of your environment in near real-time, troubleshoot failing entities and gain complete control over your public cloud infrastructure."
  },
  {
    "Event": ".conf19",
    "Title": "BA1593 - Monitoring the order fulfilment process within an automated warehouse using Splunk Business Flow",
    "Track": "Business Analytics",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Business Flow"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BA1593.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BA1593.pdf",
    "SkillLevel": "Beginner",
    "Description": "Attendees will see how Splunk Enterprise and Splunk Business Flow are used to monitor the order fulfilment process within an automated warehouse in real time. You also will see how proactive alerts inform the warehouse that a Key Performance Indicator (KPI) is under performing. Then, using process mining to investigate, find the root cause to solve issues before they become problems. This presentation will initially describe the workings of an automated warehouse and the data generated. It then will cover the process of ingesting this data into Splunk and configuring Splunk Business Flow to monitor processes and KPIs. Finally, the session will review the benefits to TGW and its customers from deploying Splunk Enterprise and Splunk Business Flow. Come to the session to see how the monitoring, alerting, and process mining functionality in Splunk Enterprise and Splunk Business Flow provides a deep insight into what is happening inside an automated warehouse."
  },
  {
    "Event": ".conf19",
    "Title": "IT1766 - Monitoring your VMWare vSphere Environment with Splunk",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1766.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1766.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Siloed IT monitoring for Virtualization struggles to scale as teams use different tools to monitor the same IT environment, increasing complexity and decreasing productivity. Imagine analysts, admins and virtualization teams collaborating and troubleshooting on a single platform. No more finger pointing, no more “my tool says”, no more data disagreements. Use Splunk for centralized infrastructure monitoring of your virtualized environment.\n\n \n"
  },
  {
    "Event": ".conf19",
    "Title": "ITS2583 - Moving Towards an Advanced Fusion Center",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Cloud",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/ITS2583.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/ITS2583.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Transform your SOC into a Cyber Fusion Center. Learn how to address alert fatigue and cut down on missed alerts by implementing AI technology in your SOC."
  },
  {
    "Event": ".conf19",
    "Title": "DEV1140 - Next Generation Data Ingestion and Preparation with Splunk",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Developer Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1140.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV1140.pdf",
    "SkillLevel": "Intermediate",
    "Description": "You think data ingestion into Splunk is cumbersome today? Don’t enjoy writing Technology Add-ons (TA) for specific use cases? Then this talk is for you!  We will walk through data ingestion using the data sources supported by the new Splunk Investigate wizard. This allows users of all levels to configure their data source and perform various manipulation functions on the ingested data to make sure it meets their use case. We will also go over the guiding principles of the underlying Data Stream Processing (DSP) pipeline which empowers the user to add their own customizations and send data to a variety of destinations.   We will compare this with current Splunk Enterprise data ingestion process by configuring a TA for a specific use case and then alter the ingested data to the desired format before sending it to an index. The user will be able to draw a contrast between the two approaches and see how it does not have to take up to 6 weeks to acquire and prepare data for analytics in Splunk. We hope this session leaves the user excited about data ingestion and prep.    "
  },
  {
    "Event": ".conf19",
    "Title": "IoT1897 - Next Generation Smart SOC",
    "Track": "Internet of Things",
    "Industry": "Communications",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1897.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IoT1897.pdf",
    "SkillLevel": "Advanced",
    "Description": "The mega event Expo brings together ideas, innovations, and inventions is will open its doors in the UAE on 20 October 2020 for a period of six months. This celebration of human ingenuity offers a glimpse into the future and is anticipated to attract 25 million visits, 70 percent of those visitors from 190 countries. The Expo 2020 Dubai is teaming up with the DarkMatter Group, which is the region’s first and only fully-integrated digital transformation, defense, and cybersecurity solutions provider, to fully deploy advanced cybersecurity technologies to oversee the entire digital platform, as well as the applications and data to secure the Expo 2020’s digital experience. This session will cover why Expo 2020 and DarkMatter chose Splunk as the right solution to reduce their operational requirements to single solution that is able to ingest and analyze events from every single asset (IT&IoT) supported by the automation frameworks in the solution."
  },
  {
    "Event": ".conf19",
    "Title": "DEV1377 - Not your parent's Splunk, an SDC journey",
    "Track": "Developer",
    "Industry": "Technology",
    "Products": [
      "Splunk Cloud",
      "Splunk Developer Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1377.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV1377.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Come join two old school Splunkers as we talk about our journey building our first app on Splunk Developer Cloud. We'll discuss the fun parts and the foibles, and hopefully show you that you can teach an old Pony new tricks. "
  },
  {
    "Event": ".conf19",
    "Title": "IT2949 - Observability Demystified ",
    "Track": "IT Operations",
    "Products": [],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT2949.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT2949.pdf",
    "Description": "What in the world is this observability stuff all about? How is it different from monitoring or the other things we’ve been doing? \n\nThis session will give you a crash course in observability, how to roll it out, how to practice and improve it, and how to ensure you’re getting the best bang for your buck. We’ll examine the goals, how people use the tools, and what benefits you can expect.\nWhat in the world is this observability stuff all about? How is it different from monitoring or the other things we’ve been doing? What in the world is this observability stuff all about? How is it different from monitoring or the other things we’ve been doing? What in the world is this observability stuff all about? How is it different from monitoring or the other things we’ve been doing? What in the world is this observability stuff all about? How is it different from monitoring or the other things we’ve been doing? What in the world is this observability stuff all about? How is it different from monitoring or the other things we’ve been doing? This session will give you a crash course in observability, how to roll it out, how to practice and improve it, and how to ensure you’re getting the best bang for your buck. We’ll examine the goals, how people use the tools, and what benefits you can expect.This session will give you a crash course in observability, how to roll it out, how to practice and improve it, and how to ensure you’re getting the best bang for your buck. We’ll examine the goals, how people use the tools, and what benefits you can expect.This session will give you a crash course in observability, how to roll it out, how to practice and improve it, and how to ensure you’re getting the best bang for your buck. We’ll examine the goals, how people use the tools, and what benefits you can expect.This session will give you a crash course in observability, how to roll it out, how to practice and improve it, and how to ensure you’re getting the best bang for your buck. We’ll examine the goals, how people use the tools, and what benefits you can expect.This session will give you a crash course in observability, how to roll it out, how to practice and improve it, and how to ensure you’re getting the best bang for your buck. We’ll examine the goals, how people use the tools, and what benefits you can expect."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1372 - One Size Fits None- Lessons from Implementing Compliance",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Aerospace & Defense",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1372.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1372.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "We will share our journey, lessons, and observations from the past year of implementing compliance at the MITRE Corporation. We'll recap our path from initially learning about Defense Federal Acquisition Regulation Supplement (DFARS), also known as NIST 800-171, to complying with it. We'll share insights from the process that may help you in your compliance journey, but we'll also discuss how your journey might be different than ours, as one size never fits all with compliance. "
  },
  {
    "Event": ".conf19",
    "Title": "FN2132 - Operating & Securing Hybrid Environments with Google Cloud & Splunk",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2132.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2132.pdf",
    "SkillLevel": "Intermediate",
    "Description": "In this session, we’ll explore how companies can adapt to multi-cloud environments using Google Cloud Anthos and Splunk Enterprise to maintain end-to-end visibility into these hybrid workloads. Google Kubernetes Engine (GKE) On-Prem part of Anthos brings the efficiency, speed, and scale of cloud to manage Kubernetes clusters in your datacenter. Combined with Splunk Connect for Kubernetes, we’ll show you how you get a single pane of glass to manage, monitor & secure your Kubernetes clusters across your organization.   We’ll also do a deep dive on Google Cloud Platform (GCP) security controls and how to export security findings from Cloud Security Command Center and cloud asset changes from Cloud Asset Inventory all into Splunk Enterprise for further forensic analysis, to accelerate incident resolution and ensure compliance. In this session, we’ll explore how companies can adapt to multi-cloud environments using Google Cloud Anthos and Splunk Enterprise to maintain end-to-end visibility into these hybrid workloads. Google Kubernetes Engine (GKE) On-Prem part of Anthos brings the efficiency, speed, and scale of cloud to manage Kubernetes clusters in your datacenter. Combined with Splunk Connect for Kubernetes, we’ll show you how you get a single pane of glass to manage, monitor & secure your Kubernetes clusters across your organization. In this session, we’ll explore how companies can adapt to multi-cloud environments using Google Cloud Anthos and Splunk Enterprise to maintain end-to-end visibility into these hybrid workloads. Google Kubernetes Engine (GKE) On-Prem part of Anthos brings the efficiency, speed, and scale of cloud to manage Kubernetes clusters in your datacenter. Combined with Splunk Connect for Kubernetes, we’ll show you how you get a single pane of glass to manage, monitor & secure your Kubernetes clusters across your organization. In this session, we’ll explore how companies can adapt to multi-cloud environments using Google Cloud Anthos and Splunk Enterprise to maintain end-to-end visibility into these hybrid workloads. Google Kubernetes Engine (GKE) On-Prem part of Anthos brings the efficiency, speed, and scale of cloud to manage Kubernetes clusters in your datacenter. Combined with Splunk Connect for Kubernetes, we’ll show you how you get a single pane of glass to manage, monitor & secure your Kubernetes clusters across your organization. In this session, we’ll explore how companies can adapt to multi-cloud environments using Google Cloud Anthos and Splunk Enterprise to maintain end-to-end visibility into these hybrid workloads. Google Kubernetes Engine (GKE) On-Prem part of Anthos brings the efficiency, speed, and scale of cloud to manage Kubernetes clusters in your datacenter. Combined with Splunk Connect for Kubernetes, we’ll show you how you get a single pane of glass to manage, monitor & secure your Kubernetes clusters across your organization. We’ll also do a deep dive on Google Cloud Platform (GCP) security controls and how to export security findings from Cloud Security Command Center and cloud asset changes from Cloud Asset Inventory all into Splunk Enterprise for further forensic analysis, to accelerate incident resolution and ensure compliance.We’ll also do a deep dive on Google Cloud Platform (GCP) security controls and how to export security findings from Cloud Security Command Center and cloud asset changes from Cloud Asset Inventory all into Splunk Enterprise for further forensic analysis, to accelerate incident resolution and ensure compliance.We’ll also do a deep dive on Google Cloud Platform (GCP) security controls and how to export security findings from Cloud Security Command Center and cloud asset changes from Cloud Asset Inventory all into Splunk Enterprise for further forensic analysis, to accelerate incident resolution and ensure compliance.We’ll also do a deep dive on Google Cloud Platform (GCP) security controls and how to export security findings from Cloud Security Command Center and cloud asset changes from Cloud Asset Inventory all into Splunk Enterprise for further forensic analysis, to accelerate incident resolution and ensure compliance."
  },
  {
    "Event": ".conf19",
    "Title": "ITS2633 - Operational Efficiencies with Splunk SmartStore",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/ITS2633.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/ITS2633.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Splunk SmartStore not only reduces the storage requirements for the aged data by maintaining only one copy of the data across the indexer cluster but significantly improves the operational aspects of the indexer cluster be it recovery from node failures or data rebalancing activities.  In this session, the speaker will go over the operational efficiency tests performed on traditional and SmartStore indexes and review the results.  "
  },
  {
    "Event": ".conf19",
    "Title": "SEC1506 - Our Splunk Phantom Journey- Implementation, Lessons Learned, and Playbook Walkthroughs",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1506.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1506.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Learn from our experience implementing Splunk Phantom so that you can speed up your automation journey.  We'll examine key decisions we made with our implementation and the good and the bad that resulted. We'll also cover our automation efforts in event triage, incident response and everything in between, with walkthroughs of our top playbooks. Additionally, we'll present how we tackled Splunk alert ingestion and what Phantom could look like in a cloud-first deployment."
  },
  {
    "Event": ".conf19",
    "Title": "FND1829 - Panel Discussion - Perspectives and Practical Skills for Men as Advocates for Gender Equity",
    "Track": "Foundations/Platform",
    "Industry": "Diversity & Inclusion",
    "Products": [
      "Splunk Developer Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FND1829.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FND1829.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Do you want to be an ally but don't know where or how to start? Diversity and inclusion in technology workplaces is not a women’s issue, or an issue relevant only to other underrepresented groups. Diversity and inclusion are business issues, and they are human issues. We know that businesses profit from the many benefits that diverse perspectives bring to innovation and company competitiveness. In this panel discussion, we aim to facilitate a conversation to better understand the barriers to advocacy, to promote best-practices for effective advocacy, and to enable sharing of first-hand experiences of successful advocacy.\n"
  },
  {
    "Event": ".conf19",
    "Title": "ITS2634 - Performance and scale testing of Splunk Enterprise",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/ITS2634.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/ITS2634.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": " What if you can scale test your Splunk Enterprise with multi-terabyte of data ingest followed by concurrent search tests on the loaded data?  Attend this session to learn more about the framework for multi-terabyte data generation and multi-user search tests on Splunk Enterprise 7.2+."
  },
  {
    "Event": ".conf19",
    "Title": "IT1734 - Porsche unleashes the Next Level of Operational Maturity with Splunk",
    "Track": "IT Operations",
    "Industry": "ManufacturingTravel & Transportation",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence",
      "Splunk Machine Learning Toolkit"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "Come learn how Porsche is transforming its IT services to better support business operations using a cutting edge data journey roadmap. By assessing its IT capabilities using a common maturity model and a comprehensive list of data-driven IT capabilities, Porsche was able to measure its maturity and reveal key gaps to produce the optimal data journey roadmap through a partnership with Splunk. Whether you’re a small business or a huge multi-national corporation, this method is of particular interest if your goal is to fuel measurable success across your business by using your data and the right sequence of IT use cases, capabilities, Splunk apps, technical add-ons, and KPIs based on your unique environment. Gain detailed insight from the Porsche experience to help build your own IT Operational Analytics journey."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1796 - Post-Pwn3D- Using Splunk Enterprise and Splunk Enterprise Security for Incident Response and Forensic Analysis",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1796.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1796.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "After breaches, incident response teams often end up with an overwhelming amount of forensic evidence data, including disk images, memory captures, PCAP, and more. We'll show you how one of our IR/forensics teams is ingesting this data into Splunk to answer the who, what, where, when and why of breaches. Our presentation will show you how to use Splunk Enterprise and Splunk Enterprise Security for Incident Response (IR) workflow tracking and reporting on multi-source forensic data captures."
  },
  {
    "Event": ".conf19",
    "Title": "SECS3101 - Power Up Your SOC- Using threat intelligence to bring your SOC ecosystem to the next level",
    "Track": "Security, Compliance and Fraud",
    "Products": [],
    "Description": "Do you ever wonder how your Security Operations Center stacks up to the rest of the industry? Come join us for our “Power Up Your SOC” presentation by Tony Lee, Sr. Technical Director for Cylance Professional Services. Tony will provide real-world examples of common pitfalls seen during SOC and SIEM build and optimization engagements. In addition to providing recommendations to avoiding these pitfalls, Tony will also discuss how best to assess your SOC to see how you compare to the latest trends and best practices. Finally we will finish with a use case covering how to improve your SOC by integrating applicable and actionable threat intelligence into your SOC.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT1722 - Predict Real World Outage using Splunk MLTK",
    "Track": "IT Operations",
    "Industry": "Communications",
    "Products": [
      "Splunk Machine Learning Toolkit"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1722.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1722.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Do you want to predict an outage before it happens? Are you wondering how to pursue the incremental journey to Artificial Intelligence Operations (AIOps)? This case study will reveal a real-world use case from T-Mobile USA and show you how to predict cell tower congestion in advance using Splunk Machine Learning Toolkit. In the age of binge watching on cell phones and wireless broadband services, cell congestion reduces speed and reliability and results in buffered video streaming and/or dropped calls that dents the use of services and the revenue. Building forecasting models for congestion requires correlation of several parameters including seasonal variations. Doing this on a large scale in real time takes significant resources. In this session, attendees will learn about the journey to build this predictive capability, including data analysis techniques, machine learning algorithms, benefits, and lessons learned. "
  },
  {
    "Event": ".conf19",
    "Title": "BA1966 - Product Overview-  Business Analytics with Business Flow",
    "Track": "Business Analytics",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BA1966.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BA1966.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Eager to get started with Splunk Business Flow? Splunk Business Flow is Splunk’s process mining solution for interactively exploring business processes to drive operational efficiencies. In this session, learn all the key concepts and terminology in Splunk Business Flow. You also will get a walkthrough of the major capabilities of Splunk Business Flow that will allow you to view a process flow, understand some basic metrics, explore and filter with attributes, and then dig into specific cases to diagnose potential anomalies."
  },
  {
    "Event": ".conf19",
    "Title": "SEC2638 - Protecting your Data- The 2020 Decennial Census and Data Security",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Public Sector",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2638.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2638.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "The Census is the nation’s largest peacetime mobilization effort and determines congressional representation. Census data is used by businesses, governments and civic organizations to inform decision-making and this year the Census is going mobile and online for the first time. This means that security is a top priority in ensuring the success of the 2020 Decennial. This segment of the conference will explore security related topics to include vulnerabilities, scalability and performance, with a special focus on Data Privacy, Compliance and Reputational Threat Management. If all things data and IT Security excite you, then this session is for you. Census executives Atri Kalluri and Zack Schwartz will provide a behind the scenes overview of the systems supporting the 2020 Decennial, including Splunk, and real world case studies on how the Census Bureau is adopting best practices across IT security and social media monitoring to ensure the security of respondent data. "
  },
  {
    "Event": ".conf19",
    "Title": "FN1096 - Public Data Exploration With Splunk",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1096.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1096.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Splunk is known as an excellent platform for exploring machine-generated data. We'll explore how the platform can be used on open datasets to search for insight on real world issues such as election participation, public health, and other topics. Takeaways will include considerations for data acquisition and ingest, domain expertise, and successes and challenges from the perspective of a civic-minded individual. Lets see how far Splunk can take us!"
  },
  {
    "Event": ".conf19",
    "Title": "SEC2186 - Pull up your SOCs 2.0",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2186.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2186.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Last year, after our outrageously successful talk \"Pull Up Your SOCs: A Splunk Primer on Building or Rebuilding your Security Operations\", we wanted to revisit this topic to cover changes in Security Operations that have taken place over the last 12 months. Whether you’re starting from scratch or rebuilding your security program, the first twelve months of standing up your security operations is absolutely critical to success. "
  },
  {
    "Event": ".conf19",
    "Title": "FN1590 - Putting the Seamless into SSO",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1590.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1590.pdf",
    "SkillLevel": "Advanced",
    "Description": "Ever wanted to create a seamless sign-on experience for your Splunk users, irrespective of when, where, who, or how many? If so come and listen to how Atlassian, the world leader in software collaboration tools, did just that. Atlassian’s in-house Splunk Enterprise cluster runs on Amazon Web Services and we wanted to take advantage of the latest and greatest authentication features released for the Application Load Balancer to provide that much-sought-after, seamless SSO experience for all our Splunk users. Join us as we share the technical wrangling required to make this magic happen and expand on how we leveraged the Splunk scripted authentication mechanism to introduce support for JSON web toke- based authentication. The cherry on top will be the chance to discover some of the unexpected things we learned along the way, because no tale is complete without a war story and everyone loves a great war story!"
  },
  {
    "Event": ".conf19",
    "Title": "DEV1173 - Python 3 Compatibility Dive- Don't Let Strings Byte You in the Apps",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1173.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV1173.pdf",
    "SkillLevel": "Advanced",
    "Description": "You knew it had to happen, Splunk is migrating to Python 3! We want this migration to be as painless as possible for apps and scripts developers, but it necessitates some compatibility requirements. This talk will dive into what parts of your apps and scripts will have to become Python 3 compatible. You’ll explore approaches to using Python community supplied backporting libraries as well as Python 2/3 compatible native syntax. By examining common and uncommon gotchas we found while migrating Splunk Enterprise, we will make sure you are prepared to run your code in the future generations of Splunk Enterprise!\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT1798 - Qualcomm’s Journey to World-Class IT Monitoring Using Splunk",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1798.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1798.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Qualcomm’s focus on inventing mobile technology breakthroughs relies on a complex, global IT infrastructure. In order to deliver on this mission, its IT team embarked on a journey using Splunk Enterprise and IT Service Intelligence to develop world-class IT monitoring approach and implement a framework for continuous improvements. Learn how Qualcomm got started with core Splunk for logging, evolved to include system metrics, and ITSI for core services. Qualcomm will present how it approaches achieving its major objectives, such as democratizing access to data, breaking down silos around teams to improve collaboration, reducing time to troubleshoot and recover incidents, and achieving better visibility and understanding around service performance."
  },
  {
    "Event": ".conf19",
    "Title": "IoT1442 - Real-time public transport information systems monitoring and analysis in the city of Amsterdam ",
    "Track": "Internet of Things",
    "Industry": "Non-ProfitPublic Sector",
    "Products": [
      "Splunk Enterprise",
      "Splunk for Industrial IoT"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1442.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IOT1442.pdf",
    "SkillLevel": "Intermediate",
    "Description": "With around a million people interacting with the public transport in and around the city of Amsterdam daily, supplying accurate travel information is critical for an efficient and well-informed public transportation experience. With four different transport types (boat, bus, tram, and metro) and a host of custom systems creating around 4.5 million update events per day, analyzing this travel information depends on everything working seamlessly together. To provide accurate and trustworthy real-time passenger information services, monitoring of not only information availability but quality is needed. This use case shows how Splunk was implemented to monitor the data streams at all points in the information chain, and how real-time insights and alerting allows for proactive problem resolution and provides high-quality real-time information services for the end-user.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT2091 - Real-world strategies for Kubernetes & docker integrations with Splunk",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT2091.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT2091.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Have you jumped on the Kubernetes train and are now thinking about your logging strategy? Or are you considering migrating your application to a microservices architecture like Docker and want to proactively plan your logging strategy? The data capture methods and data format can be significantly different from what you used in the past. Additionally, multi-line events need to be accounted for. There are a few different ways to ingest this data into Splunk. For example, Splunk 'Kubernetes Connect' leverages Fluentd behind the scenes. There also is a Splunk logging plugin for docker, and a syslog logging plugin. The Splunk Universal Forwarder also can be deployed on a sidecar. What the pros and cons with so many choices? This session will help you sort it all out."
  },
  {
    "Event": ".conf19",
    "Title": "IT2240 - Red Hat OpenShift and Splunk Better Together",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Business Flow"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT2240.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT2240.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Red Hat and Splunk are partners in providing solutions to manage and monitor Kubernetes deployments. This joint RedHat and Splunk session will cover the benefits of Red Hat OpenShift to run your Kubernetes deployments and how Splunk can provide the monitoring capabilities and insights you need to get the most out of your Kubernetes deployment.\n\n \n"
  },
  {
    "Event": ".conf19",
    "Title": "IT1186 - Remediation-as-a-Service with Splunk ITSI",
    "Track": "IT Operations",
    "Industry": "Financial ServicesTechnologyNot industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1186.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1186.pdf",
    "SkillLevel": "Intermediate",
    "Description": "A large volume of known production incidents and end-user issues are still being resolved manually. This becomes worse when it happens in unusual hours and no one is there to fix it. Every minute of downtime is detrimental to business and waking up an engineer at the middle of the night to fix simple known issues is not worthwhile. Our self-healing (Event-Driven Remediation) solution fixes simple and known incidents automatically. For complex alerts, it gets relevant diagnostics and context from the logs. This could massively improve customers' and employees' experience in any organization."
  },
  {
    "Event": ".conf19",
    "Title": "FND2245 - Retaining Talent and Striking a Work-Life Balance via Job-Sharing",
    "Track": "Foundations/Platform",
    "Industry": "Diversity & Inclusion",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FND2245.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FND2245.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Who says you can’t have it all? In today’s workplace, where the competition for talent is fierce, offering a true work/life balance is the key to keeping the best and the brightest as part of your organization. Based on more than 15 years of experiences across Fortune 500 companies and startups in the technology industry, Pam Sotnick and Katy Mann have been trailblazers in job-sharing and have shown how it works; not just for employees but also for organizations. Striking the right balance helps to retain talented women or men who need to reduce the number of hours devoted to their work life due to family responsibilities or other considerations. Job-sharing also can help to retain skilled workers who might otherwise choose to retire early or move to another organization. Come learn how job-sharing can work for your organization or how to make it work for you as an individual."
  },
  {
    "Event": ".conf19",
    "Title": "FN1190 - Running Splunk in an Air-gapped environment ",
    "Track": "Foundations/Platform",
    "Industry": "Public Sector",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1190.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1190.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Many government agencies and for-profit companies require that you run Splunk on a network disconnected from the outside Internet. This presents many challenges, including how to cross air gaps and one-way transfers, how to operate indexers in an air-gapped environment, and how to automate backwards. This session will cover lessons learned from a variety of air-gapped deployments."
  },
  {
    "Event": ".conf19",
    "Title": "FN1921 - Saving the Nation’s Food Supply with Data-Driven Analytics",
    "Track": "Foundations/Platform",
    "Industry": "Public SectorNot industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk User Behavior Analytics"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1921.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1921.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Copper River ES, a strategic partner for Splunk public sector, is working with a large federal agency that has restructured their NOC and SOC organizations into a single unified entity as part of operational optimization.  The agency is responsible for protecting IP and other assets totaling $4.3 trillion as part of safeguarding the nation’s food supply chain.   The goal was to enhance the ability to handle problem escalations quickly and improve communications between teams. They are currently ingesting more than 3TB daily across 65 data sources where Splunk is leveraged as an integrated data platform and framework service to act as a nerve center for the combined NOC and SOC teams. Implementation has resulted in dramatically reducing MTTD to an average of less than 30 min compared to previous times of up to 12 hours, MTTR times from 16 hours to often less than 1 with overall outage times having now been reduced by about 68%. From a security perspective, it is used to identify data exfiltration and insider threats, as well as for security operations and compliance.  Increasing visibility into all aspects of system operations and troubleshooting efforts is now supported through a series of custom Splunk App’s, glass tables, reports and alerts with operational guides and training to best leverage the capabilities Splunk has generated."
  },
  {
    "Event": ".conf19",
    "Title": "FN1045 - Saving Thousands of Hours Per Month at Paychex with Robotic Process Automation (RPA) and Splunk",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1045.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1045.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Robotic Process Automation (RPA) allowed Paychex to quickly eliminate over 18,000 hours of manual effort annually with only the first workflow, which was developed from start to finish in one month. Does your company have manual processes that could easily be automated through RPA efforts? If so, you could be saving thousands of hours and upwards of millions of dollars. In this session, we will show you how you can use RPA to deliver ROI and business value to your business units and executive-level management, and how Splunk can help monitor robot and workflow health and errors in real-time to developers. Robotic Process Automation (RPA) allowed Paychex to quickly eliminate over 18,000 hours of manual effort annually with only the first workflow, which was developed from start to finish in one month. Does your company have manual processes that could easily be automated through RPA efforts? If so, you could be saving thousands of hours and upwards of millions of dollars. In this session, we will show you how you can use RPA to deliver ROI and business value to your business units and executive-level management, and how Splunk can help monitor robot and workflow health and errors in real-time to developers.Robotic Process Automation (RPA) allowed Paychex to quickly eliminate over 18,000 hours of manual effort annually with only the first workflow, which was developed from start to finish in one month. Does your company have manual processes that could easily be automated through RPA efforts? If so, you could be saving thousands of hours and upwards of millions of dollars. In this session, we will show you how you can use RPA to deliver ROI and business value to your business units and executive-level management, and how Splunk can help monitor robot and workflow health and errors in real-time to developers.Robotic Process Automation (RPA) allowed Paychex to quickly eliminate over 18,000 hours of manual effort annually with only the first workflow, which was developed from start to finish in one month. Does your company have manual processes that could easily be automated through RPA efforts? If so, you could be saving thousands of hours and upwards of millions of dollars. In this session, we will show you how you can use RPA to deliver ROI and business value to your business units and executive-level management, and how Splunk can help monitor robot and workflow health and errors in real-time to developers.Robotic Process Automation (RPA) allowed Paychex to quickly eliminate over 18,000 hours of manual effort annually with only the first workflow, which was developed from start to finish in one month. Does your company have manual processes that could easily be automated through RPA efforts? If so, you could be saving thousands of hours and upwards of millions of dollars. In this session, we will show you how you can use RPA to deliver ROI and business value to your business units and executive-level management, and how Splunk can help monitor robot and workflow health and errors in real-time to developers.Robotic Process Automation (RPA) allowed Paychex to quickly eliminate over 18,000 hours of manual effort annually with only the first workflow, which was developed from start to finish in one month. Does your company have manual processes that could easily be automated through RPA efforts? If so, you could be saving thousands of hours and upwards of millions of dollars. In this session, we will show you how you can use RPA to deliver ROI and business value to your business units and executive-level management, and how Splunk can help monitor robot and workflow health and errors in real-time to developers."
  },
  {
    "Event": ".conf19",
    "Title": "SEC2120 - Scaling Splunk Enterprise Security",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2120.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2120.pdf",
    "SkillLevel": "Intermediate",
    "Description": "As the types of devices and applications used in IT organizations increase exponentially, scaling the analytics-driven SOC becomes even more imperative. In this session Splunk Professional Services will help you learn from its past experiences architecting Splunk Enterprise Security environments for scale into the terabytes per day. We will share technical details on improvements to search technology and Data Model Acceleration in Splunk Enterprise that will help you increase performance and decrease total cost of ownership. We will also take a deep dive under-the-hood into Splunk Enterprise Security Frameworks in which you should make special considerations for high volume.  Finally, we'll share important metrics on how to monitor the ongoing health of your Enterprise Security deployment, ensuring you stay on track over time, even in periods of rapid growth.\n"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1280 - Scary (Spooky-) Fast Intelligence-Based Hunting with Splunk Phantom",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1280.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1280.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Organizations today struggle with quickly and consistently applying behavior-based threat intelligence across their security tools. The hours needed to stitch together this information manually leave analysts unprepared to quickly turnaround questions from management about their vulnerability to threats that their management sees in the news. In this session we will demonstrate how to use Splunk Phantom to reduce that time lag by automating your threat hunts. Specifically, we will show you how to use Yet Another Recursive Algorithm (YARA) rules on endpoint and network security tools automatically and simultaneously. We will use a case study to show the benefits achieved from this playbook: better reporting, more robust procedures, faster time to detect malware variants, and generally more efficient and effective threat hunts."
  },
  {
    "Event": ".conf19",
    "Title": "FNS2892 - Secrets from a Splunk Ninja-  Deployment Architecture Best Practices",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FNS2892.mp4",
    "SkillLevel": "Good for all skill levels",
    "Description": "Ninjas know secrets.  Stealthy secrets hidden under black hoodies collected from the dark corners of the world about how to architect Splunk deployments the right way for optimal performance and scalability. Step out of the shadows of the clouds and learn how the best practices that power some of the largest, on-premises deployments of Splunk around the world.  Learn how Splunk premium apps impact architecture choices massively.  Determine if SmartStore is right for your environment and size it for maximum performance and benefit.  Discover exactly how you as a Splunk administrator can communicate your needs to IT operations and their vendors to insure your success.  Get your hoodie and toe-splitting shoes ready as you become an even better Splunk Ninja!"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1205 - Securing a Global Investment Fund Using Splunk Cloud and Splunk Enterprise Security",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Cloud",
      "Splunk Enterprise Security",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1205.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1205.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Join this session to learn the do’s and dont’s of rolling an effective cloud security visibility platform for a global organization. We will cover topics such as why we moved away from our previous SIEM provider, deploying and managing a cloud-based SIEM, and effectively using a third party organization to provide tier 1 and 2 event and incident support. "
  },
  {
    "Event": ".conf19",
    "Title": "SECS2856 - Securing the Intelligent Enterprise with SAP Enterprise Threat Detection and Splunk",
    "Track": "Security, Compliance and Fraud",
    "Products": [],
    "Description": "Looking to eliminate blind spots across your SAP environment and take proactive action to detect and mitigate attacks before mission-critical ERP applications are compromised? \n\nSAP recently teamed with Splunk to help a leading manufacturer build and validate a first-of-it’s-kind bi-directional integration between SAP Enterprise Threat Detection and Splunk. See a demo of how Enterprise Threat Detection’s open, extensible framework enables an exchange of alerts with Splunk to facilitate real-time attack investigations from either platform plus the ability to rapidly take action within the SAP landscape or broader heterogenous infrastructure.\n\nSAP Enterprise Threat Detection is a powerful native SAP HANA application that quickly identifies suspicious patterns at the application server and database level. When a potential SAP software-specific threat is identified, Enterprise Threat Detection can send an alert to Splunk to correlate with other application and infrastructure data for deeper investigation or trigger immediate action. Conversely, InfoSec teams using Splunk to rapidly identify anomalies across the broader security infrastructure can send alerts to Enterprise Threat Detection for forensics or to trigger appropriate actions in the SAP environment. The combination of Enterprise Threat Detection and Splunk enables organizations to more effectively combat security issues across the enterprise spanning applications and infrastructure.\nLooking to eliminate blind spots across your SAP environment and take proactive action to detect and mitigate attacks before mission-critical ERP applications are compromised? Looking to eliminate blind spots across your SAP environment and take proactive action to detect and mitigate attacks before mission-critical ERP applications are compromised? Looking to eliminate blind spots across your SAP environment and take proactive action to detect and mitigate attacks before mission-critical ERP applications are compromised? Looking to eliminate blind spots across your SAP environment and take proactive action to detect and mitigate attacks before mission-critical ERP applications are compromised? SAP recently teamed with Splunk to help a leading manufacturer build and validate a first-of-it’s-kind bi-directional integration between SAP Enterprise Threat Detection and Splunk. See a demo of how Enterprise Threat Detection’s open, extensible framework enables an exchange of alerts with Splunk to facilitate real-time attack investigations from either platform plus the ability to rapidly take action within the SAP landscape or broader heterogenous infrastructure.SAP recently teamed with Splunk to help a leading manufacturer build and validate a first-of-it’s-kind bi-directional integration between SAP Enterprise Threat Detection and Splunk. See a demo of how Enterprise Threat Detection’s open, extensible framework enables an exchange of alerts with Splunk to facilitate real-time attack investigations from either platform plus the ability to rapidly take action within the SAP landscape or broader heterogenous infrastructure.SAP recently teamed with Splunk to help a leading manufacturer build and validate a first-of-it’s-kind bi-directional integration between SAP Enterprise Threat Detection and Splunk. See a demo of how Enterprise Threat Detection’s open, extensible framework enables an exchange of alerts with Splunk to facilitate real-time attack investigations from either platform plus the ability to rapidly take action within the SAP landscape or broader heterogenous infrastructure.SAP recently teamed with Splunk to help a leading manufacturer build and validate a first-of-it’s-kind bi-directional integration between SAP Enterprise Threat Detection and Splunk. See a demo of how Enterprise Threat Detection’s open, extensible framework enables an exchange of alerts with Splunk to facilitate real-time attack investigations from either platform plus the ability to rapidly take action within the SAP landscape or broader heterogenous infrastructure.SAP Enterprise Threat Detection is a powerful native SAP HANA application that quickly identifies suspicious patterns at the application server and database level. When a potential SAP software-specific threat is identified, Enterprise Threat Detection can send an alert to Splunk to correlate with other application and infrastructure data for deeper investigation or trigger immediate action. Conversely, InfoSec teams using Splunk to rapidly identify anomalies across the broader security infrastructure can send alerts to Enterprise Threat Detection for forensics or to trigger appropriate actions in the SAP environment. The combination of Enterprise Threat Detection and Splunk enables organizations to more effectively combat security issues across the enterprise spanning applications and infrastructure.SAP Enterprise Threat Detection is a powerful native SAP HANA application that quickly identifies suspicious patterns at the application server and database level. When a potential SAP software-specific threat is identified, Enterprise Threat Detection can send an alert to Splunk to correlate with other application and infrastructure data for deeper investigation or trigger immediate action. Conversely, InfoSec teams using Splunk to rapidly identify anomalies across the broader security infrastructure can send alerts to Enterprise Threat Detection for forensics or to trigger appropriate actions in the SAP environment. The combination of Enterprise Threat Detection and Splunk enables organizations to more effectively combat security issues across the enterprise spanning applications and infrastructure.SAP Enterprise Threat Detection is a powerful native SAP HANA application that quickly identifies suspicious patterns at the application server and database level. When a potential SAP software-specific threat is identified, Enterprise Threat Detection can send an alert to Splunk to correlate with other application and infrastructure data for deeper investigation or trigger immediate action. Conversely, InfoSec teams using Splunk to rapidly identify anomalies across the broader security infrastructure can send alerts to Enterprise Threat Detection for SAP Enterprise Threat Detection is a powerful native SAP HANA application that quickly identifies suspicious patterns at the application server and database level. When a potential SAP software-specific threat is identified, Enterprise Threat Detection can send an alert to Splunk to correlate with other application and infrastructure data for deeper investigation or trigger immediate action. Conversely, InfoSec teams using Splunk to rapidly identify anomalies across the broader security infrastructure can send alerts to Enterprise Threat Detection for forensics forensics forensics or to trigger appropriate actions in the SAP environment. The combination of Enterprise Threat Detection and Splunk enables organizations to more effectively combat security issues across the enterprise spanning applications and infrastructure.or to trigger appropriate actions in the SAP environment. The combination of Enterprise Threat Detection and Splunk enables organizations to more effectively combat security issues across the enterprise spanning applications and infrastructure."
  },
  {
    "Event": ".conf19",
    "Title": "SEC2011 - Security Ninjutsu Part Six- Campfire Stories of Demons and Bad People",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "I always get a thrill when people come up to me after attending Ninjutsu events to share successes and lessons learned, but I feel bad, because I'm the only one who gets to hear those stories. So this year, we're going FULL lessons learned. We're going to hit on the best parts of the Ninjutsu Series (fear not if you're brand new) and tell you exactly what you need to know, from the folks who've deployed their learnings successfully. It's like the sports highlight reel for a lifetime of building security detections. That sounds like a party right? As always, attendance of prior Ninjutsus not required, though they are available at dvsplunk.com."
  },
  {
    "Event": ".conf19",
    "Title": "SECS2534 - Security visibility through Windows endpoint analytics",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SECS2534.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SECS2534.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Security requires visibility. uberAgent ESA provides just that. Built on top of the existing uberAgent User Experience Monitoring product, uberAgent Endpoint Security Analytics tags risky processes, detects potential threats resulting from configuration changes and provides deep insights into application and even script activity."
  },
  {
    "Event": ".conf19",
    "Title": "BA1188 - See What’s New With Splunk Business Flow",
    "Track": "Business Analytics",
    "Industry": "Online Services",
    "Products": [
      "Splunk Business Flow"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BA1188.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BA1188.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Business operations and process improvement teams have relied on Splunk for analyzing their business processes.  Earlier this year, Splunk launched Business Flow as a new solution for interactive discovery and investigation of any business process.  In this session, hear and see all of Splunk's latest innovations for business operations and process improvement professionals.\nBusiness operations and process improvement teams have relied on Splunk for analyzing their business processes.  Earlier this year, Splunk launched Business Flow as a new solution for interactive discovery and investigation of any business process.  In this session, hear and see all of Splunk's latest innovations for business operations and process improvement professionals."
  },
  {
    "Event": ".conf19",
    "Title": "IT1761 -  Service and Asset Discovery with Wire Data",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1761.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1761.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Your network is speaking to you! Listen to what your applications are saying. Monitoring the metrics already present in your wire data can provide the key to understanding and characterizing their performance. Using tools like Splunk Stream, you can collect dozens of metrics at the IP, TCP, and Application layers. This session will show you how to characterize the performance of your applications and the network, and how to tell which is the source of trouble. We'll also explore how to perform service and asset discovery with wire data as a basis of fact, correlating it with your database \"of record\" to ensure its accuracy."
  },
  {
    "Event": ".conf19",
    "Title": "FND1268 - She's the Boss- Female Leaders Smashing The Glass Ceiling ",
    "Track": "Foundations/Platform",
    "Industry": "Diversity & Inclusion",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FND1268.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FND1268.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Women are underrepresented across all levels of the technology industry. Find out how these four female leaders advanced their careers to lead the industry. Join us for an in-depth discussion about female diversity and the importance of including women in leadership from those who've made it to the top."
  },
  {
    "Event": ".conf19",
    "Title": "SECS2758 - Shift Identity Left- Using Splunk and Okta to Secure Automated Infrastructure Fleets",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SECS2758.mp4",
    "SkillLevel": "Intermediate",
    "Description": "Are you deploying infrastructure with static accounts and credentials baked in? Dynamic infrastructure needs dynamic access controls to match, especially at scale. This talk will cover best practices for automating infrastructure identity and least privilege access across large-scale distributed systems using Okta, feeding real-time authentication events into Splunk for better visibility into who is logging into which systems and from where."
  },
  {
    "Event": ".conf19",
    "Title": "FN1328 - Show and Tell- Prescriptive Use Cases for Azure and Office 365",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk Machine Learning Toolkit"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1328.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1328.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Let's face it, sometimes you don't know what you don't know. With vast amounts of cloud data coming in at cloud-speed, it can be difficult to see through the noise and know what to look for. Are malicious adversaries attempting to comprise the environment? Is my environment under- or over-provisioned? Do I have an insider possibly exfiltrating company data? Are employees actually using the services? What is all of this costing per service, department, business unit? Don't worry, we will help you figure all this out in a prescriptive manner by showcasing these and other use cases. Then, we will show you the \"how\" by exposing the searches, the data needed, and showing you how to onboard that data. You will walk away with use cases that can be implemented immediately in your own environment."
  },
  {
    "Event": ".conf19",
    "Title": "FN1435 - Sizing Splunk SmartStore- Spend Less and Get More Out of Splunk",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1435.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1435.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Data is growing exponentially; however IT budgets are not.  Growth in internal use cases and additional data sources can put organizations under intense pressure to manage spiraling costs. The good news is that help is on the way. We will show how to size and configure Splunk SmartStore to yield significant cost savings, for both current and future data growth. In addition, learn how to configure the Splunk deployment for optimal search performance. Spare a few minutes of your time at .conf19 and see it yield big returns for your organization.\nData is growing exponentially; however IT budgets are not.  Growth in internal use cases and additional data sources can put organizations under intense pressure to manage spiraling costs. The good news is that help is on the way. We will show how to size and configure Splunk SmartStore to yield significant cost savings, for both current and future data growth. In addition, learn how to configure the Splunk deployment for optimal search performance. Spare a few minutes of your time at .conf19 and see it yield big returns for your organization.Data is growing exponentially; however IT budgets are not.  Growth in internal use cases and additional data sources can put organizations under intense pressure to manage spiraling costs. The good news is that help is on the way. We will show how to size and configure Splunk SmartStore to yield significant cost savings, for both current and future data growth. In addition, learn how to configure the Splunk deployment for optimal search performance. Spare a few minutes of your time at .conf19 and see it yield big returns for your organization.Data is growing exponentially; however IT budgets are not.  Growth in internal use cases and additional data sources can put organizations under intense pressure to manage spiraling costs. The good news is that help is on the way. We will show how to size and configure Splunk SmartStore to yield significant cost savings, for both current and future data growth. In addition, learn how to configure the Splunk deployment for optimal search performance. Spare a few minutes of your time at .conf19 and see it yield big returns for your organization."
  },
  {
    "Event": ".conf19",
    "Title": "FN2168 - Smart Store Deep Dive ",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2168.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2168.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Dive into the inner workings of SmartStore. In this talk we'll go over how SmartStore works internally with ties to Indexer Clustering, and what decisions the CacheManager makes, e.g., when do we upload/download from the remote storage, etc. We'll also go over the performance numbers that we've seen!"
  },
  {
    "Event": ".conf19",
    "Title": "IoT2026 - Solar Analytics- Changing the World One Solar Panel At A Time",
    "Track": "Internet of Things",
    "Industry": "Energy & UtilitiesNon-Profit",
    "Products": [
      "Splunk Enterprise",
      "Splunk for Industrial IoT"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT2026.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IoT2026.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Solar generation and energy storage is dramatically dropping in cost and is being deployed on a massive scale across the globe. However, system monitoring and diagnostics, cost reporting, and usage can be difficult. With Splunk Essentials for ICS (Industrial Control Systems) and Splunk IAI (Industrial Asset Intelligence), a smart energy solution easily can be created. Does it seem too hard to onboard data? See how we use the Splunk add-on builder to create technology add-ons for solar panels and batteries. Want better visibility? Using Splunk IAI, we can gain faster insights into the performance of solar panels, inverters, and battery systems. Want to understand the economics? By applying business analytics, we can easily report of revenue, costs, and total return on investment. This solution can be applied to both small and large solar and energy storage installations, and we have a real world use case with some exciting energy data!\n"
  },
  {
    "Event": ".conf19",
    "Title": "SECS2899 - Solving Endpoint Security & Perimeter Blindness with Splunk – Lessons from Cisco’s Internal InfoSec Deployment ",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Cloud",
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SECS2899.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SECS2899.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Endpoint security is more than detecting malware.  Most insider threats, however, don’t involve malware, but other security issues associated with the user and endpoint.  Learn how Cisco’s own InfoSec team uses Cisco Endpoint Security Analytics Built on Splunk and Cisco NGFW integration to increase its endpoint security and threat visibility.\nEndpoint security is more than detecting malware.  Most insider threats, however, don’t involve malware, but other security issues associated with the user and endpoint.  Learn how Cisco’s own InfoSec team uses Cisco Endpoint Security Analytics Built on Splunk and Cisco NGFW integration to increase its endpoint security and threat visibility.Endpoint security is more than detecting malware.  Most insider threats, however, don’t involve malware, but other security issues associated with the user and endpoint.  Learn how Cisco’s own InfoSec team uses Cisco Endpoint Security Analytics Built on Splunk and Cisco NGFW integration to increase its endpoint security and threat visibility."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1998 - Solving the Opioid Crisis and Protecting Patient Privacy – using Splunk",
    "Track": "Security, Compliance and Fraud",
    "Industry": "HealthcareNon-Profit",
    "Products": [
      "Splunk Enterprise"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "Patient privacy is both a public health and regulatory risk issue. Join this session to learn how we use Splunk to monitor patient privacy and reduce drug diversion—the illicit exfiltration of drugs from our hospital. We'll cover the impact our partnership with Splunk makes on our Healthcare Delivery Organization, and we'll also discuss new use cases for Splunk, such as determining antimicrobial resistance and monitoring antiarrhythmic medications.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT1046 - Soup to Nuts SRE- How to leverage ITSI, VictorOps and Phantom to be a site reliability engineering super hero",
    "Track": "IT Operations",
    "Industry": "CommunicationsOnline ServicesTechnologyNot industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence",
      "Phantom",
      "VictorOps"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1046.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1046.pdf",
    "SkillLevel": "Advanced",
    "Description": "Site Reliability Engineering: Easy to say, harder to do. It can be especially difficult to make sure that all of tenants of SRE are applied to the services you support in a way that is easy for your engineers to adopt. In this session, we will take a look at how you can use Splunk's ITSI, VictorOps and Phantom platforms to make robust solutions that can help your teams consistently solve complex problems and mature their services.\nSite Reliability Engineering: Easy to say, harder to do. It can be especially difficult to make sure that all of tenants of SRE are applied to the services you support in a way that is easy for your engineers to adopt. In this session, we will take a look at how you can use Splunk's ITSI, VictorOps and Phantom platforms to make robust solutions that can help your teams consistently solve complex problems and mature their services."
  },
  {
    "Event": ".conf19",
    "Title": "FN2188 - Speed up your search! ",
    "Track": "Foundations/Platform",
    "Industry": "Non-ProfitNot industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2188.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2188.pdf",
    "SkillLevel": "Beginner",
    "Description": "\"I'm completely satisfied with my search speed and there is nothing to improve\" said no one, ever. The majority of internet users expect websites to load within two seconds. Why should Splunk dashboards and the underlying searches be any different? No one should be satisfied by searches that take minutes to complete. In this talk, we'll cover all the different strategies to take slow searches and speed them up. This is an updated talk from .conf2017.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1300 - SPLendid uses for SPL in SPLunk",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "SkillLevel": "Intermediate",
    "Description": "This session will review my top 10 SPL tips to cure what ails you. There are thousands of ways to solve a problem with SPL. Do you use a stats command by _time after a bucket, or do you use a time chart? A join, or an event stats? My Jira ticket doesn't get enough detail. How do I suppress my alerts when I know a downstream system is broken? We will walk you through our top ten SPL commands and examples that we have in our permanent clipboard for quick access. Good SPL programmers borrow from others, great SPL programmers steal outright.\nThis session will review my top 10 SPL tips to cure what ails you. There are thousands of ways to solve a problem with SPL. Do you use a stats command by _time after a bucket, or do you use a time chart? A join, or an event stats? My Jira ticket doesn't get enough detail. How do I suppress my alerts when I know a downstream system is broken? We will walk you through our top ten SPL commands and examples that we have in our permanent clipboard for quick access. Good SPL programmers borrow from others, great SPL programmers steal outright.This session will review my top 10 SPL tips to cure what ails you. There are thousands of ways to solve a problem with SPL. Do you use a stats command by _time after a bucket, or do you use a time chart? A join, or an event stats? My Jira ticket doesn't get enough detail. How do I suppress my alerts when I know a downstream system is broken? We will walk you through our top ten SPL commands and examples that we have in our permanent clipboard for quick access. Good SPL programmers borrow from others, great SPL programmers steal outright.This session will review my top 10 SPL tips to cure what ails you. There are thousands of ways to solve a problem with SPL. Do you use a stats command by _time after a bucket, or do you use a time chart? A join, or an event stats? My Jira ticket doesn't get enough detail. How do I suppress my alerts when I know a downstream system is broken? We will walk you through our top ten SPL commands and examples that we have in our permanent clipboard for quick access. Good SPL programmers borrow from others, great SPL programmers steal outright.This session will review my top 10 SPL tips to cure what ails you. There are thousands of ways to solve a problem with SPL. Do you use a stats command by _time after a bucket, or do you use a time chart? A join, or an event stats? My Jira ticket doesn't get enough detail. How do I suppress my alerts when I know a downstream system is broken? We will walk you through our top ten SPL commands and examples that we have in our permanent clipboard for quick access. Good SPL programmers borrow from others, great SPL programmers steal outright."
  },
  {
    "Event": ".conf19",
    "Title": "FND1986 - Splunk Accessibility -  Splunking made accessible for users with disabilities",
    "Track": "Foundations/Platform",
    "Industry": "Non-ProfitPublic SectorDiversity & Inclusion",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FND1986.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1986.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Have you ever wondered how people with disabilities use Splunk ? Ever wondered what it would be like to perform searches in Splunk using a screen reader or a speech recognition application ? Then come to this session to learn about the enhancements made to the Splunk platform that makes it easy to use with assistive technology tools. We will delve into the experience of performing investigation in Splunk using a screen reader and discuss the enhancements made to the platform to make this a superior experience. You will learn about the work we are doing at Splunk to address compliance to the WCAG, Section 508, and EN 301 549 accessibility standards. Finally you will hear how our customers are using Splunk with assistive technology tools to accomplish their day-to-day work.\n"
  },
  {
    "Event": ".conf19",
    "Title": "ITS2895 - Splunk Apps for Infrastructure from Dell EMC",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/ITS2895.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/ITS2895.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Monitoring infrastructure operations is easy with Splunk, but it takes more than great SPL skills to build and end to end view of infrastructure supporting your mission critical applications.  In this session, you'll hear from an experienced Site Reliability Engineer how Dell EMC's investment in Splunk apps across their platforms makes it easier for Splunk users to monitor infrastructure and integrate these these insights into an overall application performance monitoring strategy.  "
  },
  {
    "Event": ".conf19",
    "Title": "IT1717 - Splunk as a Tool for DevOps Acceleration",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence",
      "VictorOps"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1717.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1717.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "DevOps adoption requires high performing teams. One of the biggest challenges organizations have when adopting a DevOps framework is how to get early wins, get value early in the process, and overcome plateaus in adoption. These improvements are typically achieved through the use of automation, improved responsiveness, better situational awareness, and increased sharing between teams. As you will see in this session, Splunk easily sits in the middle of all of this."
  },
  {
    "Event": ".conf19",
    "Title": "IoT1937 - Splunk at the Speed of Flight – Delivering Critical Passenger Cabin Services ",
    "Track": "Internet of Things",
    "Industry": "Travel & Transportation",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence",
      "Splunk for Industrial IoT"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1937.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IOT1937.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Business General Aviation (BGA) relies on multiple critical communications services, operating in flawless concert, for a safe and successful flight.  In-Flight Connectivity (IFC) is a critical BGA cabin service driving the industry. Learn how SatCom Direct is using Splunk to capture and cross-correlate live KPI feeds from various sources such as aircraft flight dynamics, satellites and ground stations to provide the ultimate in-air experience from a single aircraft to an entire managed fleet. Armed with Splunk, Satcom Direct can immediately detect and work to restore any service impacting events and from the same data, develop improved insights for ongoing performance enhancements. From the executive suite to the operations battleground, come learn how Splunk leverages data and insights at the speed of flight.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1452 - Splunk Autobahn - SaaS proof of value program- from 0 to HERO",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Cloud",
      "Splunk Enterprise Security",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1452.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1452.pdf",
    "SkillLevel": "Beginner",
    "Description": "You already know Splunk is amazing, but now you have to prove this to someone in your organization before you get the keys to your Splunky Supercar. Good news - we've got a solution that gets your data - not fake data - into Splunk Cloud, and makes it immediately accessible, useable and valuable to everyone in your organization. You'll be able to kick-the-tires on your new data analytics engine for 30 days at no cost. Attend this session to learn more.... Let me introduce you to the Autobahn, Splunk-style."
  },
  {
    "Event": ".conf19",
    "Title": "BA1710 - Splunk Business Flow for IT Operations Workflows",
    "Track": "Business Analytics",
    "Industry": "Technology",
    "Products": [
      "Splunk Enterprise",
      "Splunk Business Flow"
    ],
    "SkillLevel": "Beginner",
    "Description": "Everyone knows Splunk can give you visibility into incidents – but imagine if Splunk could provide visibility into your CI/CD processes before you go live too! In this session, get a quick overview of how Splunk Business Flow can help anyone in IT operations identify opportunities to optimize and accelerate their organization’s IT operational workflows."
  },
  {
    "Event": ".conf19",
    "Title": "BA1529 - Splunk Business Flow + Splunk Connect for Kubernetes = Happy Containers",
    "Track": "Business Analytics",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Business Flow"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BA1529.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BA1529.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Managing container deployment is the new thing for IT and Splunk is making it easier to monitor your container deployment processes. In this theater session, see how Splunk Business Flow combined with Splunk Connect for Kubernetes can help IT administrators get immediate visibility into Kubernetes deployments to spot delays and failures."
  },
  {
    "Event": ".conf19",
    "Title": "IoT2038 - Splunk Cloud Gateway and Connected Experiences",
    "Track": "Internet of Things",
    "Industry": "Aerospace & Defense",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT2038.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IoT2038.pdf",
    "SkillLevel": "Beginner",
    "Description": "At PCAPP we use Splunk Cloud Gateway and the Connected Experiences Suite to help operators, supervisors, and managers ensure safe, compliant destruction of chemical weapons."
  },
  {
    "Event": ".conf19",
    "Title": "FN1995 - Splunk Cloud- optimizing performance, value and user experience",
    "Track": "Foundations/Platform",
    "Industry": "Technology",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1995.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1995.pdf",
    "SkillLevel": "Intermediate",
    "Description": "This session will cover how to overcome any missing insights in services and processes in Splunk Cloud. You'll learn how to develop alerts and dashboards to show performance and infrastructure health, and an API framework to retrieve data from the cloud infrastructure. We also will show you how to automate retrieval of all entry points to the Splunk on-premises infrastructure, and monitor connections to the intermediate forwarders, deployment servers, HEC and DBX servers, and syslog. In addition, we will address the lack of standardization principles for your on-premises Splunk Infrastructure, how to create gold standard alerts and dashboards, and how to create scripted inputs to gather system information. Finally, we will show you how to address the lack of a centralized view for managing your Splunk deployment server infrastructure, and how to automate the retrieval of deployment clients, server classes, and TA’s from deployment servers and feed the output to Splunk Cloud."
  },
  {
    "Event": ".conf19",
    "Title": "FN1647 - Splunk Cloud's Silver Lining",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Cloud",
      "Splunk Enterprise Security",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1647.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1647.pdf",
    "SkillLevel": "Beginner",
    "Description": "Are you considering moving to Splunk Cloud? This session will show you the benefits of migrating to Splunk Cloud and letting Splunk do all the heavy lifting so you can focus on getting value from your data. We also will go into what makes the Splunk Cloud service unique. "
  },
  {
    "Event": ".conf19",
    "Title": "IoT1603 - Splunk electrified- Building a modular application for the new Porsche Taycan",
    "Track": "Internet of Things",
    "Industry": "Manufacturing",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IOT1603.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Twenty years ago, IT was seen as a cost center. Ten years ago it was a business enabler. Today we see IT as a fundamental part of Porsche’s strategy to build the sports car of the future. Our approach with Splunk also has changed. Machine data is no longer seen as an afterthought. We partnered with the business, we understood their needs, we learned to speak their language, and we incorporated Splunk from the beginning. And it's not limited to cars. IT is included across the full digital journey of our customers. We will give you insights into the digitalization of the new Porsche Taycan, Porsche’s first all-electrically powered four-seat sports car which will be introduced in September 2019. We will share best practices as to how we mastered being heard within various business units, overcame legal challenges , and showcase an example of our digitalized charging infrastructure which analyzes the right data for monitoring as well as predicting performance and forecasting demands. "
  },
  {
    "Event": ".conf19",
    "Title": "IoT1401 - Splunk for Good, Seattle University, and University of Connecticut partner to Power Aquaponics Facilities Around the Globe",
    "Track": "Internet of Things",
    "Industry": "Higher EducationNon-Profit",
    "Products": [
      "Splunk Enterprise",
      "Splunk Mobile"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1401.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IOT1401.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Wouldn’t you love to be able to give students at your university some hands-on experience with Splunk so they have a tangible skill to add to their resume when they’re looking for jobs? What if we told you that two universities on opposite sides of the United States have partnered with Splunk4Good and Splunk Mobile and done just that. The University of Connecticut and Seattle University are working together on sustainability efforts at aquaponics facilities around the globe and collecting data from them. During this talk you will learn some useful tools, tips, and tricks for how they got there, and how you can get your university enabled with Splunk4Good. Finally, we’ll talk about what you can do to teach students about Splunk and sustainability, and why that’s so important in 2019.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT1448 - Splunk for NewOps – Using Data-Driven IT Operations to Better Manage IT Systems at Scale",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk IT Service Intelligence",
      "Splunk Machine Learning Toolkit",
      "VictorOps"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1448.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1448.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Splunk is increasingly at the forefront of new approaches to IT Operations, especially in disruptive ‘cloud-native’ businesses. This session will help you understand how ‘New Ops’ techniques like Observability, Site Reliability Engineering, SLOs/SLIs, Error Budgets, ChatOps, and Blameless Post-Mortems can help your IT Ops team; and how you can adopt ‘New Ops’ technologies like Containers, Microservice Architectures, Machine Learning, Orchestration, Predictive Analytics, and AI for IT Ops."
  },
  {
    "Event": ".conf19",
    "Title": "IoT2578 - Splunk Fortifies ABB Ability Industrial Cloud Foundation",
    "Track": "Internet of Things",
    "Industry": "Manufacturing",
    "Products": [
      "Splunk Enterprise",
      "Splunk for Industrial IoT"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT2578.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IoT2578.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "As one of the world's largest and most trusted industrial brands, our customers demand the most technologically advanced and reliable systems to operate the most critical assets on the planet. Downtime for our customers is never an option. As we migrate our business delivery backbone to the cloud, we have to enforce the same level of reliability and performance on the architecture that supports our services - .9999 is failure where we live. In this presentation learn how ABB Digital and Splunk have collaborated to create new insights into the performance, security and operations of our Azure-backed ABB Digital IOT Platform to protect our brand and increase the velocity of services that we can bring to our internal customers and to those that invest in our ABB branded services."
  },
  {
    "Event": ".conf19",
    "Title": "IT1859 - Splunk Implementation as a Customer Impact Analysis Platform for Mission Critical Core Network Service",
    "Track": "IT Operations",
    "Industry": "Communications",
    "Products": [
      "Splunk Enterprise"
    ],
    "SkillLevel": "Intermediate",
    "Description": "NTT Com Engineering Corporation is a subsidiary company to one of the largest global ICT solution providers in the world, NTT Communications, and we mainly support NTT groups long range telecommunication service within Japan and across countries. Our challenges in this market keep changing over time, but the most essential part remains the same, that is “Customer Satisfaction.” We applied Splunk to manage our deeply stacked B to B/C network service infrastructure which reduces time required for identifying the specific customers’s network lines which is affected by network layer outages. In this session, we will cover how we implemented this system through live demonstration."
  },
  {
    "Event": ".conf19",
    "Title": "FN1568 - Splunk in a Blockchain world- Development--Monitoring--Alerting--Repeat",
    "Track": "Foundations/Platform",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1568.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1568.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Are you looking into Blockchain/DLT solutions but not sure how you plan to monitor and alert your internal teams? What are the components and areas on which my team should be focused? How do I turn my team from reactive to proactive? Is monitoring an afterthought that becomes the responsibility of your support teams? In this session we will deep dive into the components of DLT , dashboards, and how to embed a monitoring mindset into your teams.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FNS2956 - Splunk in AWS- Learn how organizations leverage Splunk to power innovation and cloud adoption",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Cloud"
    ],
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FNS2956.pdf",
    "SkillLevel": "Beginner",
    "Description": "AWS cloud adoption continues at a rapid pace. Enterprises who migrate their application workloads to the cloud recognize 30-40% reduction in IT overhead costs, resource flexibility, increased speed and innovation. Throughout their cloud adoption journey, enterprises need to manage existing applications and have full visibility over their performance and security.\n\nSplunk’s platform built for real-time data management offers native applications for monitoring and analysis. Built on AWS, Splunk Cloud remains the customer’s choice solution for aggregating their data related use cases while seamlessly supporting their existing applications across both on-premises and the cloud. In this session we will share how Splunk’s data management, monitoring and analytics capabilities across IT Infrastructure, Security, and Business Analytics use cases help enterprises adopt cloud."
  },
  {
    "Event": ".conf19",
    "Title": "FN1679 - Splunking Crime Part II - Analysing Bias in Police Actions",
    "Track": "Foundations/Platform",
    "Industry": "Non-Profit",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1679.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1679.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Last year at .conf18 we used Splunk and Machine Learning Toolkit (MLTK) to analyze and predict the crime in London. This year we are taking a step forward and analyzing the bias in the police actions in the U.K. We will use police, population, religion, and race data to understand how police use their powers in different areas on people from different racial backgrounds. We will use open data sources and index them in Splunk. Using advanced visualizations we will analyze the data and understand more about police actions. Then using MLTK we will create a predictive model for crimes and then analyze the model for any bias due to the data provided. Machine bias is a real issue nowadays when machine learning algorithms are increasingly being used by government agencies to predict crime and even pass sentences on convicts. We need to understand that along with having positive impact of predicting crime, it can have a long-lasting negative impact as well.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT1990 - “Splunking” IBM i Data to Power a Complete View of Your Infrastructure",
    "Track": "IT Operations",
    "Industry": "Financial ServicesHealthcareRetailTechnologyTravel & TransportationNot industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1990.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1990.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "IBM i systems are used by many customers in a number of industries such as banking, retail, transportation and hospitality. Splunk is a perfect tool for consolidating and analyzing event, security, performance, and application data from these critical IBM i systems. Having a single and complete view of infrastructure data allows operations, security, and performance analysts to quickly identify and correlate operational, security, and performance issues. Syncsort Ironstream™ for Splunk can easily capture the needed event, security, and performance data in real time. Detailed performance data from these IBM i systems combined with Splunk’s machine learning algorithms can provide a new degree of confidence in capacity planning."
  },
  {
    "Event": ".conf19",
    "Title": "FN2285 - Splunking refugees with help from NetHope and Cisco",
    "Track": "Foundations/Platform",
    "Industry": "Non-Profit",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2285.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2285.pdf",
    "SkillLevel": "Intermediate",
    "Description": "It may not the big news, but there are 25 million refugees right now according to the UN Refugee Agency. Cisco Meraki has generously donated network equipment for NetHope to provide internet connectivity to refugee camps. Learn how NetHope sends various data to Splunk to add visibility to the camp's network infrastructure and take traditional IT data for non-traditional analytics on refugees.\nIt may not the big news, but there are 25 million refugees right now according to the UN Refugee Agency. Cisco Meraki has generously donated network equipment for NetHope to provide internet connectivity to refugee camps. Learn how NetHope sends various data to Splunk to add visibility to the camp's network infrastructure and take traditional IT data for non-traditional analytics on refugees.It may not the big news, but there are 25 million refugees right now according to the UN Refugee Agency. Cisco Meraki has generously donated network equipment for NetHope to provide internet connectivity to refugee camps. Learn how NetHope sends various data to Splunk to add visibility to the camp's network infrastructure and take traditional IT data for non-traditional analytics on refugees."
  },
  {
    "Event": ".conf19",
    "Title": "DEVS4G2287 - Splunking the 2018 Midterm Election!",
    "Track": "Developer",
    "Industry": "Non-ProfitPublic Sector",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEVS4G2287.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEVS4G2287.pdf",
    "SkillLevel": "Advanced",
    "Description": "The Federal Election Commission (FEC) is an independent regulatory agency whose purpose is to enforce campaign finance law in US federal elections. The FEC provides a REST API to query all campaign data of every candidate. By collecting and analyzing the direct and indirect (Super PAC) contributions, Splunk can show the relative influence of each candidate of the midterm. Also learn how Splunk powers the Splunk for Good midterm website using Splunk's REST API, HEC, and Amazon S3 hosting. This talk is an update of the 2016 Presidential Election talk from .conf16.\nThe Federal Election Commission (FEC) is an independent regulatory agency whose purpose is to enforce campaign finance law in US federal elections. The FEC provides a REST API to query all campaign data of every candidate. By collecting and analyzing the direct and indirect (Super PAC) contributions, Splunk can show the relative influence of each candidate of the midterm. Also learn how Splunk powers the Splunk for Good midterm website using Splunk's REST API, HEC, and Amazon S3 hosting. This talk is an update of the 2016 Presidential Election talk from .conf16.The Federal Election Commission (FEC) is an independent regulatory agency whose purpose is to enforce campaign finance law in US federal elections. The FEC provides a REST API to query all campaign data of every candidate. By collecting and analyzing the direct and indirect (Super PAC) contributions, Splunk can show the relative influence of each candidate of the midterm. Also learn how Splunk powers the Splunk for Good midterm website using Splunk's REST API, HEC, and Amazon S3 hosting. This talk is an update of the 2016 Presidential Election talk from .conf16."
  },
  {
    "Event": ".conf19",
    "Title": "BA1920 - Splunking the Shopfloor- Improving the Manufacturing Process with Splunk",
    "Track": "Business Analytics",
    "Industry": "Manufacturing",
    "Products": [
      "Splunk Enterprise",
      "Splunk Business Flow"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BA1920.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BA1920.pdf",
    "SkillLevel": "Beginner",
    "Description": "The sophisticated environment of a modern manufacturing plant demands that workers and management constantly synthesize data sources from different systems to optimize use of raw materials, work in process levels, and track inventory. This session will show how Splunk software can be used to collect, analyze, and report this information in real time to drive continuous improvement and business metrics."
  },
  {
    "Event": ".conf19",
    "Title": "FN1813 - Splunk & Intel Teams Deliver Faster Customers Insights Through Collaborative Engineering",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "SkillLevel": "Intermediate",
    "Description": "Splunk engineering and Intel teams collaborated to deliver high-performance and a scalable architecture that delivers greater business value to our customers. Our teams tested and built a software and hardware optimized platform that dramatically pushes the performance envelope forward for Splunk Enterprise. They will share the methodology used to enhance Splunk's performance with help from Intel’s latest technologies and innovative Splunk software engineering. You will hear about our journey, and our findings, so you can use our advancements in your own deployments.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT1227 - Splunk ITSI – From “Just Getting Started” to “What’s Hot in This Release",
    "Track": "IT Operations",
    "Industry": "Aerospace & DefenseCommunicationsEnergy & UtilitiesFinancial ServicesHealthcareHigher EducationManufacturingNon-ProfitOnline ServicesPublic SectorRetailTechnologyTravel & TransportationOil & Gas",
    "Products": [
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1227.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1227.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Have you heard the buzz? Splunk IT Service Intelligence (ITSI) just hit its latest release, and it’s jam-packed with new feature you just can’t miss, including the latest in infrastructure troubleshooting and monitoring, app analytics, event management, service insights, predictive analytics, and, of course, AIOps. Attendees at this session can sample it all, from just getting started to making the most of the latest and greatest.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IoT1935 - Splunk like your life depends on it",
    "Track": "Internet of Things",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk for Industrial IoT"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1935.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IOT1935.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Many of the talks at .conf will be around how to solve business problems, how to configure clusters, and how to improve your KPIs. This talk focuses instead on how to monitor and detect problems in the physical realm: IoT. What do you do when an agency's or business's security posture is a matter of life and death? What are they doing to equip themselves against cyberattacks? This talk will discuss previous security breaches and industrial IoT device failures that result in death, and how they could have been detected or prevented with Splunk. It also will give you actionable takeaway's for you to implement. After all, security isn't always saving dollars; sometimes it is actually about saving lives."
  },
  {
    "Event": ".conf19",
    "Title": "FN1916 - Splunk Machine learning and self healing at Priceline",
    "Track": "Foundations/Platform",
    "Industry": "Technology",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1916.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1916.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Do you want to rely on manual intervention to fix your application if something goes wrong? In this deep-dive session you will learn how Priceline uses machine learning to find outliers and anomalies in various data sets, including but not limited to bookings, search patterns, changes in logging patterns, etc. You will learn how we used machine learning combined with predictive analytics to solve variety of use cases. For example, we collect Kafka offset data, which is sending data to their respective syncs. We also monitor to see if the traffic is receded or data consumption has increased or decreased unexpectedly. We will show how different stages of application states are controlled with the use of data and alerts, like disabling the app and enabling it according to the data. We also will show you how Priceline deals with brownouts, the gradual degradation of volumes by using machine learning over long periods, using different self healing techniques and custom apps. "
  },
  {
    "Event": ".conf19",
    "Title": "SEC2294 - Splunk Mission Control Deep Dive Session",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [],
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2294.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Are you a security analyst? Do you like bright and shiny new things? Do you want to get a deep dive look into a new Splunk product months before it becomes Generally Available? Well then, this session is for you! Join us for this 2-hour session where you’ll get the inside scoop on the latest and greatest coming out of our security product and engineering teams.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN2048 - Splunk Multi-Deployment Server Architecture",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2048.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2048.pdf",
    "SkillLevel": "Intermediate",
    "Description": "This session is a technical capacity presentation on how to run Splunk Deployment Server (DS) as a multi-distributed layer of nodes integrated with a version controlled repository and automation techniques for keeping DS nodes in sync across changes."
  },
  {
    "Event": ".conf19",
    "Title": "IT1388 - Splunk, PCF and ITSI – supporting PCF with ITSI at scale",
    "Track": "IT Operations",
    "Industry": "Financial ServicesTechnologyNot industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1388.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1388.pdf",
    "SkillLevel": "Beginner",
    "Description": "Many Fortune 500 companies use Pivotal Cloud Foundry to push its high-quality code into production faster. While this helps companies enforce enterprise logging and application development standards, the traditional monitoring tools used to monitor development environments become the bottleneck because they are not architected to handle a firehose-nozzle connection. Learn how to use the new Splunk ITSI module for PCF, along with the new version of Splunk Firehose Nozzle for PCF to gain operational insight into PCF platform and increase developer satisfaction.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1350 - Splunk Performance-  making hardware and platform choices",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1350.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1350.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Splunk Enterprise is powerful. Don't cheat yourself of its power. When making decisions as to how to expand or standup Splunk’s footprint, you need to know what matters when making platform decisions. Considering compute, storage, virtualization, cloud infrastructure is a lot. There isn’t one place to review all the options you have. We will share the Splunk way to think of performance and how it relates to the underlying system resources. This means getting into the real nuts and bolts of performance. If you want to know how search affects indexing and what resources get consumed, this is the session to attend. If you also want to know how it affects what you purchase as a platform choice, this also is the session to attend. We also will review how partners have built architectures to simplify this process."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1949 - Splunk Phantom Ignition- Getting Automation Off the Ground and Working for You",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1949.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1949.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Did you get more staff for heartbleed? How about Shellshock or the OPM breach? Neither did we. The threat landscape is growing faster than ever and we need to cover more bases without more people. Enter Splunk Phantom: automation and integration for the masses. This session will help you understand what you need to build an effective Phantom ecosystem. I will go over initial strategies, real world examples, and use cases, and we will also take a glance at some more robust development projects that show the power of Phantom's extensibility. "
  },
  {
    "Event": ".conf19",
    "Title": "FN1172 - Splunk Python 3 Migration- What it Means for Your Deployment & Apps",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1172.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1172.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Stressed about Python 2.7 end-of-life? Terrified about how your Splunk deployment or apps will be impacted? Don’t be...we got you covered. It’s out with the old and in with the new, because Splunk is migrating to Python 3.7. As part of this migration, Splunk is also removing a handful of deprecated features. What’s the best way to learn if and what is changing for your Splunk? This session! Learn how to identify what’s impacted in your deployment or app and let us share our Python migration best practices. Soon, you too will be able to take advantage of the benefits of Python 3.\n"
  },
  {
    "Event": ".conf19",
    "Title": "ITS2726 - Splunk SAP – Be the Hero",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/ITS2726.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/ITS2726.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Be the Hero by bringing in your organization’s most important system into the world of Splunk! Join the team from RHONDOS as they show off real world use cases of how clients have leveraged SAP PowerConnect for Splunk to create a paradigm shift when it comes to monitoring SAP. Learn how to gain real-time visibility into system performance, monitor mission critical data flows and improve the security posture of your organization’s SAP environments.\n "
  },
  {
    "Event": ".conf19",
    "Title": "SEC1775 - Step Up Your Defenses with End-To-End Detection, Investigation, and Response",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security",
      "Splunk User Behavior Analytics",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1775.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1775.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Maturing and scaling your security operations rests on your ability to process and analyze huge volumes of often unrelated data in real time. But today's tools notoriously overwhelm SOC analysts with the sheer number of alerts and high percent of false positives, resulting in confusion about what tools to use for investigation and response. In this session, members of Splunk's Security Research Team will discuss the next generation of Enterprise Security Content Updates that they developed, which integrate the entire Splunk for Security product suite to create a robust end-to-end defense—detection, investigation, and response. We will go over how to use these security guides, which will leverage Splunk Enterprise Security, Splunk Phantom, and Splunk User Behavior Analytics. We'll also highlight the Run Story feature we built to operationalize ESCU Analytics stories and share tools and techniques customers can use to write and test their own use cases.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT2202 - Success in the Public Sector- How the State of Michigan uses Splunk to improve the lives of its citizens",
    "Track": "IT Operations",
    "Industry": "Public Sector",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence",
      "Splunk Mobile"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT2202.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT2202.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "If you’re not investing in new technology, you are going to be left behind! One of the key challenges within any state government agency is ensuring system and application performance, SLA enforcement, secure operations, and adhering to strict federal and state compliance mandates, all while operating with limited budgets and staff. With help from Splunk, Health and Human Services (HHS) agencies manage systems that issue public benefits to citizens, maintain child welfare, enforce child support, and monitor public health. Come to this session to learn how to leverage Splunk to help you understand the \"complete picture\" of your systems and business processes to help your team move toward proactive management and increase customer satisfaction within your governmental applications. Create something useful from millions and billons of lines of log data to improve your application!\n"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1411 - Supercharge Your Security Operations Center with Splunk and MITRE",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Enterprise",
      "Splunk Business Flow"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1411.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1411.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "DATEV provides information services to ~2.5 million payrolling, accounting, and tax clients. Given the sensitivity of the personal and financial data that our clients process, DATAEV decided to establish a SOC to secure our clients' information, and we put Splunk at the core of its operations. In this session we will discuss four key elements relevant to building a successful SOC with Splunk. We'll first discuss how we formed our SOC and orchestrated its activities internally. We'll then discuss how we use MITRE's ATT&CK™ framework to prioritize activities, how we spread our SOC's security knowledge to all relevant groups at DATEV, and how we use Splunk to create real-time situational awareness for different SOC customers, for stakeholders, and for management.\nDATEV provides information services to ~2.5 million payrolling, accounting, and tax clients. Given the sensitivity of the personal and financial data that our clients process, DATAEV decided to establish a SOC to secure our clients' information, and we put Splunk at the core of its operations. In this session we will discuss four key elements relevant to building a successful SOC with Splunk. We'll first discuss how we formed our SOC and orchestrated its activities internally. We'll then discuss how we use MITRE's ATT&CK™ framework to prioritize activities, how we spread our SOC's security knowledge to all relevant groups at DATEV, and how we use Splunk to create real-time situational awareness for different SOC customers, for stakeholders, and for management."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1573 - Survival of the Fastest- The 1-10-60 Rule",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1573.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1573.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Winston Churchill once said, “Success is not final, failure is not fatal: it is the courage to continue that counts.\" Then again, Churchill wasn’t in cybersecurity...While our successes are certainly never final, our failures can absolutely be fatal—to a company and our continued employment. What's a good way to actually measure success and failure, though, outside of not appearing on the front page of the paper? Well, as CrowdStrike notes, you have on average one minute to detect an attack in progress, ten minutes to understand it, and sixty minutes to contain it. We will show how to use this 1-10-60 Rule as a measuring metric and leverage the data and capabilities within Splunk and its ecosystem to ensure that we win the survival of the fastest."
  },
  {
    "Event": ".conf19",
    "Title": "SEC2187 - Tackle AWS Security Automatically with Splunk Phantom",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2187.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2187.pdf",
    "SkillLevel": "Intermediate",
    "Description": "This session will give you a comprehensive look into automating the investigation and remediation of AWS security events using Splunk Phantom. The session will start with an overview and then progress to a live technical walkthrough of setting up Phantom to remediate an AWS security event. "
  },
  {
    "Event": ".conf19",
    "Title": "FN1727 - Tailoring Your Data Fabric to Custom Fit Your (SOCs-NOCs) Data Needs- ",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1727.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1727.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Data Fabric Search (DFS) is a paradigm shift from native Splunk and introduces several new innovative concepts such as core scaling and federated search. This sparks questions regarding: How to maximize search performance using DFS conf parameters? How to size the DFS Spark cluster? and What search time speed ups can I expect with DFS? This discourse aims to answer these questions and presents DFS best practices in the context of improving and discovering business use cases. \n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1651 - Take Control of Port 514!- Taming the Syslog Beast",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk Data Fabric Search and Data Stream Processor"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1651.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1651.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Are you frustrated with the task of configuring syslog servers yourself to properly ingest data into Splunk? Take control of the syslog beast once and for all and point your \"514\" traffic to the new Splunk Connect for Syslog! This new Splunk-supported connector makes quick work of past struggles with syslog servers, sourcetyping, data enrichment, and scale. In this session we will dive into the configuration of the Splunk Connect for Syslog to properly filter, sourcetype, and format your data. We will demonstrate several out-of-the-box examples, highlighting new functionality such as HEC and Kafka transport for resiliency and scale, simple extensions for new device types, and data enrichment that extends far beyond simple sourcetyping of the raw message. Lastly, we will look forward to the integration of syslog with Splunk's new Data Stream Processor, and highlight appropriate use cases for each solution. By the time we wrap up, you will know how to tame the syslog beast!"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1908 - Tales From a Threat Team- Lessons and Strategies for Succeeding with a Risk-Based Approach",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1908.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1908.pdf",
    "SkillLevel": "Advanced",
    "Description": "We've run a risk-based approach with our security alerts for over a year, and we're excited to review our progress with you. We'll discuss how we increased the number of behavioral indicators by 300% while reducing our alerts by 50%. We'll also discuss how we expanded our risk approach to handle on premise and cloud environments within the same framework, which yielded a single alerting mechanism that leverages all of our data enrichment. We'll also share the roadmap for our risk-based approach, which incorporates risk rules that utilize algorithms to identify risks not discovered by traditional detection approaches."
  },
  {
    "Event": ".conf19",
    "Title": "ITS2752 - Tame the Beast of IT Complexity- AI, ML, & Automation Are the Answer",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence",
      "Splunk Machine Learning Toolkit"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/ITS2752.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/ITS2752.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Are you drowning in a sea of data that expands daily? Overwhelmed by 1000s of events and alarms? Tasked with tracking a dynamic, ever-morphing infrastructure? Expected to resolve requests, incidents, and performance issues in seconds, not days… without adding any more headcount to your team? You’re not alone.\n\nEnter Automation, AIOps, and machine learning (ML). It’s finally IT’s turn to harness these powerful technologies to improve operational efficiency, reduce MTTR, eliminate alarm noise, streamline service requests, increase performance without lifting a finger, and tame the beast of IT complexity. \n\nJoin our session as we explore practical applications for these technologies today and in the future to transform the way you approach IT operations. Get real world examples from other IT professionals and see how you can maximize your investments in Splunk, ITSM, monitoring tools, and more by bringing AI, ML, and automation to the mix.​\nAre you drowning in a sea of data that expands daily? Overwhelmed by 1000s of events and alarms? Tasked with tracking a dynamic, ever-morphing infrastructure? Expected to resolve requests, incidents, and performance issues in seconds, not days… without adding any more headcount to your team? You’re not alone.Are you drowning in a sea of data that expands daily? Overwhelmed by 1000s of events and alarms? Tasked with tracking a dynamic, ever-morphing infrastructure? Expected to resolve requests, incidents, and performance issues in seconds, not days… without adding any more headcount to your team? You’re not alone.Are you drowning in a sea of data that expands daily? Overwhelmed by 1000s of events and alarms? Tasked with tracking a dynamic, ever-morphing infrastructure? Expected to resolve requests, incidents, and performance issues in seconds, not days… without adding any more headcount to your team? You’re not alone.Are you drowning in a sea of data that expands daily? Overwhelmed by 1000s of events and alarms? Tasked with tracking a dynamic, ever-morphing infrastructure? Expected to resolve requests, incidents, and performance issues in seconds, not days… without adding any more headcount to your team? You’re not alone.Enter Automation, AIOps, and machine learning (ML). It’s finally IT’s turn to harness these powerful technologies to improve operational efficiency, reduce MTTR, eliminate alarm noise, streamline service requests, increase performance without lifting a finger, and tame the beast of IT complexity. Enter Automation, AIOps, and machine learning (ML). It’s finally IT’s turn to harness these powerful technologies to improve operational efficiency, reduce MTTR, eliminate alarm noise, streamline service requests, increase performance without lifting a finger, and tame the beast of IT complexity. Enter Automation, AIOps, and machine learning (ML). It’s finally IT’s turn to harness these powerful technologies to improve operational efficiency, reduce MTTR, eliminate alarm noise, streamline service requests, increase performance without lifting a finger, and tame the beast of IT complexity. Enter Automation, AIOps, and machine learning (ML). It’s finally IT’s turn to harness these powerful technologies to improve operational efficiency, reduce MTTR, eliminate alarm noise, streamline service requests, increase performance without lifting a finger, and tame the beast of IT complexity. Join our session as we explore practical applications for these technologies today and in the future to transform the way you approach IT operations. Get real world examples from other IT professionals and see how you can maximize your investments in Splunk, ITSM, monitoring tools, and more by bringing AI, ML, and automation to the mix.​Join our session as we explore practical applications for these technologies today and in the future to transform the way you approach IT operations. Get real world examples from other IT professionals and see how you can maximize your investments in Splunk, ITSM, monitoring tools, and more by bringing AI, ML, and automation to the mix.​Join our session as we explore practical applications for these technologies today and in the future to transform the way you approach IT operations. Get real world examples from other IT professionals and see how you can maximize your investments in Splunk, ITSM, monitoring tools, and more by bringing AI, ML, and automation to the mix.​Join our session as we explore practical applications for these technologies today and in the future to transform the way you approach IT operations. Get real world examples from other IT professionals and see how you can maximize your investments in Splunk, ITSM, monitoring tools, and more by bringing AI, ML, and automation to the mix.​"
  },
  {
    "Event": ".conf19",
    "Title": "IoT1560 - Teaching Splunk to Hear- Audio Spectrum Analysis in Splunk for Event Classification and Anomaly Detection",
    "Track": "Internet of Things",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1560.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IoT1560.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "If we hear a nearby gunshot, we instinctively react. A mechanic often knows their machine's sound so well that they can diagnose issues by sound alone. While machines can be given analytical capabilities with machine learning (ML), sensing human inputs - like auditory or other sensory data - in a form that machines can understand is challenging. In Splunk, we have been all about making machine data accessible to humans, but what if we flip that and make human data accessible to machines? I take audio captured from live and recorded sources and using Fast Fourier transform feed it into Splunk's Machine Learning Toolkit (MLTK) for classification and anomaly detection. Can we use Splunk to detect gunshots? Can we learn a machine’s normal sounds to detect pending failures? This presentation uses Splunk to apply superhuman ML detection and learning capabilities to human data to show that the MLTK contains accessible tools you can apply to your IT and security problems. "
  },
  {
    "Event": ".conf19",
    "Title": "BA2642 - Technical Overview- How We Used Splunk Business Flow at Splunk for IT Service Delivery",
    "Track": "Business Analytics",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Business Flow"
    ],
    "SkillLevel": "Advanced",
    "Description": "Are you a Splunk user that works with a business application, a development environment, or a helpdesk environment?  Splunk Business Flow is Splunk’s process mining solution for interactively exploring processes to drive greater operational efficiencies. In this interactive session, learn how different teams within Splunk have optimized their business processes using Splunk Business Flow and realized value."
  },
  {
    "Event": ".conf19",
    "Title": "FNS3310 - The 1m+ UDP messages per second ingestion challenge",
    "Track": "Foundations/Platform",
    "Products": [],
    "Description": "Even in 2019, many organizations rely on UDP to ship syslog messages from endpoints to Splunk where TCP transport is either not feasible or desirable. Scaling UDP ingestion with no message loss is far from a trivial task. Join us for a hands-on demo show-casing how we have achieved 1m+ EPS UDP ingestion with a single syslog-ng instance and single UDP listening port using no external load balancer leveraging socket level load balancing.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FNS2827 - The 1m+ UDP messages per second ingestion challenge",
    "Track": "Foundations/Platform",
    "Products": [],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FNS2827.mp4",
    "Description": "Even in 2019, many organizations rely on UDP to ship syslog messages from endpoints to Splunk where TCP transport is either not feasible or desirable. Scaling UDP ingestion with no message loss is far from a trivial task. Join us for a hands-on demo show-casing how we have achieved 1m+ EPS UDP ingestion with a single syslog-ng instance and single UDP listening port using no external load balancer leveraging socket level load balancing.\n"
  },
  {
    "Event": ".conf19",
    "Title": "DEV1299 - The A to Z of building Add-Ons",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1299.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV1299.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Building Splunk add-ons using API's and Python is easy! In fact we reckon it's is so easy that we're going to build one from scratch in less than 20 minutes... LIVE. Come and see whether we manage to pull it off, or whether we fail in front of hundreds of people! You do not need to be a Python expert, however a basic understanding will help."
  },
  {
    "Event": ".conf19",
    "Title": "IT1708 - The Basics- How to make on-call suck less with VictorOps",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "VictorOps"
    ],
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1708.pdf",
    "SkillLevel": "Beginner",
    "Description": "In this session, we’ll discuss ways that VictorOps empowers DevOps teams by delivering context-rich infrastructure and application monitoring alerts to the right people so they can collaborate cross-functionally to empower fast, efficient incident resolution—and reduced downtime. Applications are fixed quickly while teams work better together and continuously learn how to improve systems and processes for a better on-call experience. You’ll see an in-depth demo of the product to better understand the difference between Collaborative Incident Response and traditional “trouble ticketing,” and get insight into how Splunk integrates with VictorOps to make alerting a seamless experience."
  },
  {
    "Event": ".conf19",
    "Title": "FNS2837 - The Blindspot No One is Talking About…But Hackers Are Targeting",
    "Track": "Foundations/Platform",
    "Products": [],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FNS2837.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2837.pdf",
    "Description": "Today’s largest enterprises run their business on ERP Platforms. These same companies rely on SPLUNK  Enterprise Security to monitor and respond to threats targeting their infrastructure and applications. Join Tara tile and Greg name title to learn why ERP platforms are under attack, what leading enterprises are doing to protect them and how you can leverage SPLUNK to monitor threats and prevent catastrophic downtime of your most critical business applications.\nToday’s largest enterprises run their business on ERP Platforms. These same companies rely on SPLUNK  Enterprise Security to monitor and respond to threats targeting their infrastructure and applications. Join Tara tile and Greg name title to learn why ERP platforms are under attack, what leading enterprises are doing to protect them and how you can leverage SPLUNK to monitor threats and prevent catastrophic downtime of your most critical business applications.Today’s largest enterprises run their business on ERP Platforms. These same companies rely on SPLUNK  Enterprise Security to monitor and respond to threats targeting their infrastructure and applications. Join Tara tile and Greg name title to learn why ERP platforms are under attack, what leading enterprises are doing to protect them and how you can leverage SPLUNK to monitor threats and prevent catastrophic downtime of your most critical business applications."
  },
  {
    "Event": ".conf19",
    "Title": "SEC2887 - The CISO’s Guide to Shutting Down Attacks Using the Dark Web + Live Dark Web Tour",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2887.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2887.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Nick Hayes, VP of Strategy at IntSights, will take you on a tour of the dark web and explain how CISOs can successfully implement a dark web intelligence strategy to neutralize threats outside the wire and at the earliest stages of the cyber kill chain. Now equipped with IntSights External Threat Intelligence, learn how you can take advantage of it through seamless integrations with your Splunk SIEM and Phantom toolsets. Enrich your threat data with internal network security observables, expedite incident reviews and prioritization, and automate your threat prevention and response with SOAR and integrated playbooks."
  },
  {
    "Event": ".conf19",
    "Title": "FN1778 - The Cloud Gateway- A Security Deep-Dive and the Future Ahead",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1778.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1778.pdf",
    "SkillLevel": "Intermediate",
    "Description": "The Cloud Gateway is at the heart of many of Splunk’s most exciting products: Mobile, AR, NLP. It facilitates the communication from your iOS, Android, and AppleTV devices with an on-premise Splunk instance without having to set up a VPN or manage firewall rules. The connection is encrypted end to end, so even if Cloud Gateway was compromised your data would still be secure. In this session we’re going to share how Cloud Gateway provides these guarantees by doing a deep dive into the encryption used, the device authentication process, and the flow of a message from a mobile device to a Splunk instance and back again. Finally, we’re going to be talking about some exciting features rolling out to Cloud Gateway in the coming year."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1904 - The Duck Test- Leverage Machine Learning to Remediate Fraud in Huge Datasets",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1904.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1904.pdf",
    "SkillLevel": "Advanced",
    "Description": "Aflac measures risk to provide financial protection to more than 50 million people worldwide. Join this session to learn how Aflac mitigates fraud by using Splunk's Machine Learning Toolkit (MLTK) to find outliers and cluster events. Using Splunk and the MLTK reduced the time needed to conduct necessary analyses (e.g. link analysis) from weeks and months to just minutes—we will share with you how we use Splunk's MLTK to iterate quickly, develop new anomaly detection techniques, and improve our overall fraud mitigation perfomance. "
  },
  {
    "Event": ".conf19",
    "Title": "SEC1179 - The House Always Wins- Using Splunk Enterprise to Fight Data Exfiltration From Insider Threats",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1179.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1179.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "What happens when the call is coming from inside the house? Data exfiltration by insiders is a dangerous threat, but one that often doesn't get the same level of attention as the sexier external ones. We'll start this session with a brief overview of why and how users exfiltrate information, and we'll progress to tactics, such as effective SPL searches, for operationalizing insider threat detection. You'll leave this session better able to catch insider threats in the in the act of exfiltration instead of days, weeks, or months later. "
  },
  {
    "Event": ".conf19",
    "Title": "SECS3100 - The Hunt for Splunk in October",
    "Track": "Security, Compliance and Fraud",
    "Products": [],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SECS3100.mp4",
    "Description": "Leverage the DomainTools Iris API to uncover malicious domains in your logs, provide real time context and event decoration, and hunt at scale beyond the firewall with the DomainTools TA for Splunk, and the Iris App for Phantom. "
  },
  {
    "Event": ".conf19",
    "Title": "SECS3057 - The Hunt Myth to Reality ",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security"
    ],
    "SkillLevel": "Intermediate",
    "Description": "As cyber threat hunting has grown in recent years, there are still challenges in making it “real” for organizations. We’ll discuss a disciplined approach to proactive threat hunting in organizations including an overview of some key hunting methodologies to employ, examples of campaigns and their benefits and discover how to better enable your team’s ability to hunt.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IoT2066 - The most epic road trip to .Conf- IOT in an RV, OMG",
    "Track": "Internet of Things",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk for Industrial IoT",
      "VictorOps"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT2066.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IoT2066.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Splunk .conf is our favorite event every year and we wanted to extend the excitement while getting even more Splunky. So we grabbed an RV, set up some sensors, built an edge computing environment, and packed up the Big Data Beard recording equipment for a road-trip across the country for the week leading up to .conf, traveling from New York City to Las Vegas. We stopped along the way to hear awesome stories from fellow Splunk users, sharing them online via live chats and podcasts. With the power of Splunk, we captured data, discovered trends, predicted failures, and discovered more exciting ways to use Splunk to drive value from machine-generated data across the country. Hear the full story of how three engineers had the most interesting trip to .conf2019 of all!"
  },
  {
    "Event": ".conf19",
    "Title": "FN1815 - The New Dashboarding & Content Export Experience in Splunk- A single experience across Enterprise, ITSI and more!",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence",
      "Splunk Developer Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1815.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1815.pdf",
    "SkillLevel": "Intermediate",
    "Description": "So you saw the new Splunk Dashboards framework and PNG export on the main stage and want to know more? You want to understand what this will mean for your Enterprise, Cloud, ITSI, ES and/or IAI deployments? You've come to the right place. In this session, we'll provide an overview and extended demo of the new dashboarding framework and context export service. We'll cover what's different about this new framework in comparison to both SimpleXML and Glass Tables. We'll also cover the support roadmap for Simple XML and Glass Tables as well as what you need to know in order to migrate. If you're planning to attend any of the other dashboard deep dive sessions, we recommend attending this one first.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1553 - The New Experiment Experience in the Splunk Machine Learning Toolkit",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1553.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1553.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Hey mad scientist, why so angry? Learn how Splunk is rethinking experiments in the Machine Learning Toolkit (MLTK) to make your life easier. Find out how we're changing the experiment workflow to reflect real-world usage of the MLTK, and make it easier for people new to the MLTK to get up and running. Strap on your safety goggles and let's get experimenting! "
  },
  {
    "Event": ".conf19",
    "Title": "FN1206 -  The path to operational enlightenment. An introduction to wire data with Splunk Stream.",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1206.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1206.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Have you ever wondered what Joe meant when he referred to “Wire Data”? Today, you'll see the applicability of wire data in your organization, and you'll be amazed. Solve fraud, cybersecurity, ops, and business challenges, all with one single source of data. Wire data is the information that passes over computer and telecommunications networks to define communications between client and server devices. It is the result of decoding wire and transport protocols containing the bi-directional data payload. We will cover the use of wire data to solve security, IT operations, and business use cases, and see how the Splunk Stream platform is easily integrated into your existing data flows. The Splunk Essentials for Wire Data app from Splunkbase will be used to showcase dozens of examples using wire data to solve common business and technical issues. We will cover how to deploy and configure Splunk Stream in a distributed environment, including a demonstration."
  },
  {
    "Event": ".conf19",
    "Title": "SECS2839 - The SOC of the Future",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security",
      "Splunk User Behavior Analytics"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SECS2839.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SECS2839.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "This presentation will discuss how Security Operation Centers (SOCs) will need to change to meet the cybersecurity challenges of the 2020s. \n\nThe speaker will draw on his experience as a founder of the first SOC-as-a-Service company that delivers managed security services using Splunk. \n\nMost industry analysts envision that the next generation of SOCs will leverage AI, Big Data, and the Cloud, but how far can automation take us and is the concept of an autonomous SOC really practical? How will the SOC of the Future address the global shortage of cyber professionals? How will the role of security analysts need to change? Will the SOC of the Future still need to be housed in dedicated physical facilities? \n\nThe speaker will provide a blueprint of Proficio’s vision of the SOC of the Future using Splunk and provide a playbook for IT leaders and aspiring IT leaders on how to drive continuous improvement in productivity and measurable outcomes."
  },
  {
    "Event": ".conf19",
    "Title": "IT1523 - The top 10 glasstable design principles to boost your career and your business",
    "Track": "IT Operations",
    "Industry": "Aerospace & DefenseCommunicationsEnergy & UtilitiesFinancial ServicesHealthcareHigher EducationManufacturingNon-ProfitOnline ServicesPublic SectorRetailTechnologyTravel & TransportationOil & Gas",
    "Products": [
      "Splunk IT Service Intelligence"
    ],
    "SkillLevel": "BeginnerIntermediateAdvancedGood for all skill levels",
    "Description": "Dashboards and glass tables enable faster and more accurate decision making. The challenge communication and selecting the right visualization for the right information. In this session we will share the top 10 principles and best practices from UX  and data visualization designers so you can see how to structure information, select the right visualization or chart, add context, and more. You are already delivering meaningful insights using Splunk, but we will help you show it to the world using only PowerPoint, Google, and Splunk ITSI. “Perfection is achieved, not when there is nothing left to add, but when there is nothing left to remove.“– Antoine de Saint-Exupery (sorry, as a French guy, I had to use a French quote)"
  },
  {
    "Event": ".conf19",
    "Title": "FN1213 - The Two Most Common Machine Learning Solutions Everyone Needs to Know",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1213.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1213.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Tired of relying on static threshold-based alerts that don’t seem to provide much value? Do you typically end up finding outliers in your data by staring at lines on your dashboards? We are told machine learning is going make alerts and dashboards smarter, but how? We will help demystify machine learning and provide a practical guide to apply machine learning techniques for numeric outlier detection, and forecasting to make alerts and dashboards smarter and easier to use for actionable results. We will show you the basics of how you can understand your data, get them ready for machine learning, and get the machine to start working for you! You will leave the session beginning to think like a data scientist and knowing how to apply purpose-driven machine learning to your searches in Splunk! "
  },
  {
    "Event": ".conf19",
    "Title": "IoT1641 - Threat Hunting in Industrial (ICS-OT) Environments",
    "Track": "Internet of Things",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security",
      "Splunk for Industrial IoT"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IoT1641.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IOT1641.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Industrial operations comprise a diverse blend of technology that run critical processes. The proliferation of automation and networking has increased the sophistication of Industrial Control Systems (ICS), also known as Operational Technology (OT) environments.\nThreats targeting OT are increasing in both frequency and sophistication. Dragos tracks 9 OT-targeting activity groups, the most significant of which, XENOTIME, was responsible for the TRISIS malware that targeted safety systems (SIS) resulting in multiple plant shutdowns and the potential to cause harm to human operators.\nTraditional IT threat hunting is not well-suited to OT environments. This session will outline the differences between IT and OT assessments, highlight the most significant threats facing OT, and review best practices for OT-specific threat hunting engagements, including techniques that empower defenders to detect and respond more efficiently to existing and future threats, therefore reducing adversary dwell time.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT1970 - Tracking Micro Services with Splunk",
    "Track": "IT Operations",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1970.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1970.pdf",
    "SkillLevel": "Advanced",
    "Description": "Using a combination of Splunk AWS plugin, Docker logs, and direct from script real-time HEC logging, data/jobs can follow an entire data pipeline or workflow. This session will discuss how metrics can be gathered to show bottlenecks, and inefficiencies."
  },
  {
    "Event": ".conf19",
    "Title": "SEC2253 - Transforming Intel’s Security Posture with Innovations in Data Intelligence",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2253.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2253.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Intel is transforming its approach to security by deploying a new Cyber Intelligence Platform (CIP) based on Splunk, Kafka, and other leading-edge technologies. Our new platform ingests data from hundreds of data sources and security tools, providing context-rich visibility and a common work surface, and improving the efficiency of our entire information security organization. This session will address how we partnered with Splunk architects to deploy and realize benefits from this solution in just five weeks. We will detail how our solution uses real-time data, streams processing, machine learning tools and consistent data models to decrease time to detect and respond to sophisticated threats. This session will cover everything from our platform's business value to its solution architecture.\n"
  },
  {
    "Event": ".conf19",
    "Title": "DEV2518 - Triggers & Alerts in the Splunk Cloud Platform",
    "Track": "Developer",
    "Products": [],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV2518.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV2518.pdf",
    "Description": "In the new Splunk Cloud Platform, we’re reimagining the way we enable monitoring and alerting. Configure triggers to identify changes and anomalies in your data as they occur and determine the right action(s) that should be taken as a result – email, Slack, VictorOps, etc. Leverage machine learning to bring your attention to the right insights and roll that back into your core monitoring strategy.  Come to this session to learn more about both the long-term vision and what’s immediately available. In the new Splunk Cloud Platform, we’re reimagining the way we enable monitoring and alerting. Configure triggers to identify changes and anomalies in your data as they occur and determine the right action(s) that should be taken as a result – email, Slack, VictorOps, etc. Leverage machine learning to bring your attention to the right insights and roll that back into your core monitoring strategy.  Come to this session to learn more about both the long-term vision and what’s immediately available.In the new Splunk Cloud Platform, we’re reimagining the way we enable monitoring and alerting. Configure triggers to identify changes and anomalies in your data as they occur and determine the right action(s) that should be taken as a result – email, Slack, VictorOps, etc. Leverage machine learning to bring your attention to the right insights and roll that back into your core monitoring strategy.  Come to this session to learn more about both the long-term vision and what’s immediately available.In the new Splunk Cloud Platform, we’re reimagining the way we enable monitoring and alerting. Configure triggers to identify changes and anomalies in your data as they occur and determine the right action(s) that should be taken as a result – email, Slack, VictorOps, etc. Leverage machine learning to bring your attention to the right insights and roll that back into your core monitoring strategy.  Come to this session to learn more about both the long-term vision and what’s immediately available."
  },
  {
    "Event": ".conf19",
    "Title": "IT2950 - Unified Observability with OpenTelemetry",
    "Track": "IT Operations",
    "Products": [
      "SignalFx"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT2950.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT2950.pdf",
    "Description": "We now live in a Cloud Native world where we build and deploy software very differently from the previous generation. Large, centralized systems are being decoupled and distributed to address scalability needs and to allow companies to deliver value faster. Compared to monolithic applications, microservice architectures introduce complexity in network communication, feature much shorter life cycles, and require resiliency in dynamic environments. As companies began to build or migrate to microservice architectures they often run into operational complexity and struggle to efficiently monitor their environments. These challenges have highlighted the need to observe systems differently and lead to the rise of Observability. In this session, we will discuss OpenTelemetry and how it provides visibility into both distributed traces and metrics. We will walk through the architecture including client libraries and collection as well as cover key concepts including annotations, sampling policies and more. Live demos and coding examples will be presented through the talk. By the end of the session, you will know what OpenTelemetry is, why it is important, how you can get started and how you can get involved.\nWe now live in a Cloud Native world where we build and deploy software very differently from the previous generation. Large, centralized systems are being decoupled and distributed to address scalability needs and to allow companies to deliver value faster. Compared to monolithic applications, microservice architectures introduce complexity in network communication, feature much shorter life cycles, and require resiliency in dynamic environments. As companies began to build or migrate to microservice architectures they often run into operational complexity and struggle to efficiently monitor their environments. These challenges have highlighted the need to observe systems differently and lead to the rise of Observability.We now live in a Cloud Native world where we build and deploy software very differently from the previous generation. Large, centralized systems are being decoupled and distributed to address scalability needs and to allow companies to deliver value faster. Compared to monolithic applications, microservice architectures introduce complexity in network communication, feature much shorter life cycles, and require resiliency in dynamic environments. As companies began to build or migrate to microservice architectures they often run into operational complexity and struggle to efficiently monitor their environments. These challenges have highlighted the need to observe systems differently and lead to the rise of Observability.We now live in a Cloud Native world where we build and deploy software very differently from the previous generation. Large, centralized systems are being decoupled and distributed to address scalability needs and to allow companies to deliver value faster. Compared to monolithic applications, microservice architectures introduce complexity in network communication, feature much shorter life cycles, and require resiliency in dynamic environments. As companies began to build or migrate to microservice architectures they often run into operational complexity and struggle to efficiently monitor their environments. These challenges have highlighted the need to observe systems differently and lead to the rise of Observability.We now live in a Cloud Native world where we build and deploy software very differently from the previous generation. Large, centralized systems are being decoupled and distributed to address scalability needs and to allow companies to deliver value faster. Compared to monolithic applications, microservice architectures introduce complexity in network communication, feature much shorter life cycles, and require resiliency in dynamic environments. As companies began to build or migrate to microservice architectures they often run into operational complexity and struggle to efficiently monitor their environments. These challenges have highlighted the need to observe systems differently and lead to the rise of Observability. In  In    this session, we will discuss OpenTelemetry and how it provides visibility into both distributed traces and metrics. We will walk through the architecture including client libraries and collection as well as cover key concepts including annotations, sampling policies and more. Live demos and coding examples will be presented through the talk. By the end of the session, you will know what OpenTelemetry is, why it is important, how you can get started and how you can get involved.this session, we will discuss OpenTelemetry and how it provides visibility into both distributed traces and metrics. We will walk through the architecture including client libraries and collection as well as cover key concepts including annotations, sampling policies and more. Live demos and coding examples will be presented through the talk. By the end of the session, you will know what OpenTelemetry is, why it is important, how you can get started and how you can get involved.this session, we will discuss OpenTelemetry and how it provides visibility into both distributed traces and metrics. We will walk through the architecture including client libraries and collection as well as cover key concepts including annotations, sampling policies and more. Live demos and coding examples will be presented through the talk. By the end of the session, you will know what OpenTelemetry is, why it is important, how you can get started and how you can get involved.this session, we will discuss OpenTelemetry and how it provides visibility into both distributed traces and metrics. We will walk through the architecture including client libraries and collection as well as cover key concepts including annotations, sampling policies and more. Live demos and coding examples will be presented through the talk. By the end of the session, you will know what OpenTelemetry is, why it is important, how you can get started and how you can get involved.this session, we will discuss OpenTelemetry and how it provides visibility into both distributed traces and metrics. We will walk through the architecture including client libraries and collection as well as cover key concepts including annotations, sampling policies and more. Live demos and coding examples will be presented through the talk. By the end of the session, you will know what OpenTelemetry is, why it is important, how you can get started and how you can get involved."
  },
  {
    "Event": ".conf19",
    "Title": "IT2139 - United Health Group- Providing real-time visibility with Splunk IT Service Intelligence from the front end all the way to the mainframe",
    "Track": "IT Operations",
    "Industry": "HealthcareTechnology",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT2139.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT2139.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Does your IT environment look like a huge bowl of spaghetti? Is it even possible to untangle that vast, complex, and diverse ecosystem? Is moving from reactive to proactive even a possibility? Join us to learn how United Health Group is using Splunk IT Service Intelligence to provide end-to-end visibility and proactive incident response to its critical business applications. We’ll show you how mapping service dependencies and defining meaningful key performance indicators from the front end all the way back to the mainframe is providing value to the DevOps teams and the businesses they support."
  },
  {
    "Event": ".conf19",
    "Title": "FN1933 - Unleash your inner Picasso- experience the new Splunk dashboards",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1933.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1933.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Seeking guidance to help create amazing Splunk dashboards? Want to wow your team as well as your execs and become THE dashboard master? Come learn about Splunk's new dashboarding capabilities, and experience rich dashboard examples as well as the art of the possible. We will demo a variety of dashboards, and we’ll share tips and tricks, tutorials and templates to ensure you can build your own. Attend this session and you’ll be on the path to becoming a Splunk dashboard ninja in no time.\n\n \nSeeking guidance to help create amazing Splunk dashboards? Want to wow your team as well as your execs and become THE dashboard master? Come learn about Splunk's new dashboarding capabilities, and experience rich dashboard examples as well as the art of the possible. We will demo a variety of dashboards, and we’ll share tips and tricks, tutorials and templates to ensure you can build your own. Attend this session and you’ll be on the path to becoming a Splunk dashboard ninja in no time.Seeking guidance to help create amazing Splunk dashboards? Want to wow your team as well as your execs and become THE dashboard master? Come learn about Splunk's new dashboarding capabilities, and experience rich dashboard examples as well as the art of the possible. We will demo a variety of dashboards, and we’ll share tips and tricks, tutorials and templates to ensure you can build your own. Attend this session and you’ll be on the path to becoming a Splunk dashboard ninja in no time."
  },
  {
    "Event": ".conf19",
    "Title": "SEC2203 - Use Deception, Automated Response and Threat Emulation to Make Your Defense Proactive",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Aerospace & Defense",
    "Products": [
      "Splunk Enterprise Security",
      "Splunk Machine Learning Toolkit",
      "Phantom",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2203.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2203.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Deception, automation, and real-time data exploitation help security organizations go on offense vs attackers. In this session we will discuss how to use a variety of deception techniques to gather threat intelligence, how to create an automated response, and how to test response playbooks to validate that responses work as expected. "
  },
  {
    "Event": ".conf19",
    "Title": "SEC1375 - Use Red Team Exercises to Build Alerts, Train Staff, and Drive Policies",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1375.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1375.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Most of us have had (or still have) nightmares about an alert that someone's exfltrating data from our organization. We've lived that nightmare at Harris, and we've learned from it. In this session, we'll discuss how we used red and purple teaming to improve our security posture post-breach. Learn from our experience so that you can strengthen your team's alerting, staff comptency, and policies, and reduce the risk of a breach at your company."
  },
  {
    "Event": ".conf19",
    "Title": "FN1631 - User Experience Modeling with the Splunk Machine Learning Toolkit",
    "Track": "Foundations/Platform",
    "Industry": "Financial Services",
    "Products": [
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1631.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1631.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Paychex’s goal of providing the best user experience for our clients has led to a significant investment in performance testing and monitoring of our applications. Currently all Paychex applications record the execution time for every task and subtask to logs. These are indexed by Splunk, allowing us to identifying areas where changes to code and database queries will have a positive impact on the overall user experience. This presentation will focus on combining this user experience data with client demographic data (such as the number of active employees) and using the Splunk Machine Learning Toolkit to build predictive models of user experience based on client demographic data. "
  },
  {
    "Event": ".conf19",
    "Title": "SEC1671 - Use Splunk SIEMulator to Generate Data for Automated Detection, Investigation, and Response ",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security",
      "Splunk User Behavior Analytics",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1671.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1671.pdf",
    "SkillLevel": "Advanced",
    "Description": "Obtaining data to develop defenses against threats is a constant challenge for security analysts. To that end, Splunk's Security Research team developed the Splunk SIEMulator, a framework modeled after Chris Long's DetectionLab that allows a defender to replay attack scenarios using AttackIQ in a simulated environment. SIEMulator’s Attack Range environments are all configured with Splunk forwarders and the apps necessary to create and store data in CIM data models. We'll show you how to use the SIEMulator to produce shareable data that can help security analysts replicate scenarios and effectively detect, investigate, and respond to threats.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1390 - Using Machine Learning to Detect Traffic Anomalies",
    "Track": "Foundations/Platform",
    "Industry": "Non-ProfitTechnology",
    "Products": [
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1390.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1390.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Finding anomalies in network data is no easy task, especially when you have terabytes of logs per day to analyze. But have no fear, we’re going to teach you how. In this session we will perform a technical deep dive into how a global content delivery network provider is using Splunk’s Machine Learning Toolkit to discover anomalies in network traffic. We’ll take you on a data science journey and show you how we tested multiple anomaly detection techniques, overcame challenges, fine-tuned detections, and ultimately arrived at meaningful alerts based on machine learning.\n"
  },
  {
    "Event": ".conf19",
    "Title": "SECS2589 - Using Machine Learning to Unlock the Potential of Your Security Data",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SECS2589.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2589.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Vectra customers and security researchers respond to some of the world’s most consequential threats. And they tell us that there’s a consistent set of questions they must answer when investigating any attack scenario.\n \nYet, security data today is broken and unable to effectively answer those questions. It is either incomplete or storage and performance intensive.  Most teams don’t have the information necessary to properly answer the questions required to support their use cases; whether it be for threat hunting, investigations or supporting custom tools and models.\n \nIn this session, hear about real-world use cases where security teams use machine learning engines to derive unique security attributes and how it is embedded into security workflows."
  },
  {
    "Event": ".conf19",
    "Title": "IT1953 - Using real-time data to become the UK’s most recommended connectivity provider",
    "Track": "IT Operations",
    "Industry": "Communications",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1953.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1953.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "TalkTalk is the UK’s leading value telecommunications company with a strategy to become the UK’s most recommended connectivity provider. We need to intelligently use real-time data, analytics and automation in order to create a step-change improvement in customer experience and realize cost savings. This presentation explains how Splunk has helped us to use real-time network telemetry data to detect network problems, significantly improve the customer experience, and save TalkTalk money."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1106 - Using Splunk and DNS to detect that your domains are being abused for phishing",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1106.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1106.pdf",
    "SkillLevel": "Advanced",
    "Description": "As a high-profile public-sector organization, the Dutch Tax and Customs Administration deals with criminals claiming to be representatives of the organization and contacting the public with phishing e-mails every day. By using Splunk and RFC’s like, RFC7208 – Sender Policy Framework (SPF) for Authorizing Use of Domains in Email, we have developed a technique to identify phishing attacks that are carried out under the disguise of the Dutch Tax and Customs Administration. This technique is universally applicable. A precondition is access to the DNS logging. By means of this technique, insight can be obtained where the phishing e-mails are sent from and to whom the phishing e-mails are sent. In this talk we will start by explaining which standards are available to increase e-mail security and how we have build an app in Splunk, including dashboard and a wizard to create the necessary DNS records to gain insight information about the abuse of our domains.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT1962 - Using Splunk and its premium solution to accelerate DevOps lifecycle",
    "Track": "IT Operations",
    "Industry": "Technology",
    "Products": [
      "Splunk Enterprise",
      "Splunk IT Service Intelligence",
      "VictorOps"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1962.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1962.pdf",
    "SkillLevel": "Intermediate",
    "Description": "As more technology organizations pursue agility and move towards continuous delivery, a stable and reliable IT infrastructure is the foundation that enables the transformation. However, the increasing complexity of the underlying infrastructure also brings a lot of challenges. Splunk has built a variety of solutions on top of our platform to deal with this complexity and deliver analytics and troubleshooting data to our engineering teams and decision makers. We will share a bit about our continuous integration process for triaging automated tests using Splunk, how we build IT infrastructure monitoring/analytics system based on Splunk ITSI, and how we take corresponding actions via VictorOps."
  },
  {
    "Event": ".conf19",
    "Title": "FN2033 - Using Splunk Data Stream Processor as a Data Transformation, Alerting and Action Engine",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2033.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2033.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Do you wish to modify your incoming data before ingestion? How about using Splunk's real-time search feature more efficiently? Splunk Data Stream Processor (DSP) can help. DSP allows you to analyze, transform and act on your data in real-time before it is indexed by Splunk indexers.\n\nJoin us in this session to learn more about how you can use DSP as an alerting and action engine and transform your incoming data in real-time!\n\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1786 - Using Splunk Data Stream Processor for advanced stream management",
    "Track": "Foundations/Platform",
    "Industry": "Communications",
    "Products": [
      "Splunk Data Fabric Search and Data Stream Processor"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1786.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1786.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Learn how the T-Mobile Splunk Team uses Splunk Data Stream Processor (DSP) to provide advanced stream manipulation options to its user base. See how DSP is positioned in a large-scale Splunk as a service ecosystem."
  },
  {
    "Event": ".conf19",
    "Title": "FN1987 - Using Splunk Data Stream Processor as a Streaming Engine for Apache Kafka",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Data Fabric Search and Data Stream Processor"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1987.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1987.pdf",
    "SkillLevel": "Beginner",
    "Description": "Do you use Kafka but find yourself limited by what Kafka allows you to do with your data? Would you like to enrich, aggregate, and alert on your data as it moves through Kafka, but can’t figure out how? You can overcome these obstacles by integrating Kafka with the Splunk Data Stream Processor. The Splunk DSP is a data streaming platform that helps you transform and enrich your data. With DSP you can make data-driven decisions in real time as data is ingested. DSP also provides simple ways to build data pipelines, and gives you full control and visibility into your data as it flows through the platform. Apache Kafka is now widely adopted as a foundational element for data pipelines. DSP integrates seamlessly with Kafka clusters, and allows data to be read from Kafka, processed in highly scalable ways, and then written back to Kafka. Join us and see how to use DSP as a streaming engine for Kafka clusters."
  },
  {
    "Event": ".conf19",
    "Title": "IT2098 - Using Splunk for Engineering Productivity ",
    "Track": "IT Operations",
    "Industry": "Technology",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT2098.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT2098.pdf",
    "SkillLevel": "Advanced",
    "Description": "This session will discuss using Splunk to identify areas of improvement around the build and release of software by providing faster, continuous integration and delivery services for our development team at Splunk."
  },
  {
    "Event": ".conf19",
    "Title": "SEC1336 - Using Splunk to Catch Theft Rings",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Retail",
    "Products": [
      "Splunk Enterprise",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1336.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1336.pdf",
    "SkillLevel": "Advanced",
    "Description": "We helped our client use Splunk to disrupt theft rings plaguing its retail stores. We'll present how we took in public wifi data, tracked MAC addresses that appeared in multiple stores, and ultimately created a system in Splunk that alerted in-store loss prevention teams when individuals likely to be involved in theft rings entered the store. We'll go over the steps taken to operationalize our theft deterrence program so that you can adopt it in your organization or modify it to fit your needs."
  },
  {
    "Event": ".conf19",
    "Title": "DEV1114 - Vagrant and Splunk Development- Building Splunk Apps for any Environment",
    "Track": "Developer",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/DEV1114.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/DEV1114.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Vagrant is virtualization technology that builds portable, virtual software development environments. Leveraging this technology for Splunk development allows agile and DevOps teams to easily collaborate in Splunk development in as way that tightly mirrors their production environment, even when working with a mix of environments such as Mac and Windows. Vagrant’s multi-machine environments can perfectly replicate any Splunk architecture, including complicated clustering and networking configurations, using a single Vagrantfile that can be shared directly or committed to a version control system. From the Forwarder to a search head cluster member, see how your configurations and code work in your environment across the entire data pipeline right on your machine, or test your Splunkbase app on every possible architecture from single server to distributed multi-site clusters, all with a single command: vagrant up."
  },
  {
    "Event": ".conf19",
    "Title": "FN1486 - Visualizing and Augmenting Your Data with Splunk AR on Your Mobile Device",
    "Track": "Foundations/Platform",
    "Industry": "Technology",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1486.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1486.pdf",
    "SkillLevel": "Intermediate",
    "Description": "How about having an immersive data experience on your mobile device via augmented reality? Think about being in a data center where you just need to scan the QR code/NFC tag on your stacked devices to know the critical device metrics on your mobile. This session will show you how Splunk AR can be used to visualize the dashboard data that users create on the Splunk platform. Visit this session and you will learn how to create apps, dashboards and immersively reflect the data on your mobile using Splunk AR. \n"
  },
  {
    "Event": ".conf19",
    "Title": "SECD2004 - Walking the Talk for Diversity and Inclusion in Cybersecurity",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specificDiversity & Inclusion",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SECD2004.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SECD2004.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "We believe that to best defend against global security threats, an organization needs defenders who represent the diverse world that we live in. Every business will benefit greatly by bringing more people to the table with varying skills, backgrounds, leadership and views to combat the diverse adversaries out there. Here at Splunk, we have created initiatives like the \"Developing Superwomen in Cybersecurity\" program that works to diversify and equalize the cybersecurity workforce to women and other underrepresented groups. Come hear how we are taking action by making cybersecurity accessible to all with this program and some practical advice on how you can do the same when you go back to your organization! You'll receive tips on how to make information security inclusive to all with ways of engaging your staff at various levels and receive a blueprint for running your own gamified security experiences, allowing you to up-level staff while embracing their unique talents and backgrounds."
  },
  {
    "Event": ".conf19",
    "Title": "IT1785 - Want to Turbocharge your Developer Pipeline-",
    "Track": "IT Operations",
    "Industry": "TechnologyNot industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1785.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1785.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Do you want to help your developers waste less time on issues and spend more time optimizing build times using Splunk? Do you want to improve developer satisfaction while cutting build times by 35% on a long-established and entrenched code base? Then join us to hear how Jira developers at Atlassian, the world leader in software collaboration tools, did just that. We will explain how we used Splunk Enterprise to collect metrics as structured events from developer machines to improve our Maven build times for several hundred developers. We aggregated, analyzed, and visualized these events to not only identify and resolve performance bottlenecks as they occurred across our developer pipeline, but also to pinpoint the next big thing to tackle; a dream result for us. Not a Maven user? No worries. The approaches we will cover are good for any developer setup, allowing you to jump start build time improvements while generating the continuous insights you need to do more with less time and less waste. "
  },
  {
    "Event": ".conf19",
    "Title": "BA2130 - What's New in Business Analytics and Business Flow",
    "Track": "Business Analytics",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Business Flow"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/BA2130.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/BA2130.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Business operations teams have relied on Splunk for operational intelligence, helping them to discover bottlenecks, fallout, and other issues in order to deliver more efficient business processes and customer experiences with higher conversions. In this session, learn about Splunk's latest innovations for business operations professionals."
  },
  {
    "Event": ".conf19",
    "Title": "FN1740 - What's new in SPL2-",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1740.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1740.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "This session will be all about exciting Foundations/Platform related content that we'll announce .conf19. We can't tell you about it now, but trust us — it's awesome.\n\n \n"
  },
  {
    "Event": ".conf19",
    "Title": "SEC2366 - What's New in Splunk for Security",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise Security",
      "Splunk User Behavior Analytics",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC2366.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC2366.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Our security research, engineering and product teams have been hard at work building new capabilities to bolster your Splunk security stack. Find out what they’ve been up to since .conf18, and watch a demonstration of the latest innovations in Splunk Enterprise Security, Splunk User Behavior Analytics, and Splunk Phantom. There are other awesome developments that we can’t share now but are excited to share with you at .conf. Our security research, engineering and product teams have been hard at work building new capabilities to bolster your Splunk security stack. Find out what they’ve been up to since .conf18, and watch a demonstration of the latest innovations in Splunk Enterprise Security, Splunk User Behavior Analytics, and Splunk Phantom. There are other awesome developments that we can’t share now but are excited to share with you at .conf."
  },
  {
    "Event": ".conf19",
    "Title": "FN2516 - -What's New in the latest release of Splunk Cloud and Splunk Enterprise-",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN2516.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN2516.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "This session will detail new innovations and features included in the .conf19 release of Splunk Cloud and Splunk Enterprise. This is one of the most well-attended .conf19 sessions. Be sure to add it to your agenda."
  },
  {
    "Event": ".conf19",
    "Title": "FN1735 - What’s next in Geo for Splunk ",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Developer Cloud"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1735.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1735.pdf",
    "SkillLevel": "Intermediate",
    "Description": "This session will be all about exciting Foundations/Platform-related content that we'll announce at .conf19. We can't tell you about it now, but trust us — it's awesome.\n"
  },
  {
    "Event": ".conf19",
    "Title": "FN1635 - What's on your bucket list-  Scalability and high volume performance of indexer clustering at Splunk.",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1635.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1635.pdf",
    "SkillLevel": "Intermediate",
    "Description": "As customers add more and more data to Splunk, indexer clusters with large volumes of indexers, indexes, and buckets are becoming commonplace. In Splunk labs we run intensive tests to explore the boundaries of the largest indexer clusters. This session will discuss the lifecycle of a Splunk bucket, why it is a key metric in indexer scalability, and which indicators and tunables to monitor in a very large cluster. We'll also share how we do performance testing, the latest performance results, and best practices for scaling your Splunk Enterprise cluster to 20 million unique buckets and beyond."
  },
  {
    "Event": ".conf19",
    "Title": "IoT2146 - Where the rubber meets the road, an Internet of Things (IoT) and Machine Learning case study",
    "Track": "Internet of Things",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML",
      "Splunk for Industrial IoT"
    ],
    "SkillLevel": "Good for all skill levels",
    "Description": "Ever wonder how to formulate the forecasting methodology with a long-term, tactical-strategy that can help top management with decision making? This session dives into a case study that shows how to use machine learning (ML) to predict the consumer purchasing trends that affect forecast accuracy; data that has a direct impact on demand planning and managing the global supply chain. By feeding IoT data into Splunk, we can gain insights into operations that may have never been thought possible. Come join us to see where the rubber meets the road and how you can use Splunk to improve your business.\n"
  },
  {
    "Event": ".conf19",
    "Title": "IT1346 - Why Dinosaurs Make Bad Pets-  Legacy Monitoring Tools and their Extinction – TIAA Adopts ITSI as their new MoM",
    "Track": "IT Operations",
    "Industry": "Financial ServicesNot industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Cloud",
      "Splunk IT Service Intelligence"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/IT1346.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/IT1346.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Too many tools, too many silos between data and collaboration, Outages take too long to Identify Root Cause and There is So Much Noise Abstract:  TIAA had a goal – to replace Legacy Monitoring with an AIOps approach.  What did that mean?  They had to find a better way to break down the silos between data and collaboration and start focusing attention on the right things with the right people.  Monitoring had become about MTTI (mean time to innocence) instead of fixing the fight issues more quickly and finding a way to move from ‘reacting’ to outages to ‘preventing’ them.  ITSI has become the ‘aggregator’ of monitoring data and will help TIAA move from the old Dinosaur Approach of being event driven to the AiOps approach of Service and Priority Driven.  Learn about the Journey, the Lessons Learned, and the Best Practices to Ensure Success.   "
  },
  {
    "Event": ".conf19",
    "Title": "FN1698 - Winning in Starcraft 2; An Analysis of Skill Using SPL & MLTK",
    "Track": "Foundations/Platform",
    "Industry": "Technology",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit",
      "AI/ML"
    ],
    "SkillLevel": "Intermediate",
    "Description": "How do you measure skill between professional StarCraft 2 eSports players and casual players? Are data points like Actuations Per Minute (APM), duration of supply blocks, and base expansion rate accurate enough metrics? In this session, we will leverage Splunk, statistics, and MLTK to answer these questions and more! We will dive into how we deconstructed the video game’s proprietary data structures into parsable fields, the analysis performed to transform data into skill, and how to use this same data in predictive modeling. If geeking out about statistics and video games is your thing, this session is for you! "
  },
  {
    "Event": ".conf19",
    "Title": "FN1540 - You Only Learn Once (YOLO)",
    "Track": "Foundations/Platform",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Enterprise",
      "Splunk Machine Learning Toolkit"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/FN1540.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/FN1540.pdf",
    "SkillLevel": "Intermediate",
    "Description": "Want to use your custom model with the data already in Splunk? Want to contribute to an open library for Machine Learning Toolkit (MLTK) algorithms? Want to use your favorite Machine Learning library? This session will help you to create custom algorithms and leverage the power of any ML algorithm you have ever wanted to use for your application. Traverse the entire process from building a custom algorithm, fitting the model to your data, testing your application, to contributing to the MLTK Algorithms library on Github.\n"
  },
  {
    "Event": ".conf19",
    "Title": "SEC1264 - You replaced IBM QRadar with Splunk Enterprise Security. Now What-",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Not industry specific",
    "Products": [
      "Splunk Cloud",
      "Splunk Enterprise Security"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1264.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1264.pdf",
    "SkillLevel": "Beginner",
    "Description": "Never used Splunk before, have no Splunk admins and you’ve just bought Splunk Enterprise Security? That was us, and now we're using Splunk in ways that we could've only dreamed of using IBM QRadar. In this session we’ll share our implementation story, how we worked with Splunk to accelerate our learning curve, and how we went from 0 to 3TB in 3 months with no Splunk admins. We'll also cover how Splunk allows us to onboard data sources that we couldn't with QRadar. "
  },
  {
    "Event": ".conf19",
    "Title": "SEC1511 - Zero to Hero- A 202-Year-Old Firm’s Journey to End-to-End Security Visibility",
    "Track": "Security, Compliance and Fraud",
    "Industry": "Manufacturing",
    "Products": [
      "Splunk Cloud",
      "Splunk Enterprise Security",
      "Phantom"
    ],
    "VideoUrl": "https://conf.splunk.com/files/2019/recordings/SEC1511.mp4",
    "SlidesUrl": "https://conf.splunk.com/files/2019/slides/SEC1511.pdf",
    "SkillLevel": "Good for all skill levels",
    "Description": "Does your small team also run a full-featured SOC that supports a global company? In this session we’ll show you how we’ve used Splunk Cloud and Splunk Enterprise Security to bring together all the relevant security intelligence from our technology stack, transforming our security operations from ad hoc and tactical to strategic and compliance-driven. We’ll discuss key takeaways from our journey, such as the benefits of ingesting data properly from the outset so you can reap the rewards as you scale; how we leverage multiple use cases out of single data sources; and how we created easy-to-understand visualizations that convey our firm’s security posture to management.\n"
  }
]
